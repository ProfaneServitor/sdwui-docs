{"/sdwui-docs/categories/": {
    "title": "categories",
    "keywords": "Guides",
    "url": "/sdwui-docs/categories/",
    "body": "Guides categories Command line arguments and settings Custom scripts development Contributing Getting started Online services Stable Diffusion webUI Изменение настроек интерфейса Install guides API guide"
  },"/sdwui-docs/pages/en/Command-Line-Arguments-and-Settings/": {
    "title": "Command line arguments and settings",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Command-Line-Arguments-and-Settings/",
    "body": "webui-user The recommended way to customize how the program is run is editing webui-user.bat (Windows) and webui-user.sh (Linux): set PYTHON allows for setting a custom Python path Example: set PYTHON=b:/soft/Python310/Python.exe set VENV_DIR allows you to choose the directory for the virtual environment. Default is venv. Special value - runs the script without creating virtual environment. Example: set VENV_DIR=C:\\run\\var\\run will create venv in the C:\\run\\var\\run directory. Example: set VENV_DIR=- runs the program using the system’s python set COMMANDLINE_ARGS setting the command line arguments webui.py is ran with Example: set COMMANDLINE_ARGS=--ckpt a.ckpt uses the model a.ckpt instead of model.ckpt Command Line Arguments Running online Use the --share option to run online. You will get a xxx.app.gradio link. This is the intended way to use the program in collabs. You may set up authentication for said gradio shared instance with the flag --gradio-auth username:password, optionally providing multiple sets of usernames and passwords separated by commas. Use --listen to make the server listen to network connections. This will allow computers on the local network to access the UI, and if you configure port forwarding, also computers on the internet. Use --port xxxx to make the server listen on a specific port, xxxx being the wanted port. Remember that all ports below 1024 need root/admin rights, for this reason it is advised to use a port above 1024. Defaults to port 7860 if available. All command line arguments Argument Command Value Default Description CONFIGURATION       -h, –help None False show this help message and exit –config CONFIG configs/stable-diffusion/v1-inference.yaml path to config which constructs model –ckpt CKPT model.ckpt path to checkpoint of stable diffusion model; if specified, this checkpoint will be added to the list of checkpoints and loaded –ckpt-dir CKPT_DIR None Path to directory with stable diffusion checkpoints –gfpgan-dir GFPGAN_DIR GFPGAN/ GFPGAN directory –gfpgan-model GFPGAN_MODEL GFPGAN model file name   –codeformer-models-path CODEFORMER_MODELS_PATH models/Codeformer/ Path to directory with codeformer model file(s). –gfpgan-models-path GFPGAN_MODELS_PATH models/GFPGAN Path to directory with GFPGAN model file(s). –esrgan-models-path ESRGAN_MODELS_PATH models/ESRGAN Path to directory with ESRGAN model file(s). –bsrgan-models-path BSRGAN_MODELS_PATH models/BSRGAN Path to directory with BSRGAN model file(s). –realesrgan-models-path REALESRGAN_MODELS_PATH models/RealESRGAN Path to directory with RealESRGAN model file(s). –scunet-models-path SCUNET_MODELS_PATH models/ScuNET Path to directory with ScuNET model file(s). –swinir-models-path SWINIR_MODELS_PATH models/SwinIR Path to directory with SwinIR and SwinIR v2 model file(s). –ldsr-models-path LDSR_MODELS_PATH models/LDSR Path to directory with LDSR model file(s). –clip-models-path CLIP_MODELS_PATH None Path to directory with CLIP model file(s). –vae-path VAE_PATH None Path to Variational Autoencoders model –embeddings-dir EMBEDDINGS_DIR embeddings/ embeddings directory for textual inversion (default: embeddings) –hypernetwork-dir HYPERNETWORK_DIR models/hypernetworks/ hypernetwork directory –localizations-dir LOCALIZATIONS_DIR localizations/ localizations directory –styles-file STYLES_FILE styles.csv filename to use for styles –ui-config-file UI_CONFIG_FILE ui-config.json filename to use for ui configuration –no-progressbar-hiding None False do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware acceleration in browser) –max-batch-count MAX_BATCH_COUNT 16 maximum batch count value for the UI –ui-settings-file UI_SETTINGS_FILE config.json filename to use for ui settings –allow-code None False allow custom script execution from webui –share None False use share=True for gradio and make the UI accessible through their site (doesn’t work for me but you might have better luck) –listen None False launch gradio with 0.0.0.0 as server name, allowing to respond to network requests –port PORT 7860 launch gradio with given server port, you need root/admin rights for ports &lt; 1024, defaults to 7860 if available –hide-ui-dir-config None False hide directory configuration from webui –freeze-settings None False disable editing settings –enable-insecure-extension-access None False enable extensions tab regardless of other options –gradio-debug None False launch gradio with –debug option –gradio-auth GRADIO_AUTH None set gradio authentication like “username:password”; or comma-delimit multiple like “u1:p1,u2:p2,u3:p3” –gradio-img2img-tool {color-sketch,editor} editor gradio image uploader tool: can be either editor for ctopping, or color-sketch for drawing –disable-console-progressbars None False do not output progressbars to console –enable-console-prompts None False print prompts to console when generating with txt2img and img2img –api None False launch webui with API –nowebui None False only launch the API, without the UI –ui-debug-mode None False Don’t load model to quickly launch UI –device-id DEVICE_ID None Select the default CUDA device to use (export CUDA_VISIBLE_DEVICES=0,1,etc might be needed before) –administrator None False Administrator rights PERFORMANCE       –xformers None False enable xformers for cross attention layers –reinstall-xformers None False force reinstall xformers. Useful for upgrading - but remove it after upgrading or you’ll reinstall xformers perpetually. –force-enable-xformers None False enable xformers for cross attention layers regardless of whether the checking code thinks you can run it; do not make bug reports if this fails to work –opt-split-attention None False force-enables Doggettx’s cross-attention layer optimization. By default, it’s on for cuda enabled systems. –opt-split-attention-invokeai None False force-enables InvokeAI’s cross-attention layer optimization. By default, it’s on when cuda is unavailable. –opt-split-attention-v1 None False enable older version of split attention optimization that does not consume all the VRAM it can find –opt-channelslast None False change memory type for stable diffusion to channels last –disable-opt-split-attention None False force-disables cross-attention layer optimization –use-cpu {all, sd, interrogate, gfpgan, bsrgan, esrgan, scunet, codeformer} None use CPU as torch device for specified modules –no-half None False do not switch the model to 16-bit floats –precision {full,autocast} autocast evaluate at this precision –no-half-vae None False do not switch the VAE model to 16-bit floats –medvram None False enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage –lowvram None False enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage –lowram None False load stable diffusion checkpoint weights to VRAM instead of RAM –always-batch-cond-uncond None False disables cond/uncond batching that is enabled to save memory with –medvram or –lowvram FEATURES       –autolaunch None False open the webui URL in the system’s default browser upon launch –theme None Unset open the webui with the specified theme (“light” or “dark”). If not specified, uses the default browser theme –use-textbox-seed None False use textbox for seeds in UI (no up/down, but possible to input long seeds) –disable-safe-unpickle None False disable checking pytorch models for malicious code –ngrok NGROK Unset ngrok authtoken, alternative to gradio –share –ngrok-region NGROK_REGION Unset The region in which ngrok should start. DEFUNCT OPTIONS       –show-negative-prompt None False does not do anything –deepdanbooru None False does not do anything –unload-gfpgan None False does not do anything."
  },"/sdwui-docs/pages/en/Contributing/": {
    "title": "Contributing",
    "keywords": "development",
    "url": "/sdwui-docs/pages/en/Contributing/",
    "body": "Pull requests To contribute, clone the repository, make your changes, commit and push to your clone, and submit a pull request. Make sure that your changes do not break anything by running tests. If you’re adding a lot of code, consider making your contribution an extension, and only PR small changes you need in main code to make the extension possible. If you are making changes to used libraries or the installation script, you must verify them to work on default Windows installation from scratch. If you cannot test if it works (due to your OS or anything else), do not make those changes (with possible exception of changes that explicitly are guarded from being executed on Windows by ifs or something else). Code style I mostly follow code style suggested by PyCharm, with the exception of disabled line length limit. Please do not submit PRs where you just take existing lines and reformat them without changing what they do. Gradio Gradio at some point wanted to add this section to shill their project in the contributing section, which I didn’t have at the time, so here it is now. For Gradio check out the docs to contribute: Have an issue or feature request with Gradio? open a issue/feature request on github for support: https://github.com/gradio-app/gradio/issues"
  },"/sdwui-docs/pages/en/Custom-Scripts/": {
    "title": "Custom scripts",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Custom-Scripts/",
    "body": "Installing and Using Custom Scripts To install custom scripts, place them into the scripts directory and click the Reload custom script button at the bottom in the settings tab. Custom scripts will appear in the lower-left dropdown menu on the txt2img and img2img tabs after being installed. Below are some notable custom scripts created by Web UI users: Custom Scripts from Users Improved prompt matrix https://github.com/ArrowM/auto1111-improved-prompt-matrix This script is advanced-prompt-matrix modified to support batch count. Grids are not created. Usage: Use &lt; &gt; to create a group of alternate texts. Separate text options with |. Multiple groups and multiple options can be used. For example: An input of a &lt;corgi|cat&gt; wearing &lt;goggles|a hat&gt; Will output 4 prompts: a corgi wearing goggles, a corgi wearing a hat, a cat wearing goggles, a cat wearing a hat When using a batch count &gt; 1, each prompt variation will be generated for each seed. batch size is ignored. txt2img2img https://github.com/ThereforeGames/txt2img2img Greatly improve the editability of any character/subject while retaining their likeness. The main motivation for this script is improving the editability of embeddings created through Textual Inversion. (be careful with cloning as it has a bit of venv checked in) Example: (Click to expand:) txt2mask https://github.com/ThereforeGames/txt2mask Allows you to specify an inpainting mask with text, as opposed to the brush. Example: (Click to expand:) Mask drawing UI https://github.com/dfaker/stable-diffusion-webui-cv2-external-masking-script Provides a local popup window powered by CV2 that allows addition of a mask before processing. Example: (Click to expand:) Img2img Video https://github.com/memes-forever/Stable-diffusion-webui-video Using img2img, generates pictures one after another. Seed Travel https://github.com/yownas/seed_travel Pick two (or more) seeds and generate a sequence of images interpolating between them. Optionally, let it create a video of the result. Example of what you can do with it: https://www.youtube.com/watch?v=4c71iUclY4U Another example by a user: Advanced Seed Blending https://github.com/amotile/stable-diffusion-backend/tree/master/src/process/implementations/automatic1111_scripts This script allows you to base the initial noise on multiple weighted seeds. Ex. seed1:2, seed2:1, seed3:1 The weights are normalized so you can use bigger once like above, or you can do floating point numbers: Ex. seed1:0.5, seed2:0.25, seed3:0.25 Prompt Blending https://github.com/amotile/stable-diffusion-backend/tree/master/src/process/implementations/automatic1111_scripts This script allows you to combine multiple weighted prompts together by mathematically combining their textual embeddings before generating the image. Ex. Crystal containing elemental {fire|ice} It supports nested definitions so you can do this as well: Crystal containing elemental {{fire:5|ice}|earth} Animator https://github.com/Animator-Anon/Animator A basic img2img script that will dump frames and build a video file. Suitable for creating interesting zoom in warping movies, but not too much else at this time. Parameter Sequencer https://github.com/rewbs/sd-parseq Generate videos with tight control and flexible interpolation over many Stable Diffusion parameters (such as seed, scale, prompt weights, denoising strength…), as well as input processing parameter (such as zoom, pan, 3D rotation…) Alternate Noise Schedules https://gist.github.com/dfaker/f88aa62e3a14b559fe4e5f6b345db664 Uses alternate generators for the sampler’s sigma schedule. Allows access to Karras, Exponential and Variance Preserving schedules from crowsonkb/k-diffusion along with their parameters. Vid2Vid https://github.com/Filarius/stable-diffusion-webui/blob/master/scripts/vid2vid.py From real video, img2img the frames and stitch them together. Does not unpack frames to hard drive. Txt2VectorGraphics https://github.com/GeorgLegato/Txt2Vectorgraphics Create custom, scaleable icons from your prompts as SVG or PDF. Example: (Click to expand:) | prompt |PNG |SVG | | :-------- | :-----------------: | :---------------------: | | Happy Einstein | | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193370379-2680aa2a-f460-44e7-9c4e-592cf096de71.svg\" width=30%/&gt; | | Mountainbike Downhill | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193371353-f0f5ff6f-12f7-423b-a481-f9bd119631dd.png\" width=40%/&gt; | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193371585-68dea4ca-6c1a-4d31-965d-c1b5f145bb6f.svg\" width=30%/&gt; | coffe mug in shape of a heart | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193374299-98379ca1-3106-4ceb-bcd3-fa129e30817a.png\" width=40%/&gt; | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193374525-460395af-9588-476e-bcf6-6a8ad426be8e.svg\" width=30%/&gt; | | Headphones | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193376238-5c4d4a8f-1f06-4ba4-b780-d2fa2e794eda.png\" width=40%/&gt; | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193376255-80e25271-6313-4bff-a98e-ba3ae48538ca.svg\" width=30%/&gt; | Shift Attention https://github.com/yownas/shift-attention Generate a sequence of images shifting attention in the prompt. This script enables you to give a range to the weight of tokens in a prompt and then generate a sequence of images stepping from the first one to the second. Loopback and Superimpose https://github.com/DiceOwl/StableDiffusionStuff https://github.com/DiceOwl/StableDiffusionStuff/blob/main/loopback_superimpose.py Mixes output of img2img with original input image at strength alpha. The result is fed into img2img again (at loop&gt;=2), and this procedure repeats. Tends to sharpen the image, improve consistency, reduce creativity and reduce fine detail. Interpolate https://github.com/DiceOwl/StableDiffusionStuff https://github.com/DiceOwl/StableDiffusionStuff/blob/main/interpolate.py An img2img script to produce in-between images. Allows two input images for interpolation. More features shown in the readme. Run n times https://gist.github.com/camenduru/9ec5f8141db9902e375967e93250860f Run n times with random seed. Advanced Loopback https://github.com/Extraltodeus/advanced-loopback-for-sd-webui Dynamic zoom loopback with parameters variations and prompt switching amongst other features! prompt-morph https://github.com/feffy380/prompt-morph Generate morph sequences with Stable Diffusion. Interpolate between two or more prompts and create an image at each step. Uses the new AND keyword and can optionally export the sequence as a video. prompt interpolation https://github.com/EugeoSynthesisThirtyTwo/prompt-interpolation-script-for-sd-webui With this script, you can interpolate between two prompts (using the “AND” keyword), generate as many images as you want. You can also generate a gif with the result. Works for both txt2img and img2img. Example: (Click to expand:) ![gif](https://user-images.githubusercontent.com/24735555/195470874-afc3dfdc-7b35-4b23-9c34-5888a4100ac1.gif) Asymmetric Tiling https://github.com/tjm35/asymmetric-tiling-sd-webui/ Control horizontal/vertical seamless tiling independently of each other. Example: (Click to expand:) Force Symmetry https://gist.github.com/1ort/2fe6214cf1abe4c07087aac8d91d0d8a see https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/2441 applies symmetry to the image every n steps and sends the result further to img2img. Example: (Click to expand:) SD-latent-mirroring https://github.com/dfaker/SD-latent-mirroring Applies mirroring and flips to the latent images to produce anything from subtle balanced compositions to perfect reflections Example: (Click to expand:) txt2palette https://github.com/1ort/txt2palette Generate palettes by text description. This script takes the generated images and converts them into color palettes. Example: (Click to expand:) StylePile https://github.com/some9000/StylePile An easy way to mix and match elements to prompts that affect the style of the result. Example: (Click to expand:) XYZ Plot Script https://github.com/xrpgame/xyz_plot_script Generates an .html file to interactively browse the imageset. Use the scroll wheel or arrow keys to move in the Z dimension. Example: (Click to expand:) xyz-plot-grid https://github.com/Gerschel/xyz-plot-grid Place xyz_grid.py in scripts folder along side other scripts. Works like x/y plot, like how you would expect, but now has a z. Works like how you’d expect it to work, with grid legends as well. Example: (Click to expand:) Expanded-XY-grid https://github.com/0xALIVEBEEF/Expanded-XY-grid Custom script for AUTOMATIC1111’s stable-diffusion-webui that adds more features to the standard xy grid: Multitool: Allows multiple parameters in one axis, theoretically allows unlimited parameters to be adjusted in one xy grid Customizable prompt matrix Group files in a directory S/R Placeholder - replace a placeholder value (the first value in the list of parameters) with desired values. Add PNGinfo to grid image Example: (Click to expand:) Example images: Prompt: \"darth vader riding a bicycle, modifier\"; X: Multitool: \"Prompt S/R: bicycle, motorcycle | CFG scale: 7.5, 10 | Prompt S/R Placeholder: modifier, 4k, artstation\"; Y: Multitool: \"Sampler: Euler, Euler a | Steps: 20, 50\" Booru tag autocompletion https://github.com/DominikDoom/a1111-sd-webui-tagcomplete Displays autocompletion hints for tags from “image booru” boards such as Danbooru. Uses local tag CSV files and includes a config for customization. Also supports completion for wildcards Embedding to PNG https://github.com/dfaker/embedding-to-png-script Converts existing embeddings to the shareable image versions. Example: (Click to expand:) Alpha Canvas https://github.com/TKoestlerx/sdexperiments Outpaint a region. Infinite outpainting concept, used the two existing outpainting scripts from the AUTOMATIC1111 repo as a basis. Example: (Click to expand:) Random grid https://github.com/lilly1987/AI-WEBUI-scripts-Random Randomly enter xy grid values. Example: (Click to expand:) Basic logic is same as x/y plot, only internally, x type is fixed as step, and type y is fixed as cfg. Generates x values as many as the number of step counts (10) within the range of step1|2 values (10-30) Generates x values as many as the number of cfg counts (10) within the range of cfg1|2 values (6-15) Even if you put the 1|2 range cap upside down, it will automatically change it. In the case of the cfg value, it is treated as an int type and the decimal value is not read. Random https://github.com/lilly1987/AI-WEBUI-scripts-Random Repeat a simple number of times without a grid. Example: (Click to expand:) Stable Diffusion Aesthetic Scorer https://github.com/grexzen/SD-Chad Rates your images. img2tiles https://github.com/arcanite24/img2tiles generate tiles from a base image. Based on SD upscale script. Example: (Click to expand:) img2mosiac https://github.com/1ort/img2mosaic Generate mosaics from images. The script cuts the image into tiles and processes each tile separately. The size of each tile is chosen randomly. Example: (Click to expand:) Depth Maps https://github.com/thygate/stable-diffusion-webui-depthmap-script This script is an addon for AUTOMATIC1111’s Stable Diffusion Web UI that creates depthmaps from the generated images. The result can be viewed on 3D or holographic devices like VR headsets or lookingglass display, used in Render- or Game- Engines on a plane with a displacement modifier, and maybe even 3D printed. Example: (Click to expand:) Test my prompt https://github.com/Extraltodeus/test_my_prompt Have you ever used a very long prompt full of words that you are not sure have any actual impact on your image? Did you lose the courage to try to remove them one by one to test if their effects are worthy of your pwescious GPU? WELL now you don’t need any courage as this script has been MADE FOR YOU! It generates as many images as there are words in your prompt (you can select the separator of course). Example: (Click to expand:) Here the prompt is simply : \"**banana, on fire, snow**\" and so as you can see it has generated each image without each description in it. You can also test your negative prompt. Pixel Art https://github.com/C10udburst/stable-diffusion-webui-scripts Simple script which resizes images by a variable amount, also converts image to use a color palette of a given size. Example: (Click to expand:) | Disabled | Enabled x8, no resize back, no color palette | Enabled x8, no color palette | Enabled x8, 16 color palette | | :---: | :---: | :---: | :---: | |![preview](https://user-images.githubusercontent.com/18114966/201491785-e30cfa9d-c850-4853-98b8-11db8de78c8d.png) | ![preview](https://user-images.githubusercontent.com/18114966/201492204-f4303694-e98d-4ea3-8256-538a88ea26b6.png) | ![preview](https://user-images.githubusercontent.com/18114966/201491864-d0c0c9f1-e34f-4cb6-a68e-7043ec5ce74e.png) | ![preview](https://user-images.githubusercontent.com/18114966/201492175-c55fa260-a17d-47c9-a919-9116e1caa8fe.png) | [model used](https://publicprompts.art/all-in-one-pixel-art-dreambooth-model/) ```text japanese pagoda with blossoming cherry trees, full body game asset, in pixelsprite style Steps: 20, Sampler: DDIM, CFG scale: 7, Seed: 4288895889, Size: 512x512, Model hash: 916ea38c, Batch size: 4 ``` Multiple Hypernetworks https://github.com/antis0007/sd-webui-multiple-hypernetworks Adds the ability to apply multiple hypernetworks at once. Overrides the hijack, optimization and CrossAttention forward functions in order to apply multiple hypernetworks sequentially, with different weights. Hypernetwork Structure(.hns)/Variable Dropout/ Monkey Patches https://github.com/aria1th/Hypernetwork-MonkeyPatch-Extension Adds the ability to apply Hypernetwork Structure, as defining it in .hns file. see here for detailed information. Adds the ability to use proper variable dropout rate, like 0.05. Also fixes issues with using hypernetwork right after training. Adds creating beta-hypernetwork(dropout), and beta-training which allows automatic cosine annealing, and no-crop usage of original images. Config-Presets https://github.com/Zyin055/Config-Presets-Script-OLD- Quickly change settings in the txt2img and img2img tabs using a configurable dropdown of preset values. Example: (Click to expand:) Saving steps of the sampling process This script will save steps of the sampling process to a directory. import os.path import modules.scripts as scripts import gradio as gr from modules import sd_samplers, shared from modules.processing import Processed, process_images class Script(scripts.Script): def title(self): return \"Save steps of the sampling process to files\" def ui(self, is_img2img): path = gr.Textbox(label=\"Save images to path\") return [path] def run(self, p, path): index = [0] def store_latent(x): image = shared.state.current_image = sd_samplers.sample_to_image(x) image.save(os.path.join(path, f\"sample-{index[0]:05}.png\")) index[0] += 1 fun(x) fun = sd_samplers.store_latent sd_samplers.store_latent = store_latent try: proc = process_images(p) finally: sd_samplers.store_latent = fun return Processed(p, proc.images, p.seed, \"\")"
  },"/sdwui-docs/online-services/": {
    "title": "Online services",
    "keywords": "Getting started",
    "url": "/sdwui-docs/online-services/",
    "body": "Google Colab maintained by TheLastBen maintained by camenduru maintained by ddPn08 maintained by Akaibu Colab, original by me, outdated. Paperspace maintained by Cyberes Kaggle maintained by camenduru SageMaker Studio Lab maintained by fractality Hugging Face maintained by camenduru Installation Guides azure-ml - (commit)"
  },"/sdwui-docs/": {
    "title": "Stable Diffusion webUI",
    "keywords": "Getting started",
    "url": "/sdwui-docs/",
    "body": "A browser interface based on Gradio library for Stable Diffusion. Check the custom scripts wiki page for extra scripts developed by users. Features Detailed feature showcase with images: Original txt2img and img2img modes One click install and run script (but you still must install python and git) Outpainting Inpainting Color Sketch Prompt Matrix Stable Diffusion Upscale Attention, specify parts of text that the model should pay more attention to a man in a ((tuxedo)) - will pay more attention to tuxedo a man in a (tuxedo:1.21) - alternative syntax select text and press ctrl+up or ctrl+down to automatically adjust attention to selected text (code contributed by anonymous user) Loopback, run img2img processing multiple times X/Y plot, a way to draw a 2 dimensional plot of images with different parameters Textual Inversion have as many embeddings as you want and use any names you like for them use multiple embeddings with different numbers of vectors per token works with half precision floating point numbers train embeddings on 8GB (also reports of 6GB working) Extras tab with: GFPGAN, neural network that fixes faces CodeFormer, face restoration tool as an alternative to GFPGAN RealESRGAN, neural network upscaler ESRGAN, neural network upscaler with a lot of third party models SwinIR and Swin2SR(see here), neural network upscalers LDSR, Latent diffusion super resolution upscaling Resizing aspect ratio options Sampling method selection Adjust sampler eta values (noise multiplier) More advanced noise setting options Interrupt processing at any time 4GB video card support (also reports of 2GB working) Correct seeds for batches Live prompt token length validation Generation parameters parameters you used to generate images are saved with that image in PNG chunks for PNG, in EXIF for JPEG can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI can be disabled in settings drag and drop an image/text-parameters to promptbox Read Generation Parameters Button, loads parameters in promptbox to UI Settings page Running arbitrary python code from UI (must run with –allow-code to enable) Mouseover hints for most UI elements Possible to change defaults/mix/max/step values for UI elements via text config Random artist button Tiling support, a checkbox to create images that can be tiled like textures Progress bar and live image generation preview Negative prompt, an extra text field that allows you to list what you don’t want to see in generated image Styles, a way to save part of prompt and easily apply them via dropdown later Variations, a way to generate same image but with tiny differences Seed resizing, a way to generate same image but at slightly different resolution CLIP interrogator, a button that tries to guess prompt from an image Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway Batch Processing, process a group of files using img2img Img2img Alternative, reverse Euler method of cross attention control Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions Reloading checkpoints on the fly Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one Custom scripts with many extensions from community Composable-Diffusion, a way to use multiple prompts at once separate prompts using uppercase AND also supports weights for prompts: a cat :1.2 AND a dog AND a penguin :2.2 No token limit for prompts (original stable diffusion lets you use up to 75 tokens) DeepDanbooru integration, creates danbooru style tags for anime prompts xformers, major speed increase for select cards: (add –xformers to commandline args) via extension: History tab: view, direct and delete images conveniently within the UI Generate forever option Training tab hypernetworks and embeddings options Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime) Clip skip Use Hypernetworks Use VAEs Estimated completion time in progress bar API Support for dedicated inpainting model by RunwayML. via extension: Aesthetic Gradients, a way to generate images with a specific aesthetic by using clip images embeds (implementation of https://github.com/vicgalle/stable-diffusion-aesthetic-gradients) Stable Diffusion 2.0 support - see wiki for instructions Installation and Running Make sure the required dependencies are met and follow the instructions available for both NVidia (recommended) and AMD GPUs. Alternatively, use online services (like Google Colab): List of Online Services Automatic Installation on Windows Install Python 3.10.6, checking “Add Python to PATH” Install git. Download the stable-diffusion-webui repository, for example by running git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git. Place model.ckpt in the models directory (see dependencies for where to get it). *(Optional)* Place GFPGANv1.4.pth in the base directory, alongside webui.py (see dependencies for where to get it). Run webui-user.bat from Windows Explorer as normal, non-administrator, user. Automatic Installation on Linux Install the dependencies: # Debian-based: sudo apt install wget git python3 python3-venv # Red Hat-based: sudo dnf install wget git python3 # Arch-based: sudo pacman -S wget git python3 To install in /home/$(whoami)/stable-diffusion-webui/, run: bash &lt;(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh) Installation on Apple Silicon Find the instructions here. Contributing Here’s how to add code to this repo: Contributing Documentation The documentation was moved from this README over to the project’s wiki. Credits Stable Diffusion - https://github.com/CompVis/stable-diffusion, https://github.com/CompVis/taming-transformers k-diffusion - https://github.com/crowsonkb/k-diffusion.git GFPGAN - https://github.com/TencentARC/GFPGAN.git CodeFormer - https://github.com/sczhou/CodeFormer ESRGAN - https://github.com/xinntao/ESRGAN SwinIR - https://github.com/JingyunLiang/SwinIR Swin2SR - https://github.com/mv-lab/swin2sr LDSR - https://github.com/Hafiidz/latent-diffusion MiDaS - https://github.com/isl-org/MiDaS Ideas for optimizations - https://github.com/basujindal/stable-diffusion Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing. Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion) Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we’re not using his code, but we are using his ideas). Idea for SD upscale - https://github.com/jquesnelle/txt2imghd Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch xformers - https://github.com/facebookresearch/xformers DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru Security advice - RyotaK Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user. (You)"
  },"/sdwui-docs/ui-defaults/": {
    "title": "Изменение настроек интерфейса",
    "keywords": "Getting started",
    "url": "/sdwui-docs/ui-defaults/",
    "body": "Значения по умолчанию в веб-интерфейсе можно изменить, отредактировав файл ui-config.json, который появляется в базовом каталоге, содержащем webui.py, после первого запуска. Изменения применяются только после перезапуска. { \"txt2img/Sampling Steps/value\": 20, \"txt2img/Sampling Steps/minimum\": 1, \"txt2img/Sampling Steps/maximum\": 150, \"txt2img/Sampling Steps/step\": 1, \"txt2img/Batch count/value\": 1, \"txt2img/Batch count/minimum\": 1, \"txt2img/Batch count/maximum\": 32, \"txt2img/Batch count/step\": 1, \"txt2img/Batch size/value\": 1, \"txt2img/Batch size/minimum\": 1, # ... }"
  },"/sdwui-docs/api/": {
    "title": "API guide",
    "keywords": "guides",
    "url": "/sdwui-docs/api/",
    "body": "RUSSIAN First, of course, is to run web ui with --api commandline argument example in your “webui-user.bat”: set COMMANDLINE_ARGS=--api This enables the api which can be reviewed at http://127.0.0.1:7860/docs (or whever the URL is + /docs) The basic ones I’m interested in are these two. Let’s just focus only on ` /sdapi/v1/txt2img` When you expand that tab, it gives an example of a payload to send to the API. I used this often as reference. So that’s the backend. The API basically says what’s available, what it’s asking for, and where to send it. Now moving onto the frontend, I’ll start with constructing a payload with the parameters I want. An example can be: payload = { \"prompt\": \"maltese puppy\", \"steps\": 5 } I can put in as few or as many parameters as I want in the payload. The API will use the defaults for anything I don’t set. After that, I can send it to the API response = requests.post(url=f'http://127.0.0.1:7860/sdapi/v1/txt2img', json=payload) Again, this URL needs to match the web ui’s URL. If we execute this code, the web ui will generate an image based on the payload. That’s great, but then what? There is no image anywhere… After the backend does its thing, the API sends the response back in a variable that was assigned above: response. The response contains three entries; “images”, “parameters”, and “info”, and I have to find some way to get the information from these entries. First, I put this line r = response.json() to make it easier to work with the response. “images” is the generated image, which is what I want mostly. There’s no link or anything; it’s a giant string of random characters, apparently we have to decode it. This is how I do it: for i in r['images']: image = Image.open(io.BytesIO(base64.b64decode(i.split(\",\",1)[0]))) With that, we have an image in the image variable that we can work with, for example saving it with image.save('output.png'). “parameters” shows what was sent to the API, which could be useful, but what I want in this case is “info”. I use it to insert metadata into the image, so I can drop it into web ui PNG Info. For that, I can access the /sdapi/v1/png-info API. I’ll need to feed the image I got above into it. png_payload = { \"image\": \"data:image/png;base64,\" + i } response2 = requests.post(url=f'http://127.0.0.1:7860/sdapi/v1/png-info', json=png_payload) After that, I can get the information with response2.json().get(\"info\") A sample code that should work can look like this: import json import requests import io import base64 from PIL import Image, PngImagePlugin url = \"http://127.0.0.1:7860\" payload = { \"prompt\": \"puppy dog\", \"steps\": 5 } response = requests.post(url=f'{url}/sdapi/v1/txt2img', json=payload) r = response.json() for i in r['images']: image = Image.open(io.BytesIO(base64.b64decode(i.split(\",\",1)[0]))) png_payload = { \"image\": \"data:image/png;base64,\" + i } response2 = requests.post(url=f'{url}/sdapi/v1/png-info', json=png_payload) pnginfo = PngImagePlugin.PngInfo() pnginfo.add_text(\"parameters\", response2.json().get(\"info\")) image.save('output.png', pnginfo=pnginfo) Import the things I need define the url and the payload to send send said payload to said url through the API in a loop grab “images” and decode it for each image, send it to png info API and get that info back define a plugin to add png info, then add the png info I defined into it at the end here, save the image with the png info A note on \"override_settings\". The purpose of this endpoint is to override the web ui settings for a single request, such as the CLIP skip. The settings that can be passed into this parameter are visible here at the url’s /docs. You can expand the tab and the API will provide a list. There are a few ways you can add this value to your payload, but this is how I do it. I’ll demonstrate with “filter_nsfw”, and “CLIP_stop_at_last_layers”. payload = { \"prompt\": \"cirno\", \"steps\": 20 } override_settings = {} override_settings[\"filter_nsfw\"] = true override_settings[\"CLIP_stop_at_last_layers\"] = 2 override_payload = { \"override_settings\": override_settings } payload.update(override_payload) Have the normal payload after that, initialize a dictionary (I call it “override_settings”, but maybe not the best name) then I can add as many key:value pairs as I want to it make a new payload with just this parameter update the original payload to add this one to it So in this case, when I send the payload, I should get a “cirno” at 20 steps, with the CLIP skip at 2, as well as the NSFW filter on. For certain settings or situations, you may want your changes to stay. For that you can post to the /sdapi/v1/options API endpoint We can use what we learned so far and set up the code easily for this. Here is an example: url = \"http://127.0.0.1:7860\" option_payload = { \"sd_model_checkpoint\": \"Anything-V3.0-pruned.ckpt [2700c435]\", \"CLIP_stop_at_last_layers\": 2 } response = requests.post(url=f'{url}/sdapi/v1/options', json=option_payload) After sending this payload to the API, the model should swap to the one I set and set the CLIP skip to 2. Reiterating, this is different from “override_settings”, because this change will persist, while “override_settings” is for a single request. Note that if you’re changing the sd_model_checkpoint, the value should be the name of the checkpoint as it appears in the web ui. This can be referenced with this API endpoint (same way we reference “options” API) The “title” (name and hash) is what you want to use. This is as of commit 47a44c7 For a more complete implementation of a frontend, my Discord bot is here if anyone wants to look at it as an example. Most of the action happens in stablecog.py. There are many comments explaining what each code does. This guide can be found in discussions page. Also, check out this python API client library for webui: https://github.com/mix1009/sdwebuiapi"
  },"/sdwui-docs/install/": {
    "title": "Install",
    "keywords": "Getting started",
    "url": "/sdwui-docs/install/",
    "body": "Make sure the required dependencies are met and follow the instructions available for both NVidia (recommended) and AMD GPUs. Alternatively, use online services (like Google Colab): Online Services Automatic Installation on Windows Install Python 3.10.6, checking “Add Python to PATH” Install git. Download the stable-diffusion-webui repository, for example by running git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git. Place model.ckpt in the models directory (see dependencies for where to get it). *(Optional)* Place GFPGANv1.4.pth in the base directory, alongside webui.py (see dependencies for where to get it). Run webui-user.bat from Windows Explorer as normal, non-administrator, user. Automatic Installation on Linux Install the dependencies: # Debian-based: sudo apt install wget git python3 python3-venv # Red Hat-based: sudo dnf install wget git python3 # Arch-based: sudo pacman -S wget git python3 To install in /home/$(whoami)/stable-diffusion-webui/, run: bash &lt;(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh) Installation on Apple Silicon Find the instructions here."
  }}
