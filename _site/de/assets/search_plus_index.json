{"/sdwui-docs/categories/": {
    "title": "categories",
    "keywords": "Guides",
    "url": "/sdwui-docs/categories/",
    "body": "Guides categories Command line arguments and settings Contributing API guide Getting started Online services Stable Diffusion webUI Changing UI defaults Install"
  },"/sdwui-docs/pages/en/Command-Line-Arguments-and-Settings/": {
    "title": "Command line arguments and settings",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Command-Line-Arguments-and-Settings/",
    "body": "webui-user The recommended way to customize how the program is run is editing webui-user.bat (Windows) and webui-user.sh (Linux): set PYTHON allows for setting a custom Python path Example: set PYTHON=b:/soft/Python310/Python.exe set VENV_DIR allows you to choose the directory for the virtual environment. Default is venv. Special value - runs the script without creating virtual environment. Example: set VENV_DIR=C:\\run\\var\\run will create venv in the C:\\run\\var\\run directory. Example: set VENV_DIR=- runs the program using the system’s python set COMMANDLINE_ARGS setting the command line arguments webui.py is ran with Example: set COMMANDLINE_ARGS=--ckpt a.ckpt uses the model a.ckpt instead of model.ckpt Command Line Arguments Running online Use the --share option to run online. You will get a xxx.app.gradio link. This is the intended way to use the program in collabs. You may set up authentication for said gradio shared instance with the flag --gradio-auth username:password, optionally providing multiple sets of usernames and passwords separated by commas. Use --listen to make the server listen to network connections. This will allow computers on the local network to access the UI, and if you configure port forwarding, also computers on the internet. Use --port xxxx to make the server listen on a specific port, xxxx being the wanted port. Remember that all ports below 1024 need root/admin rights, for this reason it is advised to use a port above 1024. Defaults to port 7860 if available. All command line arguments Argument Command Value Default Description CONFIGURATION       -h, –help None False show this help message and exit –config CONFIG configs/stable-diffusion/v1-inference.yaml path to config which constructs model –ckpt CKPT model.ckpt path to checkpoint of stable diffusion model; if specified, this checkpoint will be added to the list of checkpoints and loaded –ckpt-dir CKPT_DIR None Path to directory with stable diffusion checkpoints –gfpgan-dir GFPGAN_DIR GFPGAN/ GFPGAN directory –gfpgan-model GFPGAN_MODEL GFPGAN model file name   –codeformer-models-path CODEFORMER_MODELS_PATH models/Codeformer/ Path to directory with codeformer model file(s). –gfpgan-models-path GFPGAN_MODELS_PATH models/GFPGAN Path to directory with GFPGAN model file(s). –esrgan-models-path ESRGAN_MODELS_PATH models/ESRGAN Path to directory with ESRGAN model file(s). –bsrgan-models-path BSRGAN_MODELS_PATH models/BSRGAN Path to directory with BSRGAN model file(s). –realesrgan-models-path REALESRGAN_MODELS_PATH models/RealESRGAN Path to directory with RealESRGAN model file(s). –scunet-models-path SCUNET_MODELS_PATH models/ScuNET Path to directory with ScuNET model file(s). –swinir-models-path SWINIR_MODELS_PATH models/SwinIR Path to directory with SwinIR and SwinIR v2 model file(s). –ldsr-models-path LDSR_MODELS_PATH models/LDSR Path to directory with LDSR model file(s). –clip-models-path CLIP_MODELS_PATH None Path to directory with CLIP model file(s). –vae-path VAE_PATH None Path to Variational Autoencoders model –embeddings-dir EMBEDDINGS_DIR embeddings/ embeddings directory for textual inversion (default: embeddings) –hypernetwork-dir HYPERNETWORK_DIR models/hypernetworks/ hypernetwork directory –localizations-dir LOCALIZATIONS_DIR localizations/ localizations directory –styles-file STYLES_FILE styles.csv filename to use for styles –ui-config-file UI_CONFIG_FILE ui-config.json filename to use for ui configuration –no-progressbar-hiding None False do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware acceleration in browser) –max-batch-count MAX_BATCH_COUNT 16 maximum batch count value for the UI –ui-settings-file UI_SETTINGS_FILE config.json filename to use for ui settings –allow-code None False allow custom script execution from webui –share None False use share=True for gradio and make the UI accessible through their site (doesn’t work for me but you might have better luck) –listen None False launch gradio with 0.0.0.0 as server name, allowing to respond to network requests –port PORT 7860 launch gradio with given server port, you need root/admin rights for ports &lt; 1024, defaults to 7860 if available –hide-ui-dir-config None False hide directory configuration from webui –freeze-settings None False disable editing settings –enable-insecure-extension-access None False enable extensions tab regardless of other options –gradio-debug None False launch gradio with –debug option –gradio-auth GRADIO_AUTH None set gradio authentication like “username:password”; or comma-delimit multiple like “u1:p1,u2:p2,u3:p3” –gradio-img2img-tool {color-sketch,editor} editor gradio image uploader tool: can be either editor for ctopping, or color-sketch for drawing –disable-console-progressbars None False do not output progressbars to console –enable-console-prompts None False print prompts to console when generating with txt2img and img2img –api None False launch webui with API –nowebui None False only launch the API, without the UI –ui-debug-mode None False Don’t load model to quickly launch UI –device-id DEVICE_ID None Select the default CUDA device to use (export CUDA_VISIBLE_DEVICES=0,1,etc might be needed before) –administrator None False Administrator rights PERFORMANCE       –xformers None False enable xformers for cross attention layers –reinstall-xformers None False force reinstall xformers. Useful for upgrading - but remove it after upgrading or you’ll reinstall xformers perpetually. –force-enable-xformers None False enable xformers for cross attention layers regardless of whether the checking code thinks you can run it; do not make bug reports if this fails to work –opt-split-attention None False force-enables Doggettx’s cross-attention layer optimization. By default, it’s on for cuda enabled systems. –opt-split-attention-invokeai None False force-enables InvokeAI’s cross-attention layer optimization. By default, it’s on when cuda is unavailable. –opt-split-attention-v1 None False enable older version of split attention optimization that does not consume all the VRAM it can find –opt-channelslast None False change memory type for stable diffusion to channels last –disable-opt-split-attention None False force-disables cross-attention layer optimization –use-cpu {all, sd, interrogate, gfpgan, bsrgan, esrgan, scunet, codeformer} None use CPU as torch device for specified modules –no-half None False do not switch the model to 16-bit floats –precision {full,autocast} autocast evaluate at this precision –no-half-vae None False do not switch the VAE model to 16-bit floats –medvram None False enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage –lowvram None False enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage –lowram None False load stable diffusion checkpoint weights to VRAM instead of RAM –always-batch-cond-uncond None False disables cond/uncond batching that is enabled to save memory with –medvram or –lowvram FEATURES       –autolaunch None False open the webui URL in the system’s default browser upon launch –theme None Unset open the webui with the specified theme (“light” or “dark”). If not specified, uses the default browser theme –use-textbox-seed None False use textbox for seeds in UI (no up/down, but possible to input long seeds) –disable-safe-unpickle None False disable checking pytorch models for malicious code –ngrok NGROK Unset ngrok authtoken, alternative to gradio –share –ngrok-region NGROK_REGION Unset The region in which ngrok should start. DEFUNCT OPTIONS       –show-negative-prompt None False does not do anything –deepdanbooru None False does not do anything –unload-gfpgan None False does not do anything."
  },"/sdwui-docs/pages/en/Contributing/": {
    "title": "Contributing",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Contributing/",
    "body": "Pull requests To contribute, clone the repository, make your changes, commit and push to your clone, and submit a pull request. Make sure that your changes do not break anything by running tests. If you’re adding a lot of code, consider making your contribution an extension, and only PR small changes you need in main code to make the extension possible. If you are making changes to used libraries or the installation script, you must verify them to work on default Windows installation from scratch. If you cannot test if it works (due to your OS or anything else), do not make those changes (with possible exception of changes that explicitly are guarded from being executed on Windows by ifs or something else). Code style I mostly follow code style suggested by PyCharm, with the exception of disabled line length limit. Please do not submit PRs where you just take existing lines and reformat them without changing what they do. Gradio Gradio at some point wanted to add this section to shill their project in the contributing section, which I didn’t have at the time, so here it is now. For Gradio check out the docs to contribute: Have an issue or feature request with Gradio? open a issue/feature request on github for support: https://github.com/gradio-app/gradio/issues"
  },"/sdwui-docs/online-services/": {
    "title": "Online services",
    "keywords": "Getting started",
    "url": "/sdwui-docs/online-services/",
    "body": "Google Colab maintained by TheLastBen maintained by camenduru maintained by ddPn08 maintained by Akaibu Colab, original by me, outdated. Paperspace maintained by Cyberes Kaggle maintained by camenduru SageMaker Studio Lab maintained by fractality Hugging Face maintained by camenduru Installation Guides azure-ml - (commit)"
  },"/sdwui-docs/": {
    "title": "Stable Diffusion webUI",
    "keywords": "Getting started",
    "url": "/sdwui-docs/",
    "body": "A browser interface based on Gradio library for Stable Diffusion. Check the custom scripts wiki page for extra scripts developed by users. Features Detailed feature showcase with images: Original txt2img and img2img modes One click install and run script (but you still must install python and git) Outpainting Inpainting Color Sketch Prompt Matrix Stable Diffusion Upscale Attention, specify parts of text that the model should pay more attention to a man in a ((tuxedo)) - will pay more attention to tuxedo a man in a (tuxedo:1.21) - alternative syntax select text and press ctrl+up or ctrl+down to automatically adjust attention to selected text (code contributed by anonymous user) Loopback, run img2img processing multiple times X/Y plot, a way to draw a 2 dimensional plot of images with different parameters Textual Inversion have as many embeddings as you want and use any names you like for them use multiple embeddings with different numbers of vectors per token works with half precision floating point numbers train embeddings on 8GB (also reports of 6GB working) Extras tab with: GFPGAN, neural network that fixes faces CodeFormer, face restoration tool as an alternative to GFPGAN RealESRGAN, neural network upscaler ESRGAN, neural network upscaler with a lot of third party models SwinIR and Swin2SR(see here), neural network upscalers LDSR, Latent diffusion super resolution upscaling Resizing aspect ratio options Sampling method selection Adjust sampler eta values (noise multiplier) More advanced noise setting options Interrupt processing at any time 4GB video card support (also reports of 2GB working) Correct seeds for batches Live prompt token length validation Generation parameters parameters you used to generate images are saved with that image in PNG chunks for PNG, in EXIF for JPEG can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI can be disabled in settings drag and drop an image/text-parameters to promptbox Read Generation Parameters Button, loads parameters in promptbox to UI Settings page Running arbitrary python code from UI (must run with –allow-code to enable) Mouseover hints for most UI elements Possible to change defaults/mix/max/step values for UI elements via text config Random artist button Tiling support, a checkbox to create images that can be tiled like textures Progress bar and live image generation preview Negative prompt, an extra text field that allows you to list what you don’t want to see in generated image Styles, a way to save part of prompt and easily apply them via dropdown later Variations, a way to generate same image but with tiny differences Seed resizing, a way to generate same image but at slightly different resolution CLIP interrogator, a button that tries to guess prompt from an image Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway Batch Processing, process a group of files using img2img Img2img Alternative, reverse Euler method of cross attention control Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions Reloading checkpoints on the fly Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one Custom scripts with many extensions from community Composable-Diffusion, a way to use multiple prompts at once separate prompts using uppercase AND also supports weights for prompts: a cat :1.2 AND a dog AND a penguin :2.2 No token limit for prompts (original stable diffusion lets you use up to 75 tokens) DeepDanbooru integration, creates danbooru style tags for anime prompts xformers, major speed increase for select cards: (add –xformers to commandline args) via extension: History tab: view, direct and delete images conveniently within the UI Generate forever option Training tab hypernetworks and embeddings options Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime) Clip skip Use Hypernetworks Use VAEs Estimated completion time in progress bar API Support for dedicated inpainting model by RunwayML. via extension: Aesthetic Gradients, a way to generate images with a specific aesthetic by using clip images embeds (implementation of https://github.com/vicgalle/stable-diffusion-aesthetic-gradients) Stable Diffusion 2.0 support - see wiki for instructions Installation and Running Make sure the required dependencies are met and follow the instructions available for both NVidia (recommended) and AMD GPUs. Alternatively, use online services (like Google Colab): List of Online Services Automatic Installation on Windows Install Python 3.10.6, checking “Add Python to PATH” Install git. Download the stable-diffusion-webui repository, for example by running git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git. Place model.ckpt in the models directory (see dependencies for where to get it). *(Optional)* Place GFPGANv1.4.pth in the base directory, alongside webui.py (see dependencies for where to get it). Run webui-user.bat from Windows Explorer as normal, non-administrator, user. Automatic Installation on Linux Install the dependencies: # Debian-based: sudo apt install wget git python3 python3-venv # Red Hat-based: sudo dnf install wget git python3 # Arch-based: sudo pacman -S wget git python3 To install in /home/$(whoami)/stable-diffusion-webui/, run: bash &lt;(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh) Installation on Apple Silicon Find the instructions here. Contributing Here’s how to add code to this repo: Contributing Documentation The documentation was moved from this README over to the project’s wiki. Credits Stable Diffusion - https://github.com/CompVis/stable-diffusion, https://github.com/CompVis/taming-transformers k-diffusion - https://github.com/crowsonkb/k-diffusion.git GFPGAN - https://github.com/TencentARC/GFPGAN.git CodeFormer - https://github.com/sczhou/CodeFormer ESRGAN - https://github.com/xinntao/ESRGAN SwinIR - https://github.com/JingyunLiang/SwinIR Swin2SR - https://github.com/mv-lab/swin2sr LDSR - https://github.com/Hafiidz/latent-diffusion MiDaS - https://github.com/isl-org/MiDaS Ideas for optimizations - https://github.com/basujindal/stable-diffusion Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing. Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion) Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we’re not using his code, but we are using his ideas). Idea for SD upscale - https://github.com/jquesnelle/txt2imghd Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch xformers - https://github.com/facebookresearch/xformers DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru Security advice - RyotaK Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user. (You)"
  },"/sdwui-docs/ui-defaults/": {
    "title": "Changing UI defaults",
    "keywords": "Getting started",
    "url": "/sdwui-docs/ui-defaults/",
    "body": "The default values in the web UI can be changed by editing ui-config.json, which appears in the base directory containing webui.py after the first run. The changes are only applied after restarting. { \"txt2img/Sampling Steps/value\": 20, \"txt2img/Sampling Steps/minimum\": 1, \"txt2img/Sampling Steps/maximum\": 150, \"txt2img/Sampling Steps/step\": 1, \"txt2img/Batch count/value\": 1, \"txt2img/Batch count/minimum\": 1, \"txt2img/Batch count/maximum\": 32, \"txt2img/Batch count/step\": 1, \"txt2img/Batch size/value\": 1, \"txt2img/Batch size/minimum\": 1, # ... }"
  },"/sdwui-docs/api/": {
    "title": "API guide",
    "keywords": "Guides",
    "url": "/sdwui-docs/api/",
    "body": "ENGLISH First, of course, is to run web ui with --api commandline argument example in your “webui-user.bat”: set COMMANDLINE_ARGS=--api This enables the api which can be reviewed at http://127.0.0.1:7860/docs (or whever the URL is + /docs) The basic ones I’m interested in are these two. Let’s just focus only on ` /sdapi/v1/txt2img` When you expand that tab, it gives an example of a payload to send to the API. I used this often as reference. So that’s the backend. The API basically says what’s available, what it’s asking for, and where to send it. Now moving onto the frontend, I’ll start with constructing a payload with the parameters I want. An example can be: payload = { \"prompt\": \"maltese puppy\", \"steps\": 5 } I can put in as few or as many parameters as I want in the payload. The API will use the defaults for anything I don’t set. After that, I can send it to the API response = requests.post(url=f'http://127.0.0.1:7860/sdapi/v1/txt2img', json=payload) Again, this URL needs to match the web ui’s URL. If we execute this code, the web ui will generate an image based on the payload. That’s great, but then what? There is no image anywhere… After the backend does its thing, the API sends the response back in a variable that was assigned above: response. The response contains three entries; “images”, “parameters”, and “info”, and I have to find some way to get the information from these entries. First, I put this line r = response.json() to make it easier to work with the response. “images” is the generated image, which is what I want mostly. There’s no link or anything; it’s a giant string of random characters, apparently we have to decode it. This is how I do it: for i in r['images']: image = Image.open(io.BytesIO(base64.b64decode(i.split(\",\",1)[0]))) With that, we have an image in the image variable that we can work with, for example saving it with image.save('output.png'). “parameters” shows what was sent to the API, which could be useful, but what I want in this case is “info”. I use it to insert metadata into the image, so I can drop it into web ui PNG Info. For that, I can access the /sdapi/v1/png-info API. I’ll need to feed the image I got above into it. png_payload = { \"image\": \"data:image/png;base64,\" + i } response2 = requests.post(url=f'http://127.0.0.1:7860/sdapi/v1/png-info', json=png_payload) After that, I can get the information with response2.json().get(\"info\") A sample code that should work can look like this: import json import requests import io import base64 from PIL import Image, PngImagePlugin url = \"http://127.0.0.1:7860\" payload = { \"prompt\": \"puppy dog\", \"steps\": 5 } response = requests.post(url=f'{url}/sdapi/v1/txt2img', json=payload) r = response.json() for i in r['images']: image = Image.open(io.BytesIO(base64.b64decode(i.split(\",\",1)[0]))) png_payload = { \"image\": \"data:image/png;base64,\" + i } response2 = requests.post(url=f'{url}/sdapi/v1/png-info', json=png_payload) pnginfo = PngImagePlugin.PngInfo() pnginfo.add_text(\"parameters\", response2.json().get(\"info\")) image.save('output.png', pnginfo=pnginfo) Import the things I need define the url and the payload to send send said payload to said url through the API in a loop grab “images” and decode it for each image, send it to png info API and get that info back define a plugin to add png info, then add the png info I defined into it at the end here, save the image with the png info A note on \"override_settings\". The purpose of this endpoint is to override the web ui settings for a single request, such as the CLIP skip. The settings that can be passed into this parameter are visible here at the url’s /docs. You can expand the tab and the API will provide a list. There are a few ways you can add this value to your payload, but this is how I do it. I’ll demonstrate with “filter_nsfw”, and “CLIP_stop_at_last_layers”. payload = { \"prompt\": \"cirno\", \"steps\": 20 } override_settings = {} override_settings[\"filter_nsfw\"] = true override_settings[\"CLIP_stop_at_last_layers\"] = 2 override_payload = { \"override_settings\": override_settings } payload.update(override_payload) Have the normal payload after that, initialize a dictionary (I call it “override_settings”, but maybe not the best name) then I can add as many key:value pairs as I want to it make a new payload with just this parameter update the original payload to add this one to it So in this case, when I send the payload, I should get a “cirno” at 20 steps, with the CLIP skip at 2, as well as the NSFW filter on. For certain settings or situations, you may want your changes to stay. For that you can post to the /sdapi/v1/options API endpoint We can use what we learned so far and set up the code easily for this. Here is an example: url = \"http://127.0.0.1:7860\" option_payload = { \"sd_model_checkpoint\": \"Anything-V3.0-pruned.ckpt [2700c435]\", \"CLIP_stop_at_last_layers\": 2 } response = requests.post(url=f'{url}/sdapi/v1/options', json=option_payload) After sending this payload to the API, the model should swap to the one I set and set the CLIP skip to 2. Reiterating, this is different from “override_settings”, because this change will persist, while “override_settings” is for a single request. Note that if you’re changing the sd_model_checkpoint, the value should be the name of the checkpoint as it appears in the web ui. This can be referenced with this API endpoint (same way we reference “options” API) The “title” (name and hash) is what you want to use. This is as of commit 47a44c7 For a more complete implementation of a frontend, my Discord bot is here if anyone wants to look at it as an example. Most of the action happens in stablecog.py. There are many comments explaining what each code does. This guide can be found in discussions page. Also, check out this python API client library for webui: https://github.com/mix1009/sdwebuiapi"
  },"/sdwui-docs/install/": {
    "title": "Install",
    "keywords": "Getting started",
    "url": "/sdwui-docs/install/",
    "body": "Make sure the required dependencies are met and follow the instructions available for both NVidia (recommended) and AMD GPUs. Alternatively, use online services (like Google Colab): Online Services Automatic Installation on Windows Install Python 3.10.6, checking “Add Python to PATH” Install git. Download the stable-diffusion-webui repository, for example by running git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git. Place model.ckpt in the models directory (see dependencies for where to get it). *(Optional)* Place GFPGANv1.4.pth in the base directory, alongside webui.py (see dependencies for where to get it). Run webui-user.bat from Windows Explorer as normal, non-administrator, user. Automatic Installation on Linux Install the dependencies: # Debian-based: sudo apt install wget git python3 python3-venv # Red Hat-based: sudo dnf install wget git python3 # Arch-based: sudo pacman -S wget git python3 To install in /home/$(whoami)/stable-diffusion-webui/, run: bash &lt;(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh) Installation on Apple Silicon Find the instructions here."
  }}
