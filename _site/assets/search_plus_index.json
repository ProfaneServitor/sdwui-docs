{"/sdwui-docs/pages/en/Command-Line-Arguments-and-Settings/": {
    "title": "Command line arguments and settings",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Command-Line-Arguments-and-Settings/",
    "body": "webui-user The recommended way to customize how the program is run is editing webui-user.bat (Windows) and webui-user.sh (Linux): set PYTHON allows for setting a custom Python path Example: set PYTHON=b:/soft/Python310/Python.exe set VENV_DIR allows you to choose the directory for the virtual environment. Default is venv. Special value - runs the script without creating virtual environment. Example: set VENV_DIR=C:\\run\\var\\run will create venv in the C:\\run\\var\\run directory. Example: set VENV_DIR=- runs the program using the systemâ€™s python set COMMANDLINE_ARGS setting the command line arguments webui.py is ran with Example: set COMMANDLINE_ARGS=--ckpt a.ckpt uses the model a.ckpt instead of model.ckpt Command Line Arguments Running online Use the --share option to run online. You will get a xxx.app.gradio link. This is the intended way to use the program in collabs. You may set up authentication for said gradio shared instance with the flag --gradio-auth username:password, optionally providing multiple sets of usernames and passwords separated by commas. Use --listen to make the server listen to network connections. This will allow computers on the local network to access the UI, and if you configure port forwarding, also computers on the internet. Use --port xxxx to make the server listen on a specific port, xxxx being the wanted port. Remember that all ports below 1024 need root/admin rights, for this reason it is advised to use a port above 1024. Defaults to port 7860 if available. All command line arguments Argument Command Value Default Description CONFIGURATION Â  Â  Â  -h, â€“help None False show this help message and exit â€“config CONFIG configs/stable-diffusion/v1-inference.yaml path to config which constructs model â€“ckpt CKPT model.ckpt path to checkpoint of stable diffusion model; if specified, this checkpoint will be added to the list of checkpoints and loaded â€“ckpt-dir CKPT_DIR None Path to directory with stable diffusion checkpoints â€“gfpgan-dir GFPGAN_DIR GFPGAN/ GFPGAN directory â€“gfpgan-model GFPGAN_MODEL GFPGAN model file name Â  â€“codeformer-models-path CODEFORMER_MODELS_PATH models/Codeformer/ Path to directory with codeformer model file(s). â€“gfpgan-models-path GFPGAN_MODELS_PATH models/GFPGAN Path to directory with GFPGAN model file(s). â€“esrgan-models-path ESRGAN_MODELS_PATH models/ESRGAN Path to directory with ESRGAN model file(s). â€“bsrgan-models-path BSRGAN_MODELS_PATH models/BSRGAN Path to directory with BSRGAN model file(s). â€“realesrgan-models-path REALESRGAN_MODELS_PATH models/RealESRGAN Path to directory with RealESRGAN model file(s). â€“scunet-models-path SCUNET_MODELS_PATH models/ScuNET Path to directory with ScuNET model file(s). â€“swinir-models-path SWINIR_MODELS_PATH models/SwinIR Path to directory with SwinIR and SwinIR v2 model file(s). â€“ldsr-models-path LDSR_MODELS_PATH models/LDSR Path to directory with LDSR model file(s). â€“clip-models-path CLIP_MODELS_PATH None Path to directory with CLIP model file(s). â€“vae-path VAE_PATH None Path to Variational Autoencoders model â€“embeddings-dir EMBEDDINGS_DIR embeddings/ embeddings directory for textual inversion (default: embeddings) â€“hypernetwork-dir HYPERNETWORK_DIR models/hypernetworks/ hypernetwork directory â€“localizations-dir LOCALIZATIONS_DIR localizations/ localizations directory â€“styles-file STYLES_FILE styles.csv filename to use for styles â€“ui-config-file UI_CONFIG_FILE ui-config.json filename to use for ui configuration â€“no-progressbar-hiding None False do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware acceleration in browser) â€“max-batch-count MAX_BATCH_COUNT 16 maximum batch count value for the UI â€“ui-settings-file UI_SETTINGS_FILE config.json filename to use for ui settings â€“allow-code None False allow custom script execution from webui â€“share None False use share=True for gradio and make the UI accessible through their site (doesnâ€™t work for me but you might have better luck) â€“listen None False launch gradio with 0.0.0.0 as server name, allowing to respond to network requests â€“port PORT 7860 launch gradio with given server port, you need root/admin rights for ports &lt; 1024, defaults to 7860 if available â€“hide-ui-dir-config None False hide directory configuration from webui â€“freeze-settings None False disable editing settings â€“enable-insecure-extension-access None False enable extensions tab regardless of other options â€“gradio-debug None False launch gradio with â€“debug option â€“gradio-auth GRADIO_AUTH None set gradio authentication like â€œusername:passwordâ€; or comma-delimit multiple like â€œu1:p1,u2:p2,u3:p3â€ â€“gradio-img2img-tool {color-sketch,editor} editor gradio image uploader tool: can be either editor for ctopping, or color-sketch for drawing â€“disable-console-progressbars None False do not output progressbars to console â€“enable-console-prompts None False print prompts to console when generating with txt2img and img2img â€“api None False launch webui with API â€“nowebui None False only launch the API, without the UI â€“ui-debug-mode None False Donâ€™t load model to quickly launch UI â€“device-id DEVICE_ID None Select the default CUDA device to use (export CUDA_VISIBLE_DEVICES=0,1,etc might be needed before) â€“administrator None False Administrator rights PERFORMANCE Â  Â  Â  â€“xformers None False enable xformers for cross attention layers â€“reinstall-xformers None False force reinstall xformers. Useful for upgrading - but remove it after upgrading or youâ€™ll reinstall xformers perpetually. â€“force-enable-xformers None False enable xformers for cross attention layers regardless of whether the checking code thinks you can run it; do not make bug reports if this fails to work â€“opt-split-attention None False force-enables Doggettxâ€™s cross-attention layer optimization. By default, itâ€™s on for cuda enabled systems. â€“opt-split-attention-invokeai None False force-enables InvokeAIâ€™s cross-attention layer optimization. By default, itâ€™s on when cuda is unavailable. â€“opt-split-attention-v1 None False enable older version of split attention optimization that does not consume all the VRAM it can find â€“opt-channelslast None False change memory type for stable diffusion to channels last â€“disable-opt-split-attention None False force-disables cross-attention layer optimization â€“use-cpu {all, sd, interrogate, gfpgan, bsrgan, esrgan, scunet, codeformer} None use CPU as torch device for specified modules â€“no-half None False do not switch the model to 16-bit floats â€“precision {full,autocast} autocast evaluate at this precision â€“no-half-vae None False do not switch the VAE model to 16-bit floats â€“medvram None False enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage â€“lowvram None False enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage â€“lowram None False load stable diffusion checkpoint weights to VRAM instead of RAM â€“always-batch-cond-uncond None False disables cond/uncond batching that is enabled to save memory with â€“medvram or â€“lowvram FEATURES Â  Â  Â  â€“autolaunch None False open the webui URL in the systemâ€™s default browser upon launch â€“theme None Unset open the webui with the specified theme (â€œlightâ€ or â€œdarkâ€). If not specified, uses the default browser theme â€“use-textbox-seed None False use textbox for seeds in UI (no up/down, but possible to input long seeds) â€“disable-safe-unpickle None False disable checking pytorch models for malicious code â€“ngrok NGROK Unset ngrok authtoken, alternative to gradio â€“share â€“ngrok-region NGROK_REGION Unset The region in which ngrok should start. DEFUNCT OPTIONS Â  Â  Â  â€“show-negative-prompt None False does not do anything â€“deepdanbooru None False does not do anything â€“unload-gfpgan None False does not do anything."
  },"/sdwui-docs/pages/en/Contributing/": {
    "title": "Contributing",
    "keywords": "development",
    "url": "/sdwui-docs/pages/en/Contributing/",
    "body": "features Pull requests To contribute, clone the repository, make your changes, commit and push to your clone, and submit a pull request. Make sure that your changes do not break anything by running tests. If youâ€™re adding a lot of code, consider making your contribution an extension, and only PR small changes you need in main code to make the extension possible. If you are making changes to used libraries or the installation script, you must verify them to work on default Windows installation from scratch. If you cannot test if it works (due to your OS or anything else), do not make those changes (with possible exception of changes that explicitly are guarded from being executed on Windows by ifs or something else). Code style I mostly follow code style suggested by PyCharm, with the exception of disabled line length limit. Please do not submit PRs where you just take existing lines and reformat them without changing what they do. Gradio Gradio at some point wanted to add this section to shill their project in the contributing section, which I didnâ€™t have at the time, so here it is now. For Gradio check out the docs to contribute: Have an issue or feature request with Gradio? open a issue/feature request on github for support: https://github.com/gradio-app/gradio/issues"
  },"/sdwui-docs/pages/en/Custom-Images-Filename-Name-and-Subdirectory/": {
    "title": "Custom images filename and subdirectory",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Custom-Images-Filename-Name-and-Subdirectory/",
    "body": "the following information is about the image filename and subdirectory name, not the Paths for saving \\ Output directorys By default, the Wub UI save images in the output directorys with a filename structure of number-seed-[prompt_spaces] 01234-987654321-((masterpiece)), ((best quality)), ((illustration)), extremely detailed,style girl.png A different image filename and optional subdirectory can be used if a user wishes. Image filename pattern can be configured under. settings tab &gt; Saving images/grids &gt; Images filename pattern Subdirectory can be configured under settings. settings tab &gt; Saving to a directory &gt; Directory name pattern Pattens Web-Ui provides several patterns that can be used as placeholders for inserting information into the filename or subdirectory, user can chain these patterns togetherm forming a filename that suits their use case. | Pattern | Description | Example | |â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“|â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”|â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”| | [seed] | Seed | 1234567890 | | [steps] | Steps | 20 | | [cfg] | CFG scale | 7 | | [sampler] | Sampling method | Euler a | | [model_hash] | Hash of the model | 7460a6fa | | [width] | Image width | 512 | | [height] | Image hight | 512 | | [styles] | Name of the chosen Styles | my style name | | [date] | Date of the computer in ISO format | 2022-10-24 | | [datetime] | Datetime in â€œ%Y%m%d%H%M%Sâ€ | 20221025013106 | | [datetime&lt;Format&gt;] | Datetime in specified &lt;Format&gt; | [datetime&lt;%Y%m%d_%H%M%S_%f&gt;]20221025014350_733877 | | [datetime&lt;Format&gt;&lt;TimeZone&gt;] | Datetime at specific &lt;Time Zone&gt; in specified &lt;Format&gt; | [datetime&lt;%Y%m%d%H%M%S_%f&gt;&lt;Asia/Tokyo&gt;]&lt;br&gt;20221025_014350_733877 | | [prompt_no_styles] | Prompt without Styles | 1gir, white space, ((very important)), [not important], (some value_1.5), (whatever), the end&lt;br&gt; | | [prompt_spaces] | Prompt with Styles | 1gir, white space, ((very important)), [not important], (some value_1.5), (whatever), the end&lt;br&gt;, (((crystals texture Hair)))ï¼Œ((( | | [prompt] | Prompt with Styles, Space bar replaced with_ | 1gir,\\_\\_\\_white_space,\\_((very\\_important)),\\_[not\\_important],\\_(some\\_value\\_1.5),\\_(whatever),\\_the\\_end,\\_(((crystals_texture_Hair)))ï¼Œ((( | | [prompt_words]` | Prompt with Styles, Bracket and Comma removed | 1gir white space very important not important some value 1 5 whatever the end crystals texture Hair ï¼Œ extremely detailed | Datetime Formatting details Reference python documentation for more details on Format Codes Datetime Time Zone details Reference List of Time Zones for a list of valid time zones If &lt;Format&gt; is blank or invalid, it will use the default time format â€œ%Y%m%d%H%M%Sâ€ tip: you can use extra characters inside &lt;Format&gt; for punctuation, such as _ - If &lt;TimeZone&gt; is blank or invalid, it will use the default system time zone The Prompts and Style used for the above [prompt] examples Prompt: 1gir, white space, ((very important)), [not important], (some value:1.5), (whatever), the end Selected Styles: (((crystals texture Hair)))ï¼Œ(((((extremely detailed CG))))),((8k_wallpaper)) note: the Styles mentioned above is referring to the two drop down menu below the generate button if the Prompts is too long, it will be short this is due to Computer has a maximum file length Add / Remove number to filename when saving you can remove the prefix number by unchecking the checkbox under Setting &gt; Saving images/grids &gt; Add number to filename when saving with prefix number 00123-`987654321-((masterpiece)).png without prefix number 987654321-((masterpiece)).png Caution The purpose of the prefix number is to ensure that the saved image file name is Unique. If you decide to not use the prefix number, make sure that your pattern will generate a unique file name, Otherwise files might be Overwritten. Generally datetime down to seconds should be able to guarantee that file name is unique. [datetime&lt;%Y%m%d_%H%M%S&gt;]-[seed] 20221025_014350-281391998.png But with some Custom Scripts might generate multiples images using the same seed in a single batch, in this case it is safer to also use %f for Microsecond as a decimal number, zero-padded to 6 digits. [datetime&lt;%Y%m%d_%H%M%S_%f&gt;]-[seed] 20221025_014350_733877-281391998.png Filename Pattern Examples If youâ€™re running Web-Ui on multiple machines, say on Google Colab and your own Computer, you might want to use a filename with a time as the Prefix. this is so that when you download the fouls you can put them in the same folde. Also since you donâ€™t know what time zone Google Colab is using, you would want to specify the time zone. [datetime&lt;%Y%m%d_%H%M%S_%f&gt;&lt;Asia/Tokyo&gt;]-[seed]-[prompt_words] 20221025_032649_058536-3822510847-1girl.png It might also be useful to set Subdirectory the date, so that one folder doesnâ€™t have too many images [datetime&lt;%Y-%m-%d&gt;&lt;Asia/Tokyo&gt;] 2022-10-25"
  },"/sdwui-docs/pages/en/Custom-Scripts/": {
    "title": "Custom scripts",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Custom-Scripts/",
    "body": "Installing and Using Custom Scripts To install custom scripts, place them into the scripts directory and click the Reload custom script button at the bottom in the settings tab. Custom scripts will appear in the lower-left dropdown menu on the txt2img and img2img tabs after being installed. Below are some notable custom scripts created by Web UI users: Custom Scripts from Users Improved prompt matrix https://github.com/ArrowM/auto1111-improved-prompt-matrix This script is advanced-prompt-matrix modified to support batch count. Grids are not created. Usage: Use &lt; &gt; to create a group of alternate texts. Separate text options with |. Multiple groups and multiple options can be used. For example: An input of a &lt;corgi|cat&gt; wearing &lt;goggles|a hat&gt; Will output 4 prompts: a corgi wearing goggles, a corgi wearing a hat, a cat wearing goggles, a cat wearing a hat When using a batch count &gt; 1, each prompt variation will be generated for each seed. batch size is ignored. txt2img2img https://github.com/ThereforeGames/txt2img2img Greatly improve the editability of any character/subject while retaining their likeness. The main motivation for this script is improving the editability of embeddings created through Textual Inversion. (be careful with cloning as it has a bit of venv checked in) Example: (Click to expand:) txt2mask https://github.com/ThereforeGames/txt2mask Allows you to specify an inpainting mask with text, as opposed to the brush. Example: (Click to expand:) Mask drawing UI https://github.com/dfaker/stable-diffusion-webui-cv2-external-masking-script Provides a local popup window powered by CV2 that allows addition of a mask before processing. Example: (Click to expand:) Img2img Video https://github.com/memes-forever/Stable-diffusion-webui-video Using img2img, generates pictures one after another. Seed Travel https://github.com/yownas/seed_travel Pick two (or more) seeds and generate a sequence of images interpolating between them. Optionally, let it create a video of the result. Example of what you can do with it: https://www.youtube.com/watch?v=4c71iUclY4U Another example by a user: Advanced Seed Blending https://github.com/amotile/stable-diffusion-backend/tree/master/src/process/implementations/automatic1111_scripts This script allows you to base the initial noise on multiple weighted seeds. Ex. seed1:2, seed2:1, seed3:1 The weights are normalized so you can use bigger once like above, or you can do floating point numbers: Ex. seed1:0.5, seed2:0.25, seed3:0.25 Prompt Blending https://github.com/amotile/stable-diffusion-backend/tree/master/src/process/implementations/automatic1111_scripts This script allows you to combine multiple weighted prompts together by mathematically combining their textual embeddings before generating the image. Ex. Crystal containing elemental {fire|ice} It supports nested definitions so you can do this as well: Crystal containing elemental {{fire:5|ice}|earth} Animator https://github.com/Animator-Anon/Animator A basic img2img script that will dump frames and build a video file. Suitable for creating interesting zoom in warping movies, but not too much else at this time. Parameter Sequencer https://github.com/rewbs/sd-parseq Generate videos with tight control and flexible interpolation over many Stable Diffusion parameters (such as seed, scale, prompt weights, denoising strengthâ€¦), as well as input processing parameter (such as zoom, pan, 3D rotationâ€¦) Alternate Noise Schedules https://gist.github.com/dfaker/f88aa62e3a14b559fe4e5f6b345db664 Uses alternate generators for the samplerâ€™s sigma schedule. Allows access to Karras, Exponential and Variance Preserving schedules from crowsonkb/k-diffusion along with their parameters. Vid2Vid https://github.com/Filarius/stable-diffusion-webui/blob/master/scripts/vid2vid.py From real video, img2img the frames and stitch them together. Does not unpack frames to hard drive. Txt2VectorGraphics https://github.com/GeorgLegato/Txt2Vectorgraphics Create custom, scaleable icons from your prompts as SVG or PDF. Example: (Click to expand:) | prompt |PNG |SVG | | :-------- | :-----------------: | :---------------------: | | Happy Einstein | | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193370379-2680aa2a-f460-44e7-9c4e-592cf096de71.svg\" width=30%/&gt; | | Mountainbike Downhill | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193371353-f0f5ff6f-12f7-423b-a481-f9bd119631dd.png\" width=40%/&gt; | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193371585-68dea4ca-6c1a-4d31-965d-c1b5f145bb6f.svg\" width=30%/&gt; | coffe mug in shape of a heart | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193374299-98379ca1-3106-4ceb-bcd3-fa129e30817a.png\" width=40%/&gt; | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193374525-460395af-9588-476e-bcf6-6a8ad426be8e.svg\" width=30%/&gt; | | Headphones | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193376238-5c4d4a8f-1f06-4ba4-b780-d2fa2e794eda.png\" width=40%/&gt; | &lt;img src=\"https://user-images.githubusercontent.com/7210708/193376255-80e25271-6313-4bff-a98e-ba3ae48538ca.svg\" width=30%/&gt; | Shift Attention https://github.com/yownas/shift-attention Generate a sequence of images shifting attention in the prompt. This script enables you to give a range to the weight of tokens in a prompt and then generate a sequence of images stepping from the first one to the second. Loopback and Superimpose https://github.com/DiceOwl/StableDiffusionStuff https://github.com/DiceOwl/StableDiffusionStuff/blob/main/loopback_superimpose.py Mixes output of img2img with original input image at strength alpha. The result is fed into img2img again (at loop&gt;=2), and this procedure repeats. Tends to sharpen the image, improve consistency, reduce creativity and reduce fine detail. Interpolate https://github.com/DiceOwl/StableDiffusionStuff https://github.com/DiceOwl/StableDiffusionStuff/blob/main/interpolate.py An img2img script to produce in-between images. Allows two input images for interpolation. More features shown in the readme. Run n times https://gist.github.com/camenduru/9ec5f8141db9902e375967e93250860f Run n times with random seed. Advanced Loopback https://github.com/Extraltodeus/advanced-loopback-for-sd-webui Dynamic zoom loopback with parameters variations and prompt switching amongst other features! prompt-morph https://github.com/feffy380/prompt-morph Generate morph sequences with Stable Diffusion. Interpolate between two or more prompts and create an image at each step. Uses the new AND keyword and can optionally export the sequence as a video. prompt interpolation https://github.com/EugeoSynthesisThirtyTwo/prompt-interpolation-script-for-sd-webui With this script, you can interpolate between two prompts (using the â€œANDâ€ keyword), generate as many images as you want. You can also generate a gif with the result. Works for both txt2img and img2img. Example: (Click to expand:) ![gif](https://user-images.githubusercontent.com/24735555/195470874-afc3dfdc-7b35-4b23-9c34-5888a4100ac1.gif) Asymmetric Tiling https://github.com/tjm35/asymmetric-tiling-sd-webui/ Control horizontal/vertical seamless tiling independently of each other. Example: (Click to expand:) Force Symmetry https://gist.github.com/1ort/2fe6214cf1abe4c07087aac8d91d0d8a see https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/2441 applies symmetry to the image every n steps and sends the result further to img2img. Example: (Click to expand:) SD-latent-mirroring https://github.com/dfaker/SD-latent-mirroring Applies mirroring and flips to the latent images to produce anything from subtle balanced compositions to perfect reflections Example: (Click to expand:) txt2palette https://github.com/1ort/txt2palette Generate palettes by text description. This script takes the generated images and converts them into color palettes. Example: (Click to expand:) StylePile https://github.com/some9000/StylePile An easy way to mix and match elements to prompts that affect the style of the result. Example: (Click to expand:) XYZ Plot Script https://github.com/xrpgame/xyz_plot_script Generates an .html file to interactively browse the imageset. Use the scroll wheel or arrow keys to move in the Z dimension. Example: (Click to expand:) xyz-plot-grid https://github.com/Gerschel/xyz-plot-grid Place xyz_grid.py in scripts folder along side other scripts. Works like x/y plot, like how you would expect, but now has a z. Works like how youâ€™d expect it to work, with grid legends as well. Example: (Click to expand:) Expanded-XY-grid https://github.com/0xALIVEBEEF/Expanded-XY-grid Custom script for AUTOMATIC1111â€™s stable-diffusion-webui that adds more features to the standard xy grid: Multitool: Allows multiple parameters in one axis, theoretically allows unlimited parameters to be adjusted in one xy grid Customizable prompt matrix Group files in a directory S/R Placeholder - replace a placeholder value (the first value in the list of parameters) with desired values. Add PNGinfo to grid image Example: (Click to expand:) Example images: Prompt: \"darth vader riding a bicycle, modifier\"; X: Multitool: \"Prompt S/R: bicycle, motorcycle | CFG scale: 7.5, 10 | Prompt S/R Placeholder: modifier, 4k, artstation\"; Y: Multitool: \"Sampler: Euler, Euler a | Steps: 20, 50\" Booru tag autocompletion https://github.com/DominikDoom/a1111-sd-webui-tagcomplete Displays autocompletion hints for tags from â€œimage booruâ€ boards such as Danbooru. Uses local tag CSV files and includes a config for customization. Also supports completion for wildcards Embedding to PNG https://github.com/dfaker/embedding-to-png-script Converts existing embeddings to the shareable image versions. Example: (Click to expand:) Alpha Canvas https://github.com/TKoestlerx/sdexperiments Outpaint a region. Infinite outpainting concept, used the two existing outpainting scripts from the AUTOMATIC1111 repo as a basis. Example: (Click to expand:) Random grid https://github.com/lilly1987/AI-WEBUI-scripts-Random Randomly enter xy grid values. Example: (Click to expand:) Basic logic is same as x/y plot, only internally, x type is fixed as step, and type y is fixed as cfg. Generates x values as many as the number of step counts (10) within the range of step1|2 values (10-30) Generates x values as many as the number of cfg counts (10) within the range of cfg1|2 values (6-15) Even if you put the 1|2 range cap upside down, it will automatically change it. In the case of the cfg value, it is treated as an int type and the decimal value is not read. Random https://github.com/lilly1987/AI-WEBUI-scripts-Random Repeat a simple number of times without a grid. Example: (Click to expand:) Stable Diffusion Aesthetic Scorer https://github.com/grexzen/SD-Chad Rates your images. img2tiles https://github.com/arcanite24/img2tiles generate tiles from a base image. Based on SD upscale script. Example: (Click to expand:) img2mosiac https://github.com/1ort/img2mosaic Generate mosaics from images. The script cuts the image into tiles and processes each tile separately. The size of each tile is chosen randomly. Example: (Click to expand:) Depth Maps https://github.com/thygate/stable-diffusion-webui-depthmap-script This script is an addon for AUTOMATIC1111â€™s Stable Diffusion Web UI that creates depthmaps from the generated images. The result can be viewed on 3D or holographic devices like VR headsets or lookingglass display, used in Render- or Game- Engines on a plane with a displacement modifier, and maybe even 3D printed. Example: (Click to expand:) Test my prompt https://github.com/Extraltodeus/test_my_prompt Have you ever used a very long prompt full of words that you are not sure have any actual impact on your image? Did you lose the courage to try to remove them one by one to test if their effects are worthy of your pwescious GPU? WELL now you donâ€™t need any courage as this script has been MADE FOR YOU! It generates as many images as there are words in your prompt (you can select the separator of course). Example: (Click to expand:) Here the prompt is simply : \"**banana, on fire, snow**\" and so as you can see it has generated each image without each description in it. You can also test your negative prompt. Pixel Art https://github.com/C10udburst/stable-diffusion-webui-scripts Simple script which resizes images by a variable amount, also converts image to use a color palette of a given size. Example: (Click to expand:) | Disabled | Enabled x8, no resize back, no color palette | Enabled x8, no color palette | Enabled x8, 16 color palette | | :---: | :---: | :---: | :---: | |![preview](https://user-images.githubusercontent.com/18114966/201491785-e30cfa9d-c850-4853-98b8-11db8de78c8d.png) | ![preview](https://user-images.githubusercontent.com/18114966/201492204-f4303694-e98d-4ea3-8256-538a88ea26b6.png) | ![preview](https://user-images.githubusercontent.com/18114966/201491864-d0c0c9f1-e34f-4cb6-a68e-7043ec5ce74e.png) | ![preview](https://user-images.githubusercontent.com/18114966/201492175-c55fa260-a17d-47c9-a919-9116e1caa8fe.png) | [model used](https://publicprompts.art/all-in-one-pixel-art-dreambooth-model/) ```text japanese pagoda with blossoming cherry trees, full body game asset, in pixelsprite style Steps: 20, Sampler: DDIM, CFG scale: 7, Seed: 4288895889, Size: 512x512, Model hash: 916ea38c, Batch size: 4 ``` Multiple Hypernetworks https://github.com/antis0007/sd-webui-multiple-hypernetworks Adds the ability to apply multiple hypernetworks at once. Overrides the hijack, optimization and CrossAttention forward functions in order to apply multiple hypernetworks sequentially, with different weights. Hypernetwork Structure(.hns)/Variable Dropout/ Monkey Patches https://github.com/aria1th/Hypernetwork-MonkeyPatch-Extension Adds the ability to apply Hypernetwork Structure, as defining it in .hns file. see here for detailed information. Adds the ability to use proper variable dropout rate, like 0.05. Also fixes issues with using hypernetwork right after training. Adds creating beta-hypernetwork(dropout), and beta-training which allows automatic cosine annealing, and no-crop usage of original images. Config-Presets https://github.com/Zyin055/Config-Presets-Script-OLD- Quickly change settings in the txt2img and img2img tabs using a configurable dropdown of preset values. Example: (Click to expand:) Saving steps of the sampling process This script will save steps of the sampling process to a directory. import os.path import modules.scripts as scripts import gradio as gr from modules import sd_samplers, shared from modules.processing import Processed, process_images class Script(scripts.Script): def title(self): return \"Save steps of the sampling process to files\" def ui(self, is_img2img): path = gr.Textbox(label=\"Save images to path\") return [path] def run(self, p, path): index = [0] def store_latent(x): image = shared.state.current_image = sd_samplers.sample_to_image(x) image.save(os.path.join(path, f\"sample-{index[0]:05}.png\")) index[0] += 1 fun(x) fun = sd_samplers.store_latent sd_samplers.store_latent = store_latent try: proc = process_images(p) finally: sd_samplers.store_latent = fun return Processed(p, proc.images, p.seed, \"\")"
  },"/sdwui-docs/pages/en/Dependencies/": {
    "title": "dependencies",
    "keywords": "Getting started",
    "url": "/sdwui-docs/pages/en/Dependencies/",
    "body": "Python 3.10.6 and Git: Windows: download and run installers for Python 3.10.6 (webpage, exe, or win7 version) and git (webpage) Linux (Debian-based): sudo apt install wget git python3 python3-venv Linux (Red Hat-based): sudo dnf install wget git python3 Linux (Arch-based): sudo pacman -S wget git python3 Code from this repository: preferred way: using git: git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git. This way is referred because it lets you update by just running git pull. Those commands can be used from command line window that opens after you right click in Explorer and select â€œGit Bash hereâ€. alternative way: use the â€œCodeâ€ (green button) -&gt; â€œDownload ZIPâ€ option on the main page of the repo. You still need to install git even if you choose this. To update, youâ€™ll have to download zip again and replace files. The Stable Diffusion model checkpoint, a file with .ckpt extension, needs to be downloaded and placed in the models/Stable-diffusion directory. Official download File storage Torrent (magnet:?xt=urn:btih:3a4a612d75ed088ea542acac52f9f45987488d1c&amp;dn=sd-v1-4.ckpt&amp;tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&amp;tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337) Optional Dependencies ESRGAN (Upscaling) ESRGAN models such as those from the Model Database, may be placed into the ESRGAN directory. A file will be loaded as a model if it has .pth extension, and it will show up with its name in the UI. Note: RealESRGAN models are not ESRGAN models, they are not compatible. Do not download RealESRGAN models. Do not place RealESRGAN into the directory with ESRGAN models. .yaml files for sd 2.x models 768-v-ema.ckpt config) 512-base-ema.ckpt config 512-depth-ema.ckpt config Download config .yaml file and store it in same folder as .ckpt with the same name."
  },"/sdwui-docs/pages/en/Developing-custom-scripts/": {
    "title": "Developing custom scripts",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Developing-custom-scripts/",
    "body": "The Script class definition can be found in modules/scripts.py. To create your own custom script, create a python script that implements the class and drop it into the scripts folder, using the below example or other scripts already in the folder as a guide. The Script class has four primary methods, described in further detail below with a simple example script that rotates and/or flips generated images. import modules.scripts as scripts import gradio as gr import os from modules import images from modules.processing import process_images, Processed from modules.processing import Processed from modules.shared import opts, cmd_opts, state class Script(scripts.Script): # The title of the script. This is what will be displayed in the dropdown menu. Â  Â  def title(self): Â  Â  Â  Â  return \"Flip/Rotate Output\" # Determines when the script should be shown in the dropdown menu via the # returned value. As an example: # is_img2img is True if the current tab is img2img, and False if it is txt2img. # Thus, return is_img2img to only show the script on the img2img tab. Â  Â  def show(self, is_img2img): Â  Â  Â  Â  return is_img2img # How the script's is displayed in the UI. See https://gradio.app/docs/#components # for the different UI components you can use and how to create them. # Most UI components can return a value, such as a boolean for a checkbox. # The returned values are passed to the run method as parameters. Â  Â  def ui(self, is_img2img): Â  Â  Â  Â  angle = gr.Slider(minimum=0.0, maximum=360.0, step=1, value=0, Â  Â  Â  Â  label=\"Angle\") Â  Â  Â  Â  hflip = gr.Checkbox(False, label=\"Horizontal flip\") Â  Â  Â  Â  vflip = gr.Checkbox(False, label=\"Vertical flip\") Â  Â  Â  Â  overwrite = gr.Checkbox(False, label=\"Overwrite existing files\") Â  Â  Â  Â  return [angle, hflip, vflip, overwrite] # This is where the additional processing is implemented. The parameters include # self, the model object \"p\" (a StableDiffusionProcessing class, see # processing.py), and the parameters returned by the ui method. # Custom functions can be defined here, and additional libraries can be imported # to be used in processing. The return value should be a Processed object, which is # what is returned by the process_images method. Â  Â  def run(self, p, angle, hflip, vflip, overwrite): Â  Â  Â  Â  # function which takes an image from the Processed object, # and the angle and two booleans indicating horizontal and Â  Â  Â  Â  # vertical flips from the UI, then returns the Â  Â  Â  Â  # image rotated and flipped accordingly Â  Â  Â  Â  def rotate_and_flip(im, angle, hflip, vflip): Â  Â  Â  Â  Â  Â  from PIL import Image Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raf = im Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if angle != 0: Â  Â  Â  Â  Â  Â  Â  Â  raf = raf.rotate(angle, expand=True) Â  Â  Â  Â  Â  Â  if hflip: Â  Â  Â  Â  Â  Â  Â  Â  raf = raf.transpose(Image.FLIP_LEFT_RIGHT) Â  Â  Â  Â  Â  Â  if vflip: Â  Â  Â  Â  Â  Â  Â  Â  raf = raf.transpose(Image.FLIP_TOP_BOTTOM) Â  Â  Â  Â  Â  Â  return raf Â  Â  Â  Â  # If overwrite is false, append the rotation information to the filename Â  Â  Â  Â  # using the \"basename\" parameter and save it in the same directory. Â  Â  Â  Â  # If overwrite is true, stop the model from saving its outputs and Â  Â  Â  Â  # save the rotated and flipped images instead. Â  Â  Â  Â  basename = \"\" Â  Â  Â  Â  if(not overwrite): Â  Â  Â  Â  Â  Â  if angle != 0: Â  Â  Â  Â  Â  Â  Â  Â  basename += \"rotated_\" + str(angle) Â  Â  Â  Â  Â  Â  if hflip: Â  Â  Â  Â  Â  Â  Â  Â  basename += \"_hflip\" Â  Â  Â  Â  Â  Â  if vflip: Â  Â  Â  Â  Â  Â  Â  Â  basename += \"_vflip\" Â  Â  Â  Â  else: Â  Â  Â  Â  Â  Â  p.do_not_save_samples = True Â  Â  Â  Â  proc = process_images(p) Â  Â  Â  Â  # rotate and flip each image in the processed images # use the save_images method from images.py to save # them. Â  Â  Â  Â  for i in range(len(proc.images)): Â  Â  Â  Â  Â  Â  proc.images[i] = rotate_and_flip(proc.images[i], angle, hflip, vflip) Â  Â  Â  Â  Â  Â  images.save_image(proc.images[i], p.outpath_samples, basename, Â  Â  Â  Â  Â  Â  proc.seed + i, proc.prompt, opts.samples_format, info= proc.info, p=p) Â  Â  Â  Â  return proc"
  },"/sdwui-docs/pages/en/Developing-extensions/": {
    "title": "Developing extensions",
    "keywords": "development",
    "url": "/sdwui-docs/pages/en/Developing-extensions/",
    "body": "An extension is just a subdirectory in the extensions directory. Web ui interacts with installed extensions in the following way: extensionâ€™s install.py script, if it exists, is executed. extensionâ€™s scripts in the scripts directory are executed as if they were just usual user scripts, except: sys.path is extended to include the extension directory, so you can import anything in it without worrying you can use scripts.basedir() to get the current extensionâ€™s directory (since user can name it anything he wants) extensionâ€™s javascript files in the javascript directory are added to the page extensionâ€™s localization files in the localizations directory are added to settings; if there are two localizations with same name, they are not merged, one replaces another. extensionâ€™s style.css file is added to the page if extension has preload.py file in its root directory, it is loaded before parsing commandline args if extensionâ€™s preload.py has a preload function, it is called, and commandline args parser is passed to it as an argument. Hereâ€™s an example of how to use it to add a command line argument: def preload(parser): parser.add_argument(\"--wildcards-dir\", type=str, help=\"directory with wildcards\", default=None) For how to develop custom scripts, which usually will do most of extensionâ€™s work, see Developing custom scripts. Localization extensions The preferred way to do localizations for the project is via making an extension. The basic file structure for the extension should be: ğŸ“ webui root directory â”—â”â” ğŸ“ extensions â”—â”â” ğŸ“ webui-localization-la_LA &lt;----- name of extension â”—â”â” ğŸ“ localizations &lt;----- the single directory inside the extension â”—â”â” ğŸ“„ la_LA.json &lt;----- actual file with translations Create a github repository with this file structure and ask any of people listed in collaborators section to add your extension to wiki. If your language needs javascript/css or even python support, you can add that to the extension too. install.py install.py is the script that is launched by the launch.py, the launcher, in a separate process before webui starts, and itâ€™s meant to install dependencies of the extension. It must be located in the root directory of the extension, not in the scripts directory. The script is launched with PYTHONPATH environment variable set to webuiâ€™s path, so you can just import launch and use its functionality: import launch if not launch.is_installed(\"aitextgen\"): launch.run_pip(\"install aitextgen==0.6.0\", \"requirements for MagicPrompt\")"
  },"/sdwui-docs/pages/en/Extensions/": {
    "title": "Extensions",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Extensions/",
    "body": "General info Extensions are a more convenient form of user scripts. Extensions all exist in their own subdirectory inside the extensions directory. You can use git to install an extension like this: git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients extensions/aesthetic-gradients This installs an extension from https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients into the extensions/aesthetic-gradients directory. Alternatively you can just copy-paste a directory into extensions. For developing extensions, see Developing extensions. Security As extensions allow the user to install and run arbitrary code, this can be used maliciously, and is disabled by default when running with options that allow remote users to connect to the server (--share or --listen) - youâ€™ll still have the UI, but trying to install anything will result in error. If you want to use those options and still be able to install extensions, use --enable-insecure-extension-access command line flag. Extensions Aesthetic Gradients https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients Create an embedding from one or few pictures and use it to apply their style to generated images. Wildcards https://github.com/AUTOMATIC1111/stable-diffusion-webui-wildcards Allows you to use __name__ syntax in your prompt to get a random line from a file named name.txt in the wildcards directory. Dynamic Prompts https://github.com/adieyal/sd-dynamic-prompts A custom extension for AUTOMATIC1111/stable-diffusion-webui that implements an expressive template language for random or combinatorial prompt generation along with features to support deep wildcard directory structures. More features and additions are shown in the readme. Using this extension, the prompt: A {house|apartment|lodge|cottage} in {summer|winter|autumn|spring} by {2$$artist1|artist2|artist3} Will any of the following prompts: A house in summer by artist1, artist2 A lodge in autumn by artist3, artist1 A cottage in winter by artist2, artist3 â€¦ This is especially useful if you are searching for interesting combinations of artists and styles. You can also pick a random string from a file. Assuming you have the file seasons.txt in WILDCARD_DIR (see below), then: __seasons__ is coming Might generate the following: Winter is coming Spring is coming â€¦ You can also use the same wildcard twice I love __seasons__ better than __seasons__ I love Winter better than Summer I love Spring better than Spring Dreambooth https://github.com/d8ahazard/sd_dreambooth_extension Dreambooth in the UI. Refer to the project readme for tuning and configuration requirements. Includes LoRA (Low Rank Adaptation) Based on ShivamShiaroâ€™s repo. Smart Process https://github.com/d8ahazard/sd_smartprocess Intelligent cropping, captioning, and image enhancement. Image browser https://github.com/yfszzx/stable-diffusion-webui-images-browser Provides an interface to browse created images in the web browser. Inspiration https://github.com/yfszzx/stable-diffusion-webui-inspiration Randomly display the pictures of the artistâ€™s or artistic genres typical style, more pictures of this artist or genre is displayed after selecting. So you donâ€™t have to worry about how hard it is to choose the right style of art when you create. Deforum https://github.com/deforum-art/deforum-for-automatic1111-webui The official port of Deforum, an extensive script for 2D and 3D animations, supporting keyframable sequences, dynamic math parameters (even inside the prompts), dynamic masking, depth estimation and warping. Artists to study https://github.com/camenduru/stable-diffusion-webui-artists-to-study https://artiststostudy.pages.dev/ adapted to an extension for web ui. To install it, clone the repo into the extensions directory and restart the web ui: git clone https://github.com/camenduru/stable-diffusion-webui-artists-to-study You can add the artist name to the clipboard by clicking on it. (thanks for the idea @gmaciocci) Aesthetic Image Scorer https://github.com/tsngo/stable-diffusion-webui-aesthetic-image-scorer Extension for https://github.com/AUTOMATIC1111/stable-diffusion-webui Calculates aesthetic score for generated images using CLIP+MLP Aesthetic Score Predictor based on Chad Scorer See Discussions Saves score to windows tags with other options planned Dataset Tag Editor https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor æ—¥æœ¬èª Readme This is an extension to edit captions in training dataset for Stable Diffusion web UI by AUTOMATIC1111. It works well with text captions in comma-separated style (such as the tags generated by DeepBooru interrogator). Caption in the filenames of images can be loaded, but edited captions can only be saved in the form of text files. auto-sd-paint-ext https://github.com/Interpause/auto-sd-paint-ext Formerly known as auto-sd-krita. Extension for AUTOMATIC1111â€™s webUI with Krita Plugin (other drawing studios soon?) Outdated demo New UI (TODO: demo image) Differences UI no longer freezes during image update Inpainting layer no longer has to be manually hidden, nor use white specifically UI has been improved &amp; squeezed further Scripts API is now possible training-picker https://github.com/Maurdekye/training-picker Adds a tab to the webui that allows the user to automatically extract keyframes from video, and manually extract 512x512 crops of those frames for use in model training. Installation Install AUTOMATIC1111â€™s Stable Diffusion Webui Install ffmpeg for your operating system Clone this repository into the extensions folder inside the webui Drop videos you want to extract cropped frames from into the training-picker/videos folder Unprompted https://github.com/ThereforeGames/unprompted Supercharge your prompt workflow with this powerful scripting language! Unprompted is a highly modular extension for AUTOMATIC1111â€™s Stable Diffusion Web UI that allows you to include various shortcodes in your prompts. You can pull text from files, set up your own variables, process text through conditional functions, and so much more - itâ€™s like wildcards on steroids. While the intended usecase is Stable Diffusion, this engine is also flexible enough to serve as an all-purpose text generator. Booru tag autocompletion https://github.com/DominikDoom/a1111-sd-webui-tagcomplete Displays autocompletion hints for tags from â€œimage booruâ€ boards such as Danbooru. Uses local tag CSV files and includes a config for customization. novelai-2-local-prompt https://github.com/animerl/novelai-2-local-prompt Add a button to convert the prompts used in NovelAI for use in the WebUI. In addition, add a button that allows you to recall a previously used prompt. Tokenizer https://github.com/AUTOMATIC1111/stable-diffusion-webui-tokenizer Adds a tab that lets you preview how CLIP model would tokenize your text. Push to ğŸ¤— Hugging Face https://github.com/camenduru/stable-diffusion-webui-huggingface To install it, clone the repo into the extensions directory and restart the web ui: git clone https://github.com/camenduru/stable-diffusion-webui-huggingface pip install huggingface-hub StylePile https://github.com/some9000/StylePile An easy way to mix and match elements to prompts that affect the style of the result. Latent Mirroring https://github.com/dfaker/SD-latent-mirroring Applies mirroring and flips to the latent images to produce anything from subtle balanced compositions to perfect reflections Embeddings editor https://github.com/CodeExplode/stable-diffusion-webui-embedding-editor Allows you to manually edit textual inversion embeddings using sliders. seed travel https://github.com/yownas/seed_travel Small script for AUTOMATIC1111/stable-diffusion-webui to create images that exists between seeds. shift-attention https://github.com/yownas/shift-attention Generate a sequence of images shifting attention in the prompt. This script enables you to give a range to the weight of tokens in a prompt and then generate a sequence of images stepping from the first one to the second. https://user-images.githubusercontent.com/13150150/193368939-c0a57440-1955-417c-898a-ccd102e207a5.mp4 prompt travel https://github.com/Kahsolt/stable-diffusion-webui-prompt-travel Extension script for AUTOMATIC1111/stable-diffusion-webui to travel between prompts in latent space. Sonar https://github.com/Kahsolt/stable-diffusion-webui-sonar Improve the generated image quality, searches for similar (yet even better!) images in the neighborhood of some known image, focuses on single prompt optimization rather than traveling between multiple prompts. Detection Detailer https://github.com/dustysys/ddetailer An object detection and auto-mask extension for Stable Diffusion web UI. conditioning-highres-fix https://github.com/klimaleksus/stable-diffusion-webui-conditioning-highres-fix This is Extension for rewriting Inpainting conditioning mask strength value relative to Denoising strength at runtime. This is useful for Inpainting models such as sd-v1-5-inpainting.ckpt Randomize https://github.com/stysmmaker/stable-diffusion-webui-randomize Allows for random parameters during txt2img generation. This script is processed for all generations, regardless of the script selected, meaning this script will function with others as well, such as AUTOMATIC1111/stable-diffusion-webui-wildcards. Auto TLS-HTTPS https://github.com/papuSpartan/stable-diffusion-webui-auto-tls-https Allows you to easily, or even completely automatically start using HTTPS. DreamArtist https://github.com/7eu7d7/DreamArtist-sd-webui-extension Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning. WD 1.4 Tagger https://github.com/toriato/stable-diffusion-webui-wd14-tagger Uses a trained model file, produces WD 1.4 Tags. Model link - https://mega.nz/file/ptA2jSSB#G4INKHQG2x2pGAVQBn-yd_U5dMgevGF8YYM9CR_R1SY booru2prompt https://github.com/Malisius/booru2prompt This SD extension allows you to turn posts from various image boorus into stable diffusion prompts. It does so by pulling a list of tags down from their API. You can copy-paste in a link to the post you want yourself, or use the built-in search feature to do it all without leaving SD. also see: https://github.com/stysmmaker/stable-diffusion-webui-booru-prompt gelbooru-prompt https://github.com/antis0007/sd-webui-gelbooru-prompt Fetch tags with image hash. Updates planned. Merge Board https://github.com/bbc-mc/sdweb-merge-board Multiple lane merge support(up to 10). Save and Load your merging combination as Recipes, which is simple text. also see: https://github.com/Maurdekye/model-kitchen Depth Maps https://github.com/thygate/stable-diffusion-webui-depthmap-script Creates depthmaps from the generated images. The result can be viewed on 3D or holographic devices like VR headsets or lookingglass display, used in Render or Game- Engines on a plane with a displacement modifier, and maybe even 3D printed. multi-subject-render https://github.com/Extraltodeus/multi-subject-render It is a depth aware extension that can help to create multiple complex subjects on a single image. It generates a background, then multiple foreground subjects, cuts their backgrounds after a depth analysis, paste them onto the background and finally does an img2img for a clean finish. depthmap2mask https://github.com/Extraltodeus/depthmap2mask Create masks for img2img based on a depth estimation made by MiDaS. ABG_extension https://github.com/KutsuyaYuki/ABG_extension Automatically remove backgrounds. Uses an onnx model fine-tuned for anime images. Runs on GPU. Visualize Cross-Attention https://github.com/benkyoujouzu/stable-diffusion-webui-visualize-cross-attention-extension Generates highlighted sectors of a submitted input image, based on input prompts. Use with tokenizer extension. See the readme for more info. DAAM https://github.com/kousw/stable-diffusion-webui-daam DAAM stands for Diffusion Attentive Attribution Maps. Enter the attention text (must be a string contained in the prompt) and run. An overlapping image with a heatmap for each attention will be generated along with the original image. Prompt Gallery https://github.com/dr413677671/PromptGallery-stable-diffusion-webui Build a yaml file filled with prompts of your character, hit generate, and quickly preview them by their word attributes and modifiers. embedding-inspector https://github.com/tkalayci71/embedding-inspector Inspect any token(a word) or Textual-Inversion embeddings and find out which embeddings are similar. You can mix, modify, or create the embeddings in seconds. Much more intriguing options have since been released, see here. Infinity Grid Generator https://github.com/mcmonkeyprojects/sd-infinity-grid-generator-script Build a yaml file with your chosen parameters, and generate infinite-dimensional grids. Built-in ability to add description text to fields. See readme for usage details. NSFW checker https://github.com/AUTOMATIC1111/stable-diffusion-webui-nsfw-censor Replaces NSFW images with black. Diffusion Defender https://github.com/WildBanjos/DiffusionDefender Prompt blacklist, find and replace, for semi-private and public instances. Config-Presets https://github.com/Zyin055/Config-Presets Adds a configurable dropdown to allow you to change UI preset settings in the txt2img and img2img tabs. Preset Utilities https://github.com/Gerschel/sd_web_ui_preset_utils Preset tool for UI. Planned support for some other custom scripts. DH Patch https://github.com/d8ahazard/sd_auto_fix Random patches by D8ahazard. Auto-load config YAML files for v2, 2.1 models; patch latent-diffusion to fix attention on 2.1 models (black boxes without no-half), whatever else I come up with. Riffusion https://github.com/enlyth/sd-webui-riffusion Use Riffusion model to produce music in gradio. To replicate original interpolation technique, input the prompt travel extension output frames into the riffusion tab. Save Intermediate Images https://github.com/AlUlkesh/sd_save_intermediate_images Implements saving intermediate images, with more advanced features. openOutpaint extension https://github.com/zero01101/openOutpaint-webUI-extension A tab with the full openOutpaint UI. Run with the â€“api flag. Enhanced-img2img https://github.com/OedoSoldier/enhanced-img2img An extension with support for batched and better inpainting."
  },"/sdwui-docs/pages/en/Features/": {
    "title": "Features",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Features/",
    "body": "This is a feature showcase page for Stable Diffusion web UI. All examples are non-cherrypicked unless specified otherwise. Stable Diffusion 2.0 Basic models Models are supported: 768-v-ema.ckpt (model, config) and 512-base-ema.ckpt (model, config). 2.1 checkpoints should also work. download the checkpoint (from here: https://huggingface.co/stabilityai/stable-diffusion-2) put it into models/Stable-Diffusion directory grab the config from SD2.0 repository and put it into same place as the checkpoint, renaming it to have same filename (i.e. if your checkpoint is named 768-v-ema.ckpt, the config should be named 768-v-ema.yaml) select the new checkpoint from the UI Train tab will most likely be broken for the 2.0 models. If 2.0 or 2.1 is generating black images, enable full precision with --no-half or try using the --xformers optimization. Note: SD 2.0 and 2.1 are more sensitive to FP16 numerical instability (as noted by themselves here) due to their new cross attention module. On fp16: comment to enable, in webui-user.bat: @echo off set PYTHON= set GIT= set VENV_DIR= set COMMANDLINE_ARGS=your command line options set STABLE_DIFFUSION_COMMIT_HASH=\"c12d960d1ee4f9134c2516862ef991ec52d3f59e\" set ATTN_PRECISION=fp16 call webui.bat Depth-guided model More info. PR. Instructions: download the 512-depth-ema.ckpt checkpoint place it in models/Stable-diffusion grab the config and place it in the same folder as the checkpoint rename the config to 512-depth-ema.yaml select the new checkpoint from the UI The depth-guided model will only work in img2img tab. Outpainting Outpainting extends the original image and inpaints the created empty space. Example: Original Outpainting Outpainting again Original image by Anonymous user from 4chan. Thank you, Anonymous user. You can find the feature in the img2img tab at the bottom, under Script -&gt; Poor manâ€™s outpainting. Outpainting, unlike normal image generation, seems to profit very much from large step count. A recipe for a good outpainting is a good prompt that matches the picture, sliders for denoising and CFG scale set to max, and step count of 50 to 100 with Euler ancestral or DPM2 ancestral samplers. 81 steps, Euler A 30 steps, Euler A 10 steps, Euler A 80 steps, Euler A Inpainting In img2img tab, draw a mask over a part of the image, and that part will be in-painted. Options for inpainting: draw a mask yourself in the web editor erase a part of the picture in an external editor and upload a transparent picture. Any even slightly transparent areas will become part of the mask. Be aware that some editors save completely transparent areas as black by default. change mode (to the bottom right of the picture) to â€œUpload maskâ€ and choose a separate black and white image for the mask (white=inpaint). Inpainting model RunwayML has trained an additional model specifically designed for inpainting. This model accepts additional inputs - the initial image without noise plus the mask - and seems to be much better at the job. Download and extra info for the model is here: https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion To use the model, you must rename the checkpoint so that its filename ends in inpainting.ckpt, for example, 1.5-inpainting.ckpt. After that just select the checkpoint as youâ€™d usually select any checkpoint and youâ€™re good to go. Masked content The masked content field determines content is placed to put into the masked regions before they are inpainted. mask fill original latent noise latent nothing Inpaint at full resolution Normally, inpainting resizes the image to the target resolution specified in the UI. With Inpaint at full resolution enabled, only the masked region is resized, and after processing it is pasted back to the original picture. This allows you to work with large pictures and allows you to render the inpainted object at a much larger resolution. Input Inpaint normal Inpaint at whole resolution Masking mode There are two options for masked mode: Inpaint masked - the region under the mask is inpainted Inpaint not masked - under the mask is unchanged, everything else is inpainted Alpha mask Input Output Prompt matrix Separate multiple prompts using the | character, and the system will produce an image for every combination of them. For example, if you use a busy city street in a modern city|illustration|cinematic lighting prompt, there are four combinations possible (first part of the prompt is always kept): a busy city street in a modern city a busy city street in a modern city, illustration a busy city street in a modern city, cinematic lighting a busy city street in a modern city, illustration, cinematic lighting Four images will be produced, in this order, all with the same seed and each with a corresponding prompt: Another example, this time with 5 prompts and 16 variations: You can find the feature at the bottom, under Script -&gt; Prompt matrix. Color Sketch Basic coloring tool for img2img. To use this feature in img2img, enable with --gradio-img2img-tool color-sketch in commandline args. To use this feature in inpainting mode, enable with --gradio-inpaint-tool color-sketch. Chromium-based browsers support a dropper tool. (see picture) Stable Diffusion upscale Upscale image using RealESRGAN/ESRGAN and then go through tiles of the result, improving them with img2img. It also has an option to let you do the upscaling part yourself in an external program, and just go through tiles with img2img. Original idea by: https://github.com/jquesnelle/txt2imghd. This is an independent implementation. To use this feature, select SD upscale from the scripts dropdown selection (img2img tab). The input image will be upscaled to twice the original width and height, and UIâ€™s width and height sliders specify the size of individual tiles. Because of overlap, the size of the tile can be very important: 512x512 image needs nine 512x512 tiles (because of overlap), but only four 640x640 tiles. Recommended parameters for upscaling: Sampling method: Euler a Denoising strength: 0.2, can go up to 0.4 if you feel adventurous Original RealESRGAN Topaz Gigapixel SD upscale Attention/emphasis Using () in the prompt increases the modelâ€™s attention to enclosed words, and [] decreases it. You can combine multiple modifiers: Cheat sheet: a (word) - increase attention to word by a factor of 1.1 a ((word)) - increase attention to word by a factor of 1.21 (= 1.1 * 1.1) a [word] - decrease attention to word by a factor of 1.1 a (word:1.5) - increase attention to word by a factor of 1.5 a (word:0.25) - decrease attention to word by a factor of 4 (= 1 / 0.25) a \\(word\\) - use literal () characters in prompt With (), a weight can be specified like this: (text:1.4). If the weight is not specified, it is assumed to be 1.1. Specifying weight only works with () not with []. If you want to use any of the literal ()[] characters in the prompt, use the backslash to escape them: anime_\\(character\\). On 2022-09-29, a new implementation was added that supports escape characters and numerical weights. A downside of the new implementation is that the old one was not perfect and sometimes ate characters: â€œa (((farm))), daytimeâ€, for example, would become â€œa farm daytimeâ€ without the comma. This behavior is not shared by the new implementation which preserves all text correctly, and this means that your saved seeds may produce different pictures. For now, there is an option in settings to use the old implementation. NAI uses my implementation from before 2022-09-29, except they have 1.05 as the multiplier and use {} instead of (). So the conversion applies: their {word} = our (word:1.05) their `` = our (word:1.1025) their [word] = our (word:0.952) (0.952 = 1/1.05) their [[word]] = our (word:0.907) (0.907 = 1/1.05/1.05) Loopback Selecting the loopback script in img2img allows you to automatically feed output image as input for the next batch. Equivalent to saving output image, and replacing the input image with it. Batch count setting controls how many iterations of this you get. Usually, when doing this, you would choose one of many images for the next iteration yourself, so the usefulness of this feature may be questionable, but Iâ€™ve managed to get some very nice outputs with it that I wasnâ€™t able to get otherwise. Example: (cherrypicked result) Original image by Anonymous user from 4chan. Thank you, Anonymous user. X/Y plot Creates a grid of images with varying parameters. Select which parameters should be shared by rows and columns using X type and Y type fields, and input those parameters separated by comma into X values/Y values fields. For integer, and floating point numbers, and ranges are supported. Examples: Simple ranges: 1-5 = 1, 2, 3, 4, 5 Ranges with increment in bracket: 1-5 (+2) = 1, 3, 5 10-5 (-3) = 10, 7 1-3 (+0.5) = 1, 1.5, 2, 2.5, 3 Ranges with the count in square brackets: 1-10 [5] = 1, 3, 5, 7, 10 0.0-1.0 [6] = 0.0, 0.2, 0.4, 0.6, 0.8, 1.0 Here are the settings that create the graph above: Prompt S/R Prompt S/R is one of more difficult to understand modes of operation for X/Y Plot. S/R stands for search/replace, and thatâ€™s what it does - you input a list of words or phrases, it takes the first from the list and treats it as keyword, and replaces all instances of that keyword with other entries from the list. For example, with prompt a man holding an apple, 8k clean, and Prompt S/R an apple, a watermelon, a gun you will get three prompts: a man holding an apple, 8k clean a man holding a watermelon, 8k clean a man holding a gun, 8k clean The list uses the same syntax as a line in a CSV file, so if you want to include commas into your entries you have to put text in quotes and make sure there is no space between quotes and separating commas: darkness, light, green, heat - 4 items - darkness, light, green, heat darkness, \"light, green\", heat - WRONG - 4 items - darkness, \"light, green\", heat darkness,\"light, green\",heat - RIGHT - 3 items - darkness, light, green, heat Textual Inversion Short explanation: place your embeddings into the embeddings directory, and use the filename in the prompt. Long explanation: Textual Inversion Resizing There are three options for resizing input images in img2img mode: Just resize - simply resizes the source image to the target resolution, resulting in an incorrect aspect ratio Crop and resize - resize source image preserving aspect ratio so that entirety of target resolution is occupied by it, and crop parts that stick out Resize and fill - resize source image preserving aspect ratio so that it entirely fits target resolution, and fill empty space by rows/columns from the source image Example: Sampling method selection Pick out of multiple sampling methods for txt2img: Seed resize This function allows you to generate images from known seeds at different resolutions. Normally, when you change resolution, the image changes entirely, even if you keep all other parameters including seed. With seed resizing you specify the resolution of the original image, and the model will very likely produce something looking very similar to it, even at a different resolution. In the example below, the leftmost picture is 512x512, and others are produced with exact same parameters but with larger vertical resolution. Info Image Seed resize not enabled Seed resized from 512x512 Ancestral samplers are a little worse at this than the rest. You can find this feature by clicking the â€œExtraâ€ checkbox near the seed. Variations A Variation strength slider and Variation seed field allow you to specify how much the existing picture should be altered to look like a different one. At maximum strength, you will get pictures with the Variation seed, at minimum - pictures with the original Seed (except for when using ancestral samplers). You can find this feature by clicking the â€œExtraâ€ checkbox near the seed. Styles Press the â€œSave prompt as styleâ€ button to write your current prompt to styles.csv, the file with a collection of styles. A dropbox to the right of the prompt will allow you to choose any style out of previously saved, and automatically append it to your input. To delete a style, manually delete it from styles.csv and restart the program. if you use the special string {prompt} in your style, it will substitute anything currently in the prompt into that position, rather than appending the style to your prompt. Negative prompt Allows you to use another prompt of things the model should avoid when generating the picture. This works by using the negative prompt for unconditional conditioning in the sampling process instead of an empty string. Advanced explanation: Negative prompt Original Negative: purple Negative: tentacles CLIP interrogator Originally by: https://github.com/pharmapsychotic/clip-interrogator CLIP interrogator allows you to retrieve the prompt from an image. The prompt wonâ€™t allow you to reproduce this exact image (and sometimes it wonâ€™t even be close), but it can be a good start. The first time you run CLIP interrogator it will download a few gigabytes of models. CLIP interrogator has two parts: one is a BLIP model that creates a text description from the picture. Other is a CLIP model that will pick few lines relevant to the picture out of a list. By default, there is only one list - a list of artists (from artists.csv). You can add more lists by doing the following: create interrogate directory in the same place as webui put text files in it with a relevant description on each line For example of what text files to use, see https://github.com/pharmapsychotic/clip-interrogator/tree/main/data. In fact, you can just take files from there and use them - just skip artists.txt because you already have a list of artists in artists.csv (or use that too, whoâ€™s going to stop you). Each file adds one line of text to the final description. If you add â€œ.top3.â€ to filename, for example, flavors.top3.txt, the three most relevant lines from this file will be added to the prompt (other numbers also work). There are settings relevant to this feature: Interrogate: keep models in VRAM - do not unload Interrogate models from memory after using them. For users with a lot of VRAM. Interrogate: use artists from artists.csv - adds artist from artists.csv when interrogating. Can be useful to disable when you have your list of artists in interrogate directory Interrogate: num_beams for BLIP - parameter that affects how detailed descriptions from BLIP model are (the first part of generated prompt) Interrogate: minimum description length - minimum length for BLIP modelâ€™s text Interrogate: maximum descripton length - maximum length for BLIP modelâ€™s text Interrogate: maximum number of lines in text file - interrogator will only consider this many first lines in a file. Set to 0, the default is 1500, which is about as much as a 4GB videocard can handle. Prompt editing Prompt editing allows you to start sampling one picture, but in the middle swap to something else. The base syntax for this is: [from:to:when] Where from and to are arbitrary texts, and when is a number that defines how late in the sampling cycle should the switch be made. The later it is, the less power the model has to draw the to text in place of from text. If when is a number between 0 and 1, itâ€™s a fraction of the number of steps after which to make the switch. If itâ€™s an integer greater than zero, itâ€™s just the step after which to make the switch. Nesting one prompt editing inside another does work. Additionally: [to:when] - adds to to the prompt after a fixed number of steps (when) [from::when] - removes from from the prompt after a fixed number of steps (when) Example: a [fantasy:cyberpunk:16] landscape At start, the model will be drawing a fantasy landscape. After step 16, it will switch to drawing a cyberpunk landscape, continuing from where it stopped with fantasy. Hereâ€™s a more complex example with multiple edits: fantasy landscape with a [mountain:lake:0.25] and [an oak:a christmas tree:0.75][ in foreground::0.6][ in background:0.25] [shoddy:masterful:0.5] (sampler has 100 steps) at start, fantasy landscape with a mountain and an oak in foreground shoddy after step 25, fantasy landscape with a lake and an oak in foreground in background shoddy after step 50, fantasy landscape with a lake and an oak in foreground in background masterful after step 60, fantasy landscape with a lake and an oak in background masterful after step 75, fantasy landscape with a lake and a christmas tree in background masterful The picture at the top was made with the prompt: `Official portrait of a smiling world war ii general, [male:female:0.99], cheerful, happy, detailed face, 20th century, highly detailed, cinematic lighting, digital art painting by Greg Rutkowskiâ€™s And the number 0.99 is replaced with whatever you see in column labels on the image. The last column in the picture is [male:female:0.0], which essentially means that you are asking the model to draw a female from the start, without starting with a male general, and that is why it looks so different from others. Alternating Words Convenient Syntax for swapping every other step. [cow|horse] in a field On step 1, prompt is â€œcow in a field.â€ Step 2 is â€œhorse in a field.â€ Step 3 is â€œcow in a fieldâ€ and so on. See more advanced example below. On step 8, the chain loops back from â€œmanâ€ to â€œcow.â€ [cow|cow|horse|man|siberian tiger|ox|man] in a field Prompt editing was first implemented by Doggettx in this myspace.com post. Highres. fix A convenience option to partially render your image at a lower resolution, upscale it, and then add details at a high resolution. By default, txt2img makes horrible images at very high resolutions, and this makes it possible to avoid using the small pictureâ€™s composition. Enabled by checking the â€œHighres. fixâ€ checkbox on the txt2img page. Without With Composable Diffusion A method to allow the combination of multiple prompts. combine prompts using an uppercase AND a cat AND a dog Supports weights for prompts: a cat :1.2 AND a dog AND a penguin :2.2 The default weight value is 1. It can be quite useful for combining multiple embeddings to your result: creature_embedding in the woods:0.7 AND arcane_embedding:0.5 AND glitch_embedding:0.2 Using a value lower than 0.1 will barely have an effect. a cat AND a dog:0.03 will produce basically the same output as a cat This could be handy for generating fine-tuned recursive variations, by continuing to append more prompts to your total. creature_embedding on log AND frog:0.13 AND yellow eyes:0.08 Interrupt Press the Interrupt button to stop current processing. 4GB videocard support Optimizations for GPUs with low VRAM. This should make it possible to generate 512x512 images on videocards with 4GB memory. --lowvram is a reimplementation of an optimization idea by basujindal. Model is separated into modules, and only one module is kept in GPU memory; when another module needs to run, the previous is removed from GPU memory. The nature of this optimization makes the processing run slower â€“ about 10 times slower compared to normal operation on my RTX 3090. --medvram is another optimization that should reduce VRAM usage significantly by not processing conditional and unconditional denoising in the same batch. This implementation of optimization does not require any modification to the original Stable Diffusion code. Face restoration Lets you improve faces in pictures using either GFPGAN or CodeFormer. There is a checkbox in every tab to use face restoration, and also a separate tab that just allows you to use face restoration on any picture, with a slider that controls how visible the effect is. You can choose between the two methods in settings. Original GFPGAN CodeFormer Saving Click the Save button under the output section, and generated images will be saved to a directory specified in settings; generation parameters will be appended to a csv file in the same directory. Loading Gradioâ€™s loading graphic has a very negative effect on the processing speed of the neural network. My RTX 3090 makes images about 10% faster when the tab with gradio is not active. By default, the UI now hides loading progress animation and replaces it with static â€œLoadingâ€¦â€ text, which achieves the same effect. Use the --no-progressbar-hiding commandline option to revert this and show loading animations. Prompt validation Stable Diffusion has a limit for input text length. If your prompt is too long, you will get a warning in the text output field, showing which parts of your text were truncated and ignored by the model. Png info Adds information about generation parameters to PNG as a text chunk. You can view this information later using any software that supports viewing PNG chunk info, for example: https://www.nayuki.io/page/png-file-chunk-inspector Settings A tab with settings, allows you to use UI to edit more than half of parameters that previously were commandline. Settings are saved to config.js file. Settings that remain as commandline options are ones that are required at startup. Filenames format The Images filename pattern field in the Settings tab allows customization of generated txt2img and img2img images filenames. This pattern defines the generation parameters you want to include in filenames and their order. The supported tags are: [steps], [cfg], [prompt], [prompt_no_styles], [prompt_spaces], [width], [height], [styles], [sampler], [seed], [model_hash], [prompt_words], [date], [datetime], [job_timestamp]. This list will evolve though, with new additions. You can get an up-to-date list of supported tags by hovering your mouse over the â€œImages filename patternâ€ label in the UI. Example of a pattern: [seed]-[steps]-[cfg]-[sampler]-[prompt_spaces] Note about â€œpromptâ€ tags: [prompt] will add underscores between the prompt words, while [prompt_spaces] will keep the prompt intact (easier to copy/paste into the UI again). [prompt_words] is a simplified and cleaned-up version of your prompt, already used to generate subdirectories names, with only the words of your prompt (no punctuation). If you leave this field empty, the default pattern will be applied ([seed]-[prompt_spaces]). Please note that the tags are actually replaced inside the pattern. It means that you can also add non-tags words to this pattern, to make filenames even more explicit. For example: s=[seed],p=[prompt_spaces] User scripts If the program is launched with --allow-code option, an extra text input field for script code is available at the bottom of the page, under Scripts -&gt; Custom code. It allows you to input python code that will do the work with the image. In code, access parameters from web UI using the p variable, and provide outputs for web UI using the display(images, seed, info) function. All globals from the script are also accessible. A simple script that would just process the image and output it normally: import modules.processing processed = modules.processing.process_images(p) print(\"Seed was: \" + str(processed.seed)) display(processed.images, processed.seed, processed.info) UI config You can change parameters for UI elements: radio groups: default selection sliders: default value, min, max, step checkboxes: checked state text and number inputs: default values The file is ui-config.json in webui dir, and it is created automatically if you donâ€™t have one when the program starts. Checkboxes that would usually expand a hidden section will not initially do so when set as UI config entries. Some settings will break processing, like step not divisible by 64 for width and height, and some, like changing the default function on the img2img tab, may break UI. I do not have plans to address those in near future. ESRGAN Itâ€™s possible to use ESRGAN models on the Extras tab, as well as in SD upscale. To use ESRGAN models, put them into ESRGAN directory in the same location as webui.py. A file will be loaded as a model if it has .pth extension. Grab models from the Model Database. Not all models from the database are supported. All 2x models are most likely not supported. img2img alternative test Deconstructs an input image using a reverse of the Euler diffuser to create the noise pattern used to construct the input prompt. As an example, you can use this image. Select the img2img alternative test from the scripts section. Adjust your settings for the reconstruction process: Use a brief description of the scene: â€œA smiling woman with brown hair.â€ Describing features you want to change helps. Set this as your starting prompt, and â€˜Original Input Promptâ€™ in the script settings. You MUST use the Euler sampling method, as this script is built on it. Sampling steps: 50-60. This MUCH match the decode steps value in the script, or youâ€™ll have a bad time. Use 50 for this demo. CFG scale: 2 or lower. For this demo, use 1.8. (Hint, you can edit ui-config.json to change â€œimg2img/CFG Scale/stepâ€ to .1 instead of .5. Denoising strength - this does matter, contrary to what the old docs said. Set it to 1. Width/Height - Use the width/height of the input image. Seedâ€¦you can ignore this. The reverse Euler is generating the noise for the image now. Decode cfg scale - Somewhere lower than 1 is the sweet spot. For the demo, use 1. Decode steps - as mentioned above, this should match your sampling steps. 50 for the demo, consider increasing to 60 for more detailed images. Once all of the above are dialed in, you should be able to hit â€œGenerateâ€ and get back a result that is a very close approximation to the original. After validating that the script is re-generating the source photo with a good degree of accuracy, you can try to change the details of the prompt. Larger variations of the original will likely result in an image with an entirely different composition than the source. Example outputs using the above settings and prompts below (Red hair/pony not pictured) â€œA smiling woman with blue hair.â€ Works. â€œA frowning woman with brown hair.â€ Works. â€œA frowning woman with red hair.â€ Works. â€œA frowning woman with red hair riding a horse.â€ Seems to replace the woman entirely, and now we have a ginger pony. user.css Create a file named user.css near webui.py and put custom CSS code into it. For example, this makes the gallery taller: #txt2img_gallery, #img2img_gallery{ min-height: 768px; } A useful tip is you can append /?__theme=dark to your webui url to enable a built in dark theme e.g. (http://127.0.0.1:7860/?__theme=dark) Alternatively, you can add the --theme=dark to the set COMMANDLINE_ARGS= in webui-user.bat e.g. set COMMANDLINE_ARGS=--theme=dark notification.mp3 If an audio file named notification.mp3 is present in webuiâ€™s root folder, it will be played when the generation process completes. As a source of inspiration: https://pixabay.com/sound-effects/search/ding/?duration=0-30 https://pixabay.com/sound-effects/search/notification/?duration=0-30 Tweaks Ignore last layers of CLIP model This is a slider in settings, and it controls how early the processing of prompt by CLIP network should be stopped. A more detailed explanation: CLIP is a very advanced neural network that transforms your prompt text into a numerical representation. Neural networks work very well with this numerical representation and thatâ€™s why devs of SD chose CLIP as one of 3 models involved in stable diffusionâ€™s method of producing images. As CLIP is a neural network, it means that it has a lot of layers. Your prompt is digitized in a simple way, and then fed through layers. You get numerical representation of the prompt after the 1st layer, you feed that into the second layer, you feed the result of that into third, etc, until you get to the last layer, and thatâ€™s the output of CLIP that is used in stable diffusion. This is the slider value of 1. But you can stop early, and use the output of the next to last layer - thatâ€™s slider value of 2. The earlier you stop, the less layers of neural network have worked on the prompt. Some models were trained with this kind of tweak, so setting this value helps produce better results on those models."
  },"/sdwui-docs/pages/en/Install-and-Run-on-AMD-GPUs/": {
    "title": "install on AMD",
    "keywords": "Getting started",
    "url": "/sdwui-docs/pages/en/Install-and-Run-on-AMD-GPUs/",
    "body": "The instructions below only work on Linux! An alternative guide for Windows user can be found here (untested). Running natively Execute the following: git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui cd stable-diffusion-webui python -m venv venv source venv/bin/activate python -m pip install --upgrade pip wheel # It's possible that you don't need \"--precision full\", dropping \"--no-half\" however crashes my drivers TORCH_COMMAND='pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/rocm5.1.1' python launch.py --precision full --no-half In following runs you will only need to execute: cd stable-diffusion-webui # Optional: \"git pull\" to update the repository source venv/bin/activate # It's possible that you don't need \"--precision full\", dropping \"--no-half\" however crashes my drivers TORCH_COMMAND='pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/rocm5.1.1' python launch.py --precision full --no-half The first generation after starting the WebUI might take very long, and you might see a message similar to this: MIOpen(HIP): Warning [SQLiteBase] Missing system database file: gfx1030_40.kdb Performance may degrade. Please follow instructions to install: https://github.com/ROCmSoftwarePlatform/MIOpen#installing-miopen-kernels-package The next generations should work with regular performance. You can follow the link in the message, and if you happen to use the same operating system, follow the steps there to fix this issue. If there is no clear way to compile or install the MIOpen kernels for your operating system, consider following the â€œRunning inside Dockerâ€-guide below. Running inside Docker Pull the latest rocm/pytorch Docker image, start the image and attach to the container (taken from the rocm/pytorch documentation): docker run -it --network=host --device=/dev/kfd --device=/dev/dri --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -v $HOME/dockerx:/dockerx rocm/pytorch Execute the following inside the container: cd /dockerx git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui cd stable-diffusion-webui python -m venv venv source venv/bin/activate python -m pip install --upgrade pip wheel # It's possible that you don't need \"--precision full\", dropping \"--no-half\" however crashes my drivers TORCH_COMMAND='pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/rocm5.1.1' REQS_FILE='requirements.txt' python launch.py --precision full --no-half Following runs will only require you to restart the container, attach to it again and execute the following inside the container: Find the container name from this listing: docker container ls --all, select the one matching the rocm/pytorch image, restart it: docker container restart &lt;container-id&gt; then attach to it: `docker exec -it bash`. ```bash cd /dockerx/stable-diffusion-webui # Optional: \"git pull\" to update the repository source venv/bin/activate # It's possible that you don't need \"--precision full\", dropping \"--no-half\" however crashes my drivers TORCH_COMMAND='pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/rocm5.1.1' REQS_FILE='requirements.txt' python launch.py --precision full --no-half ``` The `/dockerx` folder inside the container should be accessible in your home directory under the same name. ## Updating Python version inside Docker If the web UI becomes incompatible with the pre-installed Python 3.7 version inside the Docker image, here are instructions on how to update it (assuming you have successfully followed \"Running inside Docker\"): Execute the following inside the container: ```bash apt install python3.9-full # Confirm every prompt update-alternatives --install /usr/local/bin/python python /usr/bin/python3.9 1 echo 'PATH=/usr/local/bin:$PATH' &gt;&gt; ~/.bashrc ``` Then restart the container and attach again. If you check `python --version` it should now say `Python 3.9.5` or newer. Run `rm -rf /dockerx/stable-diffusion-webui/venv` inside the container and then follow the steps in \"Running inside Docker\" again, skipping the `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui` and using the modified launch-command below instead: ```bash TORCH_COMMAND='pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/rocm5.1.1' python launch.py --precision full --no-half ``` It's possible that you don't need \"--precision full\", dropping \"--no-half\" however it may not work for everyone. Certain cards like the Radeon RX 6000 Series and the RX 500 Series have been will function normally without the option `--precision full --no-half`, saving plenty of vram. (noted [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/5468).) Always use this new launch-command from now on, also when restarting the web UI in following runs."
  },"/sdwui-docs/pages/en/Install-and-Run-on-NVidia-GPUs/": {
    "title": "install on Nvidia",
    "keywords": "Getting started",
    "url": "/sdwui-docs/pages/en/Install-and-Run-on-NVidia-GPUs/",
    "body": "Before attempting to install make sure all the required dependencies are met. Automatic Installation Windows Run webui-user.bat from Windows Explorer as normal, non-administrator, user. See Troubleshooting section for what to do if things go wrong. Linux To install in the default directory /home/$(whoami)/stable-diffusion-webui/, run: bash &lt;(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh) In order to customize the installation, clone the repository into the desired location, change the required variables in webui-user.sh and run : bash webui.sh Almost Automatic Installation and Launch To install the required packages via pip without creating a virtual environment, run: python launch.py Command line arguments may be passed directly, for example: python launch.py --opt-split-attention --ckpt ../secret/anime9999.ckpt Manual Installation Manual installation is very outdated and probably wonâ€™t work. check colab in the repoâ€™s readme for instructions. The following process installs everything manually on both Windows or Linux (the latter requiring dir to be replaced by ls): # install torch with CUDA support. See https://pytorch.org/get-started/locally/ for more instructions if this fails. pip install torch --extra-index-url https://download.pytorch.org/whl/cu113 # check if torch supports GPU; this must output \"True\". You need CUDA 11. installed for this. You might be able to use # a different version, but this is what I tested. python -c \"import torch; print(torch.cuda.is_available())\" # clone web ui and go into its directory git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git cd stable-diffusion-webui # clone repositories for Stable Diffusion and (optionally) CodeFormer mkdir repositories git clone https://github.com/CompVis/stable-diffusion.git repositories/stable-diffusion git clone https://github.com/CompVis/taming-transformers.git repositories/taming-transformers git clone https://github.com/sczhou/CodeFormer.git repositories/CodeFormer git clone https://github.com/salesforce/BLIP.git repositories/BLIP # install requirements of Stable Diffusion pip install transformers==4.19.2 diffusers invisible-watermark --prefer-binary # install k-diffusion pip install git+https://github.com/crowsonkb/k-diffusion.git --prefer-binary # (optional) install GFPGAN (face restoration) pip install git+https://github.com/TencentARC/GFPGAN.git --prefer-binary # (optional) install requirements for CodeFormer (face restoration) pip install -r repositories/CodeFormer/requirements.txt --prefer-binary # install requirements of web ui pip install -r requirements.txt --prefer-binary # update numpy to latest version pip install -U numpy --prefer-binary # (outside of command line) put stable diffusion model into web ui directory # the command below must output something like: 1 File(s) 4,265,380,512 bytes dir model.ckpt The installation is finished, to start the web ui, run: python webui.py Windows 11 WSL2 instructions To install under a Linux distro in Windows 11â€™s WSL2: # install conda (if not already done) wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh chmod +x Anaconda3-2022.05-Linux-x86_64.sh ./Anaconda3-2022.05-Linux-x86_64.sh # Clone webui repo git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git cd stable-diffusion-webui # Create and activate conda env conda env create -f environment-wsl2.yaml conda activate automatic At this point, the instructions for the Manual installation may be applied starting at step # clone repositories for Stable Diffusion and (optionally) CodeFormer. Alternative installation on Windows using Conda Prerequisites *(Only needed if you do not have them)*. Assumes Chocolatey is installed. # install git choco install git # install conda choco install anaconda3 Optional parameters: git, conda Install (warning: some files exceed multiple gigabytes, make sure you have space first) Download as .zip and extract or use git to clone. git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git Launch the Anaconda prompt. It should be noted that you can use older Python versions, but you may be forced to manually remove features like cache optimization, which will degrade your performance. # Navigate to the git directory cd \"GIT\\StableDiffusion\" # Create environment conda create -n StableDiffusion python=3.10.6 # Activate environment conda activate StableDiffusion # Validate environment is selected conda env list # Start local webserver webui-user.bat # Wait for \"Running on local URL: http://127.0.0.1:7860\" and open that URI. *(Optional)* Go to CompVis and download latest model, for example 1.4 and unpack it to ex: GIT\\StableDiffusion\\models\\Stable-diffusion after that restart the server by restarting Anaconda prompt and webui-user.bat Alternative defaults worth trying out: Try euler a (Ancestral Euler) with higher Sampling Steps ex: 40 or others with 100. Set â€œSettings &gt; User interface &gt; Show image creation progress every N sampling stepsâ€ to 1 and pick a deterministic Seed value. Can visually see how image defusion happens and record a .gif with ScreenToGif. Use Restore faces. Generally, better results, but that quality comes at the cost of speed."
  },"/sdwui-docs/pages/en/Installation-on-Apple-Silicon/": {
    "title": "install on Apple",
    "keywords": "Getting started",
    "url": "/sdwui-docs/pages/en/Installation-on-Apple-Silicon/",
    "body": "Mac users: Please provide feedback on if these instructions do or donâ€™t work for you, and if anything is unclear or you are otherwise still having problems with your install that are not currently mentioned here. Important notes Currently most functionality in the web UI works correctly on macOS, with the most notable exceptions being CLIP interrogator and training. Although training does seem to work, it is incredibly slow and consumes an excessive amount of memory. CLIP interrogator can be used but it doesnâ€™t work correctly with the GPU acceleration macOS uses so the default configuration will run it entirely via CPU (which is slow). Most samplers are known to work with the only exception being the PLMS sampler when using the Stable Diffusion 2.0 model. Generated images with GPU acceleration on macOS should usually match or almost match generated images on CPU with the same settings and seed. Automatic installation New install: If Homebrew is not installed, follow the instructions at https://brew.sh to install it. Keep the terminal window open and follow the instructions under â€œNext stepsâ€ to add Homebrew to your PATH. Open a new terminal window and run brew install cmake protobuf rust python@3.10 git wget Clone the web UI repository by running git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui Place Stable Diffusion models/checkpoints you want to use into stable-diffusion-webui/models/Stable-diffusion. If you donâ€™t have any, see Downloading Stable Diffusion Models below. cd stable-diffusion-webui and then ./webui.sh to run the web UI. A Python virtual environment will be created and activated using venv and any remaining missing dependencies will be automatically downloaded and installed. To relaunch the web UI process later, run ./webui.sh again. Note that it doesnâ€™t auto update the web UI; to update, run git pull before running ./webui.sh. Existing Install: If you have an existing install of web UI that was created with setup_mac.sh, delete the run_webui_mac.sh file and repositories folder from your stable-diffusion-webui folder. Then run git pull to update web UI and then ./webui.sh to run it. Downloading Stable Diffusion Models If you donâ€™t have any models to use, Stable Diffusion models can be downloaded from Hugging Face. To download, click on a model and then click on the Files and versions header. Look for files listed with the â€œ.ckptâ€ or â€œ.safetensorsâ€ extensions, and then click the down arrow to the right of the file size to download them. Some popular official Stable Diffusion models are: Stable DIffusion 1.4 (sd-v1-4.ckpt) Stable Diffusion 1.5 (v1-5-pruned-emaonly.ckpt) Stable Diffusion 1.5 Inpainting (sd-v1-5-inpainting.ckpt) Stable Diffusion 2.0 and 2.1 require both a model and a configuration file, and image width &amp; height will need to be set to 768 or higher when generating images: Stable Diffusion 2.0 (768-v-ema.ckpt) Stable Diffusion 2.1 (v2-1_768-ema-pruned.ckpt) For the configuration file, hold down the option key on the keyboard and click here to download v2-inference-v.yaml (it may download as v2-inference-v.yaml.yml). In Finder select that file then go to the menu and select File &gt; Get Info. In the window that appears select the filename and change it to the filename of the model, except with the file extension .yaml instead of .ckpt, press return on the keyboard (confirm changing the file extension if prompted), and place it in the same folder as the model (e.g. if you downloaded the 768-v-ema.ckpt model, rename it to 768-v-ema.yaml and put it in stable-diffusion-webui/models/Stable-diffusion along with the model). Also available is a Stable Diffusion 2.0 depth model (512-depth-ema.ckpt). Download the v2-midas-inference.yaml configuration file by holding down option on the keyboard and clicking here, then rename it with the .yaml extension in the same way as mentioned above and put it in stable-diffusion-webui/models/Stable-diffusion along with the model. Note that this model works at image dimensions of 512 width/height or higher instead of 768. Troubleshooting Web UI Wonâ€™t Start: If you encounter errors when trying to start the Web UI with ./webui.sh, try deleting the repositories and venv folders from your stable-diffusion-webui folder and then update web UI with git pull before running ./webui.sh again. Poor Performance: Currently GPU acceleration on macOS uses a lot of memory. If performance is poor (if it takes more than a minute to generate a 512x512 image with 20 steps with any sampler) first try starting with the --opt-split-attention-v1 command line option (i.e. ./webui.sh --opt-split-attention-v1) and see if that helps. If that doesnâ€™t make much difference, then open the Activity Monitor application located in /Applications/Utilities and check the memory pressure graph under the Memory tab. If memory pressure is being displayed in red when an image is generated, close the web UI process and then add the --medvram command line option (i.e. ./webui.sh --opt-split-attention-v1 --medvram). If performance is still poor and memory pressure still red with that option, then instead try --lowvram (i.e. ./webui.sh --opt-split-attention-v1 --lowvram). If it still takes more than a few minutes to generate a 512x512 image with 20 steps with with any sampler, then you may need to turn off GPU acceleration. Open webui-user.sh in Xcode and change #export COMMANDLINE_ARGS=\"\" to export COMMANDLINE_ARGS=\"--skip-torch-cuda-test --no-half --use-cpu all\". Discussions/Feedback here: https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/5461"
  },"/sdwui-docs/pages/en/List-of-Time-Zones/": {
    "title": "List of time zones",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/List-of-Time-Zones/",
    "body": "You can generate the list of valid time zones by running this Python Script import pytz for tz in pytz.all_timezones: print(tz) Or you can reference this this pre-generated list (might be outdated). Africa/Accra Africa/Addis_Ababa Africa/Algiers Africa/Asmara Africa/Asmera Africa/Bamako Africa/Bangui Africa/Banjul Africa/Bissau Africa/Blantyre Africa/Brazzaville Africa/Bujumbura Africa/Cairo Africa/Casablanca Africa/Ceuta Africa/Conakry Africa/Dakar Africa/Dar_es_Salaam Africa/Djibouti Africa/Douala Africa/El_Aaiun Africa/Freetown Africa/Gaborone Africa/Harare Africa/Johannesburg Africa/Juba Africa/Kampala Africa/Khartoum Africa/Kigali Africa/Kinshasa Africa/Lagos Africa/Libreville Africa/Lome Africa/Luanda Africa/Lubumbashi Africa/Lusaka Africa/Malabo Africa/Maputo Africa/Maseru Africa/Mbabane Africa/Mogadishu Africa/Monrovia Africa/Nairobi Africa/Ndjamena Africa/Niamey Africa/Nouakchott Africa/Ouagadougou Africa/Porto-Novo Africa/Sao_Tome Africa/Timbuktu Africa/Tripoli Africa/Tunis Africa/Windhoek America/Adak America/Anchorage America/Anguilla America/Antigua America/Araguaina America/Argentina/Buenos_Aires America/Argentina/Catamarca America/Argentina/ComodRivadavia America/Argentina/Cordoba America/Argentina/Jujuy America/Argentina/La_Rioja America/Argentina/Mendoza America/Argentina/Rio_Gallegos America/Argentina/Salta America/Argentina/San_Juan America/Argentina/San_Luis America/Argentina/Tucuman America/Argentina/Ushuaia America/Aruba America/Asuncion America/Atikokan America/Atka America/Bahia America/Bahia_Banderas America/Barbados America/Belem America/Belize America/Blanc-Sablon America/Boa_Vista America/Bogota America/Boise America/Buenos_Aires America/Cambridge_Bay America/Campo_Grande America/Cancun America/Caracas America/Catamarca America/Cayenne America/Cayman America/Chicago America/Chihuahua America/Coral_Harbour America/Cordoba America/Costa_Rica America/Creston America/Cuiaba America/Curacao America/Danmarkshavn America/Dawson America/Dawson_Creek America/Denver America/Detroit America/Dominica America/Edmonton America/Eirunepe America/El_Salvador America/Ensenada America/Fort_Nelson America/Fort_Wayne America/Fortaleza America/Glace_Bay America/Godthab America/Goose_Bay America/Grand_Turk America/Grenada America/Guadeloupe America/Guatemala America/Guayaquil America/Guyana America/Halifax America/Havana America/Hermosillo America/Indiana/Indianapolis America/Indiana/Knox America/Indiana/Marengo America/Indiana/Petersburg America/Indiana/Tell_City America/Indiana/Vevay America/Indiana/Vincennes America/Indiana/Winamac America/Indianapolis America/Inuvik America/Iqaluit America/Jamaica America/Jujuy America/Juneau America/Kentucky/Louisville America/Kentucky/Monticello America/Knox_IN America/Kralendijk America/La_Paz America/Lima America/Los_Angeles America/Louisville America/Lower_Princes America/Maceio America/Managua America/Manaus America/Marigot America/Martinique America/Matamoros America/Mazatlan America/Mendoza America/Menominee America/Merida America/Metlakatla America/Mexico_City America/Miquelon America/Moncton America/Monterrey America/Montevideo America/Montreal America/Montserrat America/Nassau America/New_York America/Nipigon America/Nome America/Noronha America/North_Dakota/Beulah America/North_Dakota/Center America/North_Dakota/New_Salem America/Nuuk America/Ojinaga America/Panama America/Pangnirtung America/Paramaribo America/Phoenix America/Port-au-Prince America/Port_of_Spain America/Porto_Acre America/Porto_Velho America/Puerto_Rico America/Punta_Arenas America/Rainy_River America/Rankin_Inlet America/Recife America/Regina America/Resolute America/Rio_Branco America/Rosario America/Santa_Isabel America/Santarem America/Santiago America/Santo_Domingo America/Sao_Paulo America/Scoresbysund America/Shiprock America/Sitka America/St_Barthelemy America/St_Johns America/St_Kitts America/St_Lucia America/St_Thomas America/St_Vincent America/Swift_Current America/Tegucigalpa America/Thule America/Thunder_Bay America/Tijuana America/Toronto America/Tortola America/Vancouver America/Virgin America/Whitehorse America/Winnipeg America/Yakutat America/Yellowknife Antarctica/Casey Antarctica/Davis Antarctica/DumontDUrville Antarctica/Macquarie Antarctica/Mawson Antarctica/McMurdo Antarctica/Palmer Antarctica/Rothera Antarctica/South_Pole Antarctica/Syowa Antarctica/Troll Antarctica/Vostok Arctic/Longyearbyen Asia/Aden Asia/Almaty Asia/Amman Asia/Anadyr Asia/Aqtau Asia/Aqtobe Asia/Ashgabat Asia/Ashkhabad Asia/Atyrau Asia/Baghdad Asia/Bahrain Asia/Baku Asia/Bangkok Asia/Barnaul Asia/Beirut Asia/Bishkek Asia/Brunei Asia/Calcutta Asia/Chita Asia/Choibalsan Asia/Chongqing Asia/Chungking Asia/Colombo Asia/Dacca Asia/Damascus Asia/Dhaka Asia/Dili Asia/Dubai Asia/Dushanbe Asia/Famagusta Asia/Gaza Asia/Harbin Asia/Hebron Asia/Ho_Chi_Minh Asia/Hong_Kong Asia/Hovd Asia/Irkutsk Asia/Istanbul Asia/Jakarta Asia/Jayapura Asia/Jerusalem Asia/Kabul Asia/Kamchatka Asia/Karachi Asia/Kashgar Asia/Kathmandu Asia/Katmandu Asia/Khandyga Asia/Kolkata Asia/Krasnoyarsk Asia/Kuala_Lumpur Asia/Kuching Asia/Kuwait Asia/Macao Asia/Macau Asia/Magadan Asia/Makassar Asia/Manila Asia/Muscat Asia/Nicosia Asia/Novokuznetsk Asia/Novosibirsk Asia/Omsk Asia/Oral Asia/Phnom_Penh Asia/Pontianak Asia/Pyongyang Asia/Qatar Asia/Qostanay Asia/Qyzylorda Asia/Rangoon Asia/Riyadh Asia/Saigon Asia/Sakhalin Asia/Samarkand Asia/Seoul Asia/Shanghai Asia/Singapore Asia/Srednekolymsk Asia/Taipei Asia/Tashkent Asia/Tbilisi Asia/Tehran Asia/Tel_Aviv Asia/Thimbu Asia/Thimphu Asia/Tokyo Asia/Tomsk Asia/Ujung_Pandang Asia/Ulaanbaatar Asia/Ulan_Bator Asia/Urumqi Asia/Ust-Nera Asia/Vientiane Asia/Vladivostok Asia/Yakutsk Asia/Yangon Asia/Yekaterinburg Asia/Yerevan Atlantic/Azores Atlantic/Bermuda Atlantic/Canary Atlantic/Cape_Verde Atlantic/Faeroe Atlantic/Faroe Atlantic/Jan_Mayen Atlantic/Madeira Atlantic/Reykjavik Atlantic/South_Georgia Atlantic/St_Helena Atlantic/Stanley Australia/ACT Australia/Adelaide Australia/Brisbane Australia/Broken_Hill Australia/Canberra Australia/Currie Australia/Darwin Australia/Eucla Australia/Hobart Australia/LHI Australia/Lindeman Australia/Lord_Howe Australia/Melbourne Australia/NSW Australia/North Australia/Perth Australia/Queensland Australia/South Australia/Sydney Australia/Tasmania Australia/Victoria Australia/West Australia/Yancowinna Brazil/Acre Brazil/DeNoronha Brazil/East Brazil/West CET CST6CDT Canada/Atlantic Canada/Central Canada/Eastern Canada/Mountain Canada/Newfoundland Canada/Pacific Canada/Saskatchewan Canada/Yukon Chile/Continental Chile/EasterIsland Cuba EET EST EST5EDT Egypt Eire Etc/GMT Etc/GMT+0 Etc/GMT+1 Etc/GMT+10 Etc/GMT+11 Etc/GMT+12 Etc/GMT+2 Etc/GMT+3 Etc/GMT+4 Etc/GMT+5 Etc/GMT+6 Etc/GMT+7 Etc/GMT+8 Etc/GMT+9 Etc/GMT-0 Etc/GMT-1 Etc/GMT-10 Etc/GMT-11 Etc/GMT-12 Etc/GMT-13 Etc/GMT-14 Etc/GMT-2 Etc/GMT-3 Etc/GMT-4 Etc/GMT-5 Etc/GMT-6 Etc/GMT-7 Etc/GMT-8 Etc/GMT-9 Etc/GMT0 Etc/Greenwich Etc/UCT Etc/UTC Etc/Universal Etc/Zulu Europe/Amsterdam Europe/Andorra Europe/Astrakhan Europe/Athens Europe/Belfast Europe/Belgrade Europe/Berlin Europe/Bratislava Europe/Brussels Europe/Bucharest Europe/Budapest Europe/Busingen Europe/Chisinau Europe/Copenhagen Europe/Dublin Europe/Gibraltar Europe/Guernsey Europe/Helsinki Europe/Isle_of_Man Europe/Istanbul Europe/Jersey Europe/Kaliningrad Europe/Kiev Europe/Kirov Europe/Kyiv Europe/Lisbon Europe/Ljubljana Europe/London Europe/Luxembourg Europe/Madrid Europe/Malta Europe/Mariehamn Europe/Minsk Europe/Monaco Europe/Moscow Europe/Nicosia Europe/Oslo Europe/Paris Europe/Podgorica Europe/Prague Europe/Riga Europe/Rome Europe/Samara Europe/San_Marino Europe/Sarajevo Europe/Saratov Europe/Simferopol Europe/Skopje Europe/Sofia Europe/Stockholm Europe/Tallinn Europe/Tirane Europe/Tiraspol Europe/Ulyanovsk Europe/Uzhgorod Europe/Vaduz Europe/Vatican Europe/Vienna Europe/Vilnius Europe/Volgograd Europe/Warsaw Europe/Zagreb Europe/Zaporozhye Europe/Zurich GB GB-Eire GMT GMT+0 GMT-0 GMT0 Greenwich HST Hongkong Iceland Indian/Antananarivo Indian/Chagos Indian/Christmas Indian/Cocos Indian/Comoro Indian/Kerguelen Indian/Mahe Indian/Maldives Indian/Mauritius Indian/Mayotte Indian/Reunion Iran Israel Jamaica Japan Kwajalein Libya MET MST MST7MDT Mexico/BajaNorte Mexico/BajaSur Mexico/General NZ NZ-CHAT Navajo PRC PST8PDT Pacific/Apia Pacific/Auckland Pacific/Bougainville Pacific/Chatham Pacific/Chuuk Pacific/Easter Pacific/Efate Pacific/Enderbury Pacific/Fakaofo Pacific/Fiji Pacific/Funafuti Pacific/Galapagos Pacific/Gambier Pacific/Guadalcanal Pacific/Guam Pacific/Honolulu Pacific/Johnston Pacific/Kanton Pacific/Kiritimati Pacific/Kosrae Pacific/Kwajalein Pacific/Majuro Pacific/Marquesas Pacific/Midway Pacific/Nauru Pacific/Niue Pacific/Norfolk Pacific/Noumea Pacific/Pago_Pago Pacific/Palau Pacific/Pitcairn Pacific/Pohnpei Pacific/Ponape Pacific/Port_Moresby Pacific/Rarotonga Pacific/Saipan Pacific/Samoa Pacific/Tahiti Pacific/Tarawa Pacific/Tongatapu Pacific/Truk Pacific/Wake Pacific/Wallis Pacific/Yap Poland Portugal ROC ROK Singapore Turkey UCT US/Alaska US/Aleutian US/Arizona US/Central US/East-Indiana US/Eastern US/Hawaii US/Indiana-Starke US/Michigan US/Mountain US/Pacific US/Samoa UTC Universal W-SU WET Zulu"
  },"/sdwui-docs/pages/en/Localization/": {
    "title": "Localization",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Localization/",
    "body": "Using localization files The intended way to do localizations now is via extensions. See: https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Developing-extensions Creating localization files Go to settings and click Download localization template button at the bottom. This will download a template for localization that you can edit. Updating old localization with new keys This repository contrains old orphaned localizations. If you wish to update them with new keys, you can use the following script: import json files=['localization_template.json', 'old_localization.json'] with open('new_localization.json', \"w\") as outfile: res = dict() for f in files: dct = dict(json.load(open(f, \"r\").read())) res.update(dct) outfile.write(res) Then translate what was added."
  },"/sdwui-docs/pages/en/Negative-prompt/": {
    "title": "Negative prompt",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Negative-prompt/",
    "body": "Negative prompt is a way to use the Stable Diffusion in a way that allows the user to specify what he doesnâ€™t want to see, without any extra load or requirements for the model. As far as I know, I was the first to use this approach; the commit that adds it is 757bb7c4. The feature has found extreme popularity among users who remove the usual deformities of Stable Diffusion like extra limbs with it. In addition to just being able to specify what you donâ€™t want to see, which sometimes is possible via usual prompt, and sometimes isnâ€™t, this allows you to do that without using any of your allowance of 75 tokens the prompt consists of. The way negative prompt works is by using user-specified text instead of empty string for unconditional_conditioning when doing sampling. Hereâ€™s the (simplified) code from txt2img.py: # prompts = [\"a castle in a forest\"] # batch_size = 1 c = model.get_learned_conditioning(prompts) uc = model.get_learned_conditioning(batch_size * [\"\"]) samples_ddim, _ = sampler.sample(conditioning=c, unconditional_conditioning=uc, [...]) This launches the sampler that repeatedly: de-noises the picture guiding it to look more like your prompt (conditioning) de-noises the picture guiding it to look more like an empty prompt (unconditional_conditioning) looks at difference between those and uses it to produce a set of changes for the noisy picture (different samplers do that part differently) To use negative prompt, all thatâ€™s needed is this: # prompts = [\"a castle in a forest\"] # negative_prompts = [\"grainy, fog\"] c = model.get_learned_conditioning(prompts) uc = model.get_learned_conditioning(negative_prompts) samples_ddim, _ = sampler.sample(conditioning=c, unconditional_conditioning=uc, [...]) The sampler then will look at differences between image de-noised to look like your prompt (a castle), and an image de-noised to look like your negative prompt (grainy, fog), and try to move the final results towards the former and away from latter. Examples: a colorful photo of a castle in the middle of a forest with trees and (((bushes))), by Ismail Inceoglu, ((((shadows)))), ((((high contrast)))), dynamic shading, ((hdr)), detailed vegetation, digital painting, digital drawing, detailed painting, a detailed digital painting, gothic art, featured on deviantart Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 749109862, Size: 896x448, Model hash: 7460a6fa negative prompt image none fog grainy fog, grainy fog, grainy, purple"
  },"/sdwui-docs/online-services/": {
    "title": "Online services",
    "keywords": "Getting started",
    "url": "/sdwui-docs/online-services/",
    "body": "Google Colab maintained by TheLastBen maintained by camenduru maintained by ddPn08 maintained by Akaibu Colab, original by AUTOMATIC1111, outdated. Russian colab, maintained by PR0LAPSE Paperspace maintained by Cyberes Kaggle maintained by camenduru SageMaker Studio Lab maintained by fractality Hugging Face maintained by camenduru Installation Guides azure-ml - (commit)"
  },"/sdwui-docs/pages/en/Optimizations/": {
    "title": "Optimizations",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Optimizations/",
    "body": "A number of optimization can be enabled by commandline arguments: commandline argument explanation --xformers Use xformers library. Great improvement to memory consumption and speed. Windows version installs binaries mainained by C43H66N12O12S2. Will only be enabled on small subset of configuration because thatâ€™s what we have binaries for. Documentation --force-enable-xformers Enables xformers above regardless of whether the program thinks you can run it or not. Do not report bugs you get running this. --opt-split-attention Cross attention layer optimization significantly reducing memory use for almost no cost (some report improved preformance with it). Black magic. On by default for torch.cuda, which includes both NVidia and AMD cards. --disable-opt-split-attention Disables the optimization above. --opt-split-attention-v1 Uses an older version of the optimization above that is not as memory hungry (it will use less VRAM, but will be more limiting in the maximum size of pictures you can make). --medvram Makes the Stable Diffusion model consume less VRAM by splitting it into three parts - cond (for transforming text into numerical representation), first_stage (for converting a picture into latent space and back), and unet (for actual denoising of latent space) and making it so that only one is in VRAM at all times, sending others to CPU RAM. Lowers performance, but only by a bit - except if live previews are enabled. --lowvram An even more thorough optimization of the above, splitting unet into many modules, and only one module is kept in VRAM. Devastating for performance. *do-not-batch-cond-uncond Prevents batching of positive and negative prompts during sampling, which essentially lets you run at 0.5 batch size, saving a lot of memory. Decreases performance. Not a command line option, but an optimization implicitly enabled by using --medvram or --lowvram. --always-batch-cond-uncond Disables the optimization above. Only makes sense together with --medvram or --lowvram --opt-channelslast Changes torch memory type for stable diffusion to channels last. Effects not closely studied. Extra tips (Windows): https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/3889 Disable Hardware GPU scheduling. disable browser hardware acceleration Go in nvidia control panel, 3d parameters, and change power profile to â€œmaximum performanceâ€"
  },"/sdwui-docs/": {
    "title": "Stable Diffusion webUI",
    "keywords": "Getting started",
    "url": "/sdwui-docs/",
    "body": "A browser interface based on Gradio library for Stable Diffusion. Check the custom scripts wiki page for extra scripts developed by users. Features Detailed feature showcase with images: Original txt2img and img2img modes One click install and run script (but you still must install python and git) Outpainting Inpainting Color Sketch Prompt Matrix Stable Diffusion Upscale Attention, specify parts of text that the model should pay more attention to a man in a ((tuxedo)) - will pay more attention to tuxedo a man in a (tuxedo:1.21) - alternative syntax select text and press ctrl+up or ctrl+down to automatically adjust attention to selected text (code contributed by anonymous user) Loopback, run img2img processing multiple times X/Y plot, a way to draw a 2 dimensional plot of images with different parameters Textual Inversion have as many embeddings as you want and use any names you like for them use multiple embeddings with different numbers of vectors per token works with half precision floating point numbers train embeddings on 8GB (also reports of 6GB working) Extras tab with: GFPGAN, neural network that fixes faces CodeFormer, face restoration tool as an alternative to GFPGAN RealESRGAN, neural network upscaler ESRGAN, neural network upscaler with a lot of third party models SwinIR and Swin2SR(see here), neural network upscalers LDSR, Latent diffusion super resolution upscaling Resizing aspect ratio options Sampling method selection Adjust sampler eta values (noise multiplier) More advanced noise setting options Interrupt processing at any time 4GB video card support (also reports of 2GB working) Correct seeds for batches Live prompt token length validation Generation parameters parameters you used to generate images are saved with that image in PNG chunks for PNG, in EXIF for JPEG can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI can be disabled in settings drag and drop an image/text-parameters to promptbox Read Generation Parameters Button, loads parameters in promptbox to UI Settings page Running arbitrary python code from UI (must run with â€“allow-code to enable) Mouseover hints for most UI elements Possible to change defaults/mix/max/step values for UI elements via text config Random artist button Tiling support, a checkbox to create images that can be tiled like textures Progress bar and live image generation preview Negative prompt, an extra text field that allows you to list what you donâ€™t want to see in generated image Styles, a way to save part of prompt and easily apply them via dropdown later Variations, a way to generate same image but with tiny differences Seed resizing, a way to generate same image but at slightly different resolution CLIP interrogator, a button that tries to guess prompt from an image Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway Batch Processing, process a group of files using img2img Img2img Alternative, reverse Euler method of cross attention control Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions Reloading checkpoints on the fly Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one Custom scripts with many extensions from community Composable-Diffusion, a way to use multiple prompts at once separate prompts using uppercase AND also supports weights for prompts: a cat :1.2 AND a dog AND a penguin :2.2 No token limit for prompts (original stable diffusion lets you use up to 75 tokens) DeepDanbooru integration, creates danbooru style tags for anime prompts xformers, major speed increase for select cards: (add â€“xformers to commandline args) via extension: History tab: view, direct and delete images conveniently within the UI Generate forever option Training tab hypernetworks and embeddings options Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime) Clip skip Use Hypernetworks Use VAEs Estimated completion time in progress bar API Support for dedicated inpainting model by RunwayML. via extension: Aesthetic Gradients, a way to generate images with a specific aesthetic by using clip images embeds (implementation of https://github.com/vicgalle/stable-diffusion-aesthetic-gradients) Stable Diffusion 2.0 support - see wiki for instructions Installation and Running Make sure the required dependencies are met and follow the instructions available for both NVidia (recommended) and AMD GPUs. Alternatively, use online services (like Google Colab): List of Online Services Automatic Installation on Windows Install Python 3.10.6, checking â€œAdd Python to PATHâ€ Install git. Download the stable-diffusion-webui repository, for example by running git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git. Place model.ckpt in the models directory (see dependencies for where to get it). *(Optional)* Place GFPGANv1.4.pth in the base directory, alongside webui.py (see dependencies for where to get it). Run webui-user.bat from Windows Explorer as normal, non-administrator, user. Automatic Installation on Linux Install the dependencies: # Debian-based: sudo apt install wget git python3 python3-venv # Red Hat-based: sudo dnf install wget git python3 # Arch-based: sudo pacman -S wget git python3 To install in /home/$(whoami)/stable-diffusion-webui/, run: bash &lt;(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh) Installation on Apple Silicon Find the instructions here. Contributing Hereâ€™s how to add code to this repo: Contributing Documentation The documentation was moved from this README over to the projectâ€™s wiki. Credits Stable Diffusion - https://github.com/CompVis/stable-diffusion, https://github.com/CompVis/taming-transformers k-diffusion - https://github.com/crowsonkb/k-diffusion.git GFPGAN - https://github.com/TencentARC/GFPGAN.git CodeFormer - https://github.com/sczhou/CodeFormer ESRGAN - https://github.com/xinntao/ESRGAN SwinIR - https://github.com/JingyunLiang/SwinIR Swin2SR - https://github.com/mv-lab/swin2sr LDSR - https://github.com/Hafiidz/latent-diffusion MiDaS - https://github.com/isl-org/MiDaS Ideas for optimizations - https://github.com/basujindal/stable-diffusion Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing. Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion) Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (weâ€™re not using his code, but we are using his ideas). Idea for SD upscale - https://github.com/jquesnelle/txt2imghd Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch xformers - https://github.com/facebookresearch/xformers DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru Security advice - RyotaK Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user. (You)"
  },"/sdwui-docs/pages/en/Tests/": {
    "title": "Optimizations",
    "keywords": "development",
    "url": "/sdwui-docs/pages/en/Tests/",
    "body": "There are tests that just verify that basic image creation works vi API. To run tests, add --tests as a commandline argument to launch.py along with your other command line arguments: python launch.py --skip-torch-cuda-test --deepdanbooru --no-half-vae --tests Youâ€™ll find outputs of main program in test/stdout.txt and test/stderr.txt."
  },"/sdwui-docs/pages/en/Textual-Inversion/": {
    "title": "Textual inversion",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Textual-Inversion/",
    "body": "What is Textual Inversion? Textual Inversion allows you to train a tiny part of the neural network on your own pictures, and use results when generating new ones. In this context, embedding is the name of the tiny bit of the neural network you trained. The result of the training is a .pt or a .bin file (former is the format used by original author, latter is by the diffusers library) with the embedding in it. See original site for more details about what textual inversion is: https://textual-inversion.github.io/. Using pre-trained embeddings Put the embedding into the embeddings directory and use its filename in the prompt. You donâ€™t have to restart the program for this to work. As an example, here is an embedding of Usada Pekora I trained on WD1.2 model, on 53 pictures (119 augmented) for 19500 steps, with 8 vectors per token setting. Pictures it generates: portrait of usada pekora Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 45dee52b You can combine multiple embeddings in one prompt: portrait of usada pekora, mignon Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 45dee52b Be very careful about which model you are using with your embeddings: they work well with the model you used during training, and not so well on different models. For example, here is the above embedding and vanilla 1.4 stable diffusion model: portrait of usada pekora Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 7460a6fa Training embeddings Textual inversion tab Experimental support for training embeddings in user interface. create a new empty embedding, select directory with images, train the embedding on it the feature is very raw, use at own risk i was able to reproduce results I got with other repos in training anime artists as styles, after few tens of thousands steps works with half precision floats, but needs experimentation to see if results will be just as good if you have enough memory, safer to run with --no-half --precision full Section for UI to run preprocessing for images automatically. you can interrupt and resume training without any loss of data (except for AdamW optimization parameters, but it seems none of existing repos save those anyway so the general opinion is they are not important) no support for batch sizes or gradient accumulation it should not be possible to run this with --lowvram and --medvram flags. Explanation for parameters Creating an embedding Name: filename for the created embedding. You will also use this text in prompts when referring to the embedding. Initialization text: the embedding you create will initially be filled with vectors of this text. If you create a one vector embedding named â€œzzzz1234â€ with â€œtreeâ€ as initialization text, and use it in prompt without training, then prompt â€œa zzzz1234 by monetâ€ will produce same pictures as â€œa tree by monetâ€. Number of vectors per token: the size of embedding. The larger this value, the more information about subject you can fit into the embedding, but also the more words it will take away from your prompt allowance. With stable diffusion, you have a limit of 75 tokens in the prompt. If you use an embedding with 16 vectors in a prompt, that will leave you with space for 75 - 16 = 59. Also from my experience, the larger the number of vectors, the more pictures you need to obtain good results. Preprocess This takes images from a directory, processes them to be ready for textual inversion, and writes results to another directory. This is a convenience feature and you can preprocess pictures yourself if you wish. Source directory: directory with images Destination directory: directory where the results will be written Create flipped copies: for each image, also write its mirrored copy Split oversized images into two: if the image is too tall or wide, resize it to have the short side match the desired resolution, and create two, possibly intersecting pictures out of it. Use BLIP caption as filename: use BLIP model from the interrogator to add a caption to the filename. Training an embedding Embedding: select the embedding you want to train from this dropdown. Learning rate: how fast should the training go. The danger of setting this parameter to a high value is that you may break the embedding if you set it too high. If you see Loss: nan in the training info textbox, that means you failed and the embedding is dead. With the default value, this should not happen. Itâ€™s possible to specify multiple learning rates in this setting using the following syntax: 0.005:100, 1e-3:1000, 1e-5 - this will train with lr of 0.005 for first 100 steps, then 1e-3 until 1000 steps, then 1e-5 until the end. Dataset directory: directory with images for training. They all must be square. Log directory: sample images and copies of partially trained embeddings will be written to this directory. Prompt template file: text file with prompts, one per line, for training the model on. See files in directory textual_inversion_templates for what you can do with those. Use style.txt when training styles, and subject.txt when training object embeddings. Following tags can be used in the file: [name]: the name of embedding [filewords]: words from the file name of the image from the dataset. See below for more info. Max steps: training will stop after this many steps have been completed. A step is when one picture (or one batch of pictures, but batches are currently not supported) is shown to the model and is used to improve embedding. if you interrupt training and resume it at a later date, the number of steps is preserved. Save images with embedding in PNG chunks: every time an image is generated it is combined with the most recently logged embedding and saved to image_embeddings in a format that can be both shared as an image, and placed into your embeddings folder and loaded. Preview prompt: if not empty, this prompt will be used to generate preview pictures. If empty, the prompt from training will be used. filewords [filewords] is a tag for prompt template file that allows you to insert text from filename into the prompt. By default, fileâ€™s extension is removed, as well as all numbers and dashes (-) at the start of filename. So this filename: 000001-1-a man in suit.png will become this text for prompt: a man in suit. Formatting of the text in the filename is left as it is. Itâ€™s possible to use options Filename word regex and Filename join string to alter the text from filename: for example, with word regex = \\w+ and join string = , , the file from above will produce this text: a, man, in, suit. regex is used to extract words from text (and they are ['a', 'man', 'in', 'suit', ]), and join string (â€˜, â€˜) is placed between those words to create one text: a, man, in, suit. Itâ€™s also possible to make a text file with same filename as image (000001-1-a man in suit.txt) and just put the prompt text there. The filename and regex options will not be used. Third party repos I successfully trained embeddings using those repositories: nicolai256 lstein Other options are to train on colabs and/or using diffusers library, which I know nothing about. Finding embeddings online huggingface concepts library - a lot of different embeddings, but 16777216c - NSFW, anime artist styles by a mysterious stranger. cattoroboto - some anime embeddings by anon. viper1 - NSFW, furry girls. anonâ€™s embeddings - NSFW, anime artists. rentry - a page with links to embeddings from many sources. Hypernetworks Hypernetworks is a novel (get it?) concept for fine tuning a model without touching any of its weights. The current way to train hypernets is in the textual inversion tab. Training works the same way as with textual inversion. The only requirement is to use a very, very low learning rate, something like 0.000005 or 0.0000005. Dum Dum Guide An anonymous user has written a guide with pictures for using hypernetworks: https://rentry.org/hypernetwork4dumdums Unload VAE and CLIP from VRAM when training This option on settings tab allows you to save some memoryat the cost of slower preview picture generation."
  },"/sdwui-docs/pages/en/Troubleshooting/": {
    "title": "Troubleshooting",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Troubleshooting/",
    "body": "The program is tested to work on Python 3.10.6. Donâ€™t use other versions unless you are looking for trouble. The installer creates a python virtual environment, so none of the installed modules will affect existing system installations of python. To use the systemâ€™s python rather than creating a virtual environment, use custom parameter replacing set VENV_DIR=-. To reinstall from scratch, delete directories: venv, repositories. When starting the program for the first time, the path to python interpreter is displayed. If this is not the python you installed, you can specify full path in the webui-user script; see Running with custom parameters. If the desired version of Python is not in PATH, modify the line set PYTHON=python in webui-user.bat with the full path to the python executable. Example: set PYTHON=B:\\soft\\Python310\\python.exe Installer requirements from requirements_versions.txt, which lists versions for modules specifically compatible with Python 3.10.6. If this doesnâ€™t work with other versions of Python, setting the custom parameter set REQS_FILE=requirements.txt may help. Low VRAM Video-cards When running on video cards with a low amount of VRAM (&lt;=4GB), out of memory errors may arise. Various optimizations may be enabled through command line arguments, sacrificing some/a lot of speed in favor of using less VRAM: If you have 4GB VRAM and want to make 512x512 (or maybe up to 640x640) images, use --medvram. If you have 4GB VRAM and want to make 512x512 images, but you get an out of memory error with --medvram, use --medvram --opt-split-attention instead. If you have 4GB VRAM and want to make 512x512 images, and you still get an out of memory error, use --lowvram --always-batch-cond-uncond --opt-split-attention instead. If you have 4GB VRAM and want to make images larger than you can with --medvram, use --lowvram --opt-split-attention. If you have more VRAM and want to make larger images than you can usually make (for example 1024x1024 instead of 512x512), use --medvram --opt-split-attention. You can use --lowvram also but the effect will likely be barely noticeable. Otherwise, do not use any of those. Green or Black screen Video cards When running on video cards which donâ€™t support half precision floating point numbers (a known issue with 16xx cards), a green or black screen may appear instead of the generated pictures. This may be fixed by using the command line arguments --precision full --no-half at a significant increase in VRAM usage, which may require --medvram. â€œCUDA error: no kernel image is available for execution on the deviceâ€ after enabling xformers Your installed xformers is incompatible with your GPU. If you use Python 3.10, have a Pascal or higher card and run on Windows, add --reinstall-xformers --xformers to your COMMANDLINE_ARGS to upgrade to a working version. Remove --reinstall-xformers after upgrading. NameError: name â€˜xformersâ€™ is not defined If you use Windows, this means your Python is too old. Use 3.10 If Linux, youâ€™ll have to build xformers yourself or just avoid using xformers."
  },"/sdwui-docs/ui-defaults/": {
    "title": "Changing UI defaults",
    "keywords": "Getting started",
    "url": "/sdwui-docs/ui-defaults/",
    "body": "The default values in the web UI can be changed by editing ui-config.json, which appears in the base directory containing webui.py after the first run. The changes are only applied after restarting. { \"txt2img/Sampling Steps/value\": 20, \"txt2img/Sampling Steps/minimum\": 1, \"txt2img/Sampling Steps/maximum\": 150, \"txt2img/Sampling Steps/step\": 1, \"txt2img/Batch count/value\": 1, \"txt2img/Batch count/minimum\": 1, \"txt2img/Batch count/maximum\": 32, \"txt2img/Batch count/step\": 1, \"txt2img/Batch size/value\": 1, \"txt2img/Batch size/minimum\": 1, # ... }"
  },"/sdwui-docs/pages/en/Xformers/": {
    "title": "Xformers",
    "keywords": "Guides",
    "url": "/sdwui-docs/pages/en/Xformers/",
    "body": "Xformers library is an optional way to speedup your image generation. There are no binaries for Windows except for one specific configuration, but you can build it yourself. A guide from an anonymous user, although I think it is for building on Linux: GUIDES ON HOW TO BUILD XFORMERS also includes how to uncuck yourself from sm86 restriction on voldyâ€™s new commit go to the webui directory source ./venv/bin/activate cd repositories git clone https://github.com/facebookresearch/xformers.git cd xformers git submodule update --init --recursive pip install -r requirements.txt pip install -e . Building xFormers on Windows by @duckness If you use a Pascal, Turing, Ampere, Lovelace or Hopper card with Python 3.10, you shouldnâ€™t need to build manually anymore. Uninstall your existing xformers and launch the repo with --xformers. A compatible wheel will be installed. Install VS Build Tools 2022, you only need Desktop development with C++ Install CUDA 11.3 (later versions are not tested), select custom, you only need the following (VS integration is probably unecessary): Clone the xFormers repo, create a venv and activate it git clone https://github.com/facebookresearch/xformers.git cd xformers git submodule update --init --recursive python -m venv venv ./venv/scripts/activate To avoid issues with getting the CPU version, install pyTorch seperately: pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113 Then install the rest of the dependencies: pip install -r requirements.txt pip install wheel As CUDA 11.3 is rather old, you need to force enable it to be built on MS Build Tools 2022. Do $env:NVCC_FLAGS = \"-allow-unsupported-compiler\" if on powershell, or set NVCC_FLAGS=-allow-unsupported-compiler if on cmd You can finally build xFormers, note that the build will take a long time (probably 10-20minutes), it may initially complain of some errors but it should still compile correctly. OPTIONAL tip: To further speed up on multi-core CPU Windows systems, install ninja https://github.com/ninja-build/ninja. Steps to install: download ninja-win.zip from https://github.com/ninja-build/ninja/releases and unzip place ninja.exe under C:\\Windows OR add the full path to the extracted ninja.exe into system PATH Run ninja -h in cmd and verify if you see a help message printed Run the follow commands to start building. It should automatically use Ninja, no extra config is needed. You should see significantly higher CPU usage (40%+). python setup.py build python setup.py bdist_wheel This has reduced build time on a windows PC with a AMD 5800X CPU from 1.5hr to 10min. Ninja is also supported on Linux and MacOS but I do not have these OS to test thus can not provide step-by-step tutorial. Run the following: python setup.py build python setup.py bdist_wheel In xformers directory, navigate to the dist folder and copy the .whl file to the base directory of stable-diffusion-webui In stable-diffusion-webui directory, install the .whl, change the name of the file in the command below if the name is different: ./venv/scripts/activate pip install xformers-0.0.14.dev0-cp310-cp310-win_amd64.whl Ensure that xformers is activated by launching stable-diffusion-webui with --force-enable-xformers Non-deterministic / unstable / inconsistent results: Known issue. See this list on the discussion page."
  },"/sdwui-docs/api/": {
    "title": "API guide",
    "keywords": "Guides",
    "url": "/sdwui-docs/api/",
    "body": "ENGLISH First, of course, is to run web ui with --api commandline argument example in your â€œwebui-user.batâ€: set COMMANDLINE_ARGS=--api This enables the api which can be reviewed at http://127.0.0.1:7860/docs (or whever the URL is + /docs) The basic ones Iâ€™m interested in are these two. Letâ€™s just focus only on ` /sdapi/v1/txt2img` When you expand that tab, it gives an example of a payload to send to the API. I used this often as reference. So thatâ€™s the backend. The API basically says whatâ€™s available, what itâ€™s asking for, and where to send it. Now moving onto the frontend, Iâ€™ll start with constructing a payload with the parameters I want. An example can be: payload = { \"prompt\": \"maltese puppy\", \"steps\": 5 } I can put in as few or as many parameters as I want in the payload. The API will use the defaults for anything I donâ€™t set. After that, I can send it to the API response = requests.post(url=f'http://127.0.0.1:7860/sdapi/v1/txt2img', json=payload) Again, this URL needs to match the web uiâ€™s URL. If we execute this code, the web ui will generate an image based on the payload. Thatâ€™s great, but then what? There is no image anywhereâ€¦ After the backend does its thing, the API sends the response back in a variable that was assigned above: response. The response contains three entries; â€œimagesâ€, â€œparametersâ€, and â€œinfoâ€, and I have to find some way to get the information from these entries. First, I put this line r = response.json() to make it easier to work with the response. â€œimagesâ€ is the generated image, which is what I want mostly. Thereâ€™s no link or anything; itâ€™s a giant string of random characters, apparently we have to decode it. This is how I do it: for i in r['images']: image = Image.open(io.BytesIO(base64.b64decode(i.split(\",\",1)[0]))) With that, we have an image in the image variable that we can work with, for example saving it with image.save('output.png'). â€œparametersâ€ shows what was sent to the API, which could be useful, but what I want in this case is â€œinfoâ€. I use it to insert metadata into the image, so I can drop it into web ui PNG Info. For that, I can access the /sdapi/v1/png-info API. Iâ€™ll need to feed the image I got above into it. png_payload = { \"image\": \"data:image/png;base64,\" + i } response2 = requests.post(url=f'http://127.0.0.1:7860/sdapi/v1/png-info', json=png_payload) After that, I can get the information with response2.json().get(\"info\") A sample code that should work can look like this: import json import requests import io import base64 from PIL import Image, PngImagePlugin url = \"http://127.0.0.1:7860\" payload = { \"prompt\": \"puppy dog\", \"steps\": 5 } response = requests.post(url=f'{url}/sdapi/v1/txt2img', json=payload) r = response.json() for i in r['images']: image = Image.open(io.BytesIO(base64.b64decode(i.split(\",\",1)[0]))) png_payload = { \"image\": \"data:image/png;base64,\" + i } response2 = requests.post(url=f'{url}/sdapi/v1/png-info', json=png_payload) pnginfo = PngImagePlugin.PngInfo() pnginfo.add_text(\"parameters\", response2.json().get(\"info\")) image.save('output.png', pnginfo=pnginfo) Import the things I need define the url and the payload to send send said payload to said url through the API in a loop grab â€œimagesâ€ and decode it for each image, send it to png info API and get that info back define a plugin to add png info, then add the png info I defined into it at the end here, save the image with the png info A note on \"override_settings\". The purpose of this endpoint is to override the web ui settings for a single request, such as the CLIP skip. The settings that can be passed into this parameter are visible here at the urlâ€™s /docs. You can expand the tab and the API will provide a list. There are a few ways you can add this value to your payload, but this is how I do it. Iâ€™ll demonstrate with â€œfilter_nsfwâ€, and â€œCLIP_stop_at_last_layersâ€. payload = { \"prompt\": \"cirno\", \"steps\": 20 } override_settings = {} override_settings[\"filter_nsfw\"] = true override_settings[\"CLIP_stop_at_last_layers\"] = 2 override_payload = { \"override_settings\": override_settings } payload.update(override_payload) Have the normal payload after that, initialize a dictionary (I call it â€œoverride_settingsâ€, but maybe not the best name) then I can add as many key:value pairs as I want to it make a new payload with just this parameter update the original payload to add this one to it So in this case, when I send the payload, I should get a â€œcirnoâ€ at 20 steps, with the CLIP skip at 2, as well as the NSFW filter on. For certain settings or situations, you may want your changes to stay. For that you can post to the /sdapi/v1/options API endpoint We can use what we learned so far and set up the code easily for this. Here is an example: url = \"http://127.0.0.1:7860\" option_payload = { \"sd_model_checkpoint\": \"Anything-V3.0-pruned.ckpt [2700c435]\", \"CLIP_stop_at_last_layers\": 2 } response = requests.post(url=f'{url}/sdapi/v1/options', json=option_payload) After sending this payload to the API, the model should swap to the one I set and set the CLIP skip to 2. Reiterating, this is different from â€œoverride_settingsâ€, because this change will persist, while â€œoverride_settingsâ€ is for a single request. Note that if youâ€™re changing the sd_model_checkpoint, the value should be the name of the checkpoint as it appears in the web ui. This can be referenced with this API endpoint (same way we reference â€œoptionsâ€ API) The â€œtitleâ€ (name and hash) is what you want to use. This is as of commit 47a44c7 For a more complete implementation of a frontend, my Discord bot is here if anyone wants to look at it as an example. Most of the action happens in stablecog.py. There are many comments explaining what each code does. This guide can be found in discussions page. Also, check out this python API client library for webui: https://github.com/mix1009/sdwebuiapi"
  },"/sdwui-docs/install/": {
    "title": "Install",
    "keywords": "Getting started",
    "url": "/sdwui-docs/install/",
    "body": "Make sure the required dependencies are met and follow the instructions available for both NVidia (recommended) and AMD GPUs. Alternatively, use online services (like Google Colab): Online Services Automatic Installation on Windows Install Python 3.10.6, checking â€œAdd Python to PATHâ€ Install git. Download the stable-diffusion-webui repository, for example by running git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git. Place model.ckpt in the models directory (see dependencies for where to get it). *(Optional)* Place GFPGANv1.4.pth in the base directory, alongside webui.py (see dependencies for where to get it). Run webui-user.bat from Windows Explorer as normal, non-administrator, user. Automatic Installation on Linux Install the dependencies: # Debian-based: sudo apt install wget git python3 python3-venv # Red Hat-based: sudo dnf install wget git python3 # Arch-based: sudo pacman -S wget git python3 To install in /home/$(whoami)/stable-diffusion-webui/, run: bash &lt;(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh) Installation on Apple Silicon Find the instructions here."
  },"/sdwui-docs/2022-12-27-my-new-post.html": {
    "title": "Release note example",
    "keywords": "",
    "url": "/sdwui-docs/2022-12-27-my-new-post.html",
    "body": "Test"
  }}
