{"/sdwui-docs/cli/": {
    "title": "命令行参数和设置",
    "keywords": "Guides",
    "url": "/sdwui-docs/cli/",
    "body": "webui-用户 自定义程序运行方式的推荐方法是编辑 webui-user.bat (Windows) 和 webui-user.sh (Linux)： set PYTHON 允许设置自定义 Python 路径 示例：set PYTHON=b:/soft/Python310/Python.exe set VENV_DIR 允许您选择虚拟环境的目录。默认为“venv”。特殊值 - 运行脚本而不创建虚拟环境。 示例：set VENV_DIR=C:\\run\\var\\run 将在 C:\\run\\var\\run 目录中创建 venv。 示例：set VENV_DIR=- 使用系统的 python 运行程序 set COMMANDLINE_ARGS 设置命令行参数 webui.py 运行 示例：set COMMANDLINE_ARGS=--ckpt a.ckpt 使用模型 a.ckpt 而不是 model.ckpt 命令行参数 在线运行 使用 --share 选项在线运行。你会得到一个 xxx.app.gradio 链接。这是在协作中使用该程序的预期方式。您可以使用标志 --gradio-auth username:password 为所述 gradio 共享实例设置身份验证，可选择提供多组用户名和密码，以逗号分隔。 使用 --listen 让服务器监听网络连接。这将允许本地网络上的计算机访问 UI，如果您配置端口转发，互联网上的计算机也可以访问。 使用 --port xxxx 让服务器侦听特定端口，xxxx 是需要的端口。请记住，所有低于 1024 的端口都需要 root/admin 权限，因此建议使用高于 1024 的端口。如果可用，默认为端口 7860。 所有命令行参数 |参数命令 |价值 |默认 |说明 | | —————- | —– | ——- | ———– | 配置       -h, –帮助 无 错误 显示此帮助消息并退出 –配置 配置 配置/稳定扩散/v1-inference.yaml 构建模型的配置路径 –ckpt CKPT 模型.ckpt 稳定扩散模型检查点的路径；如果指定，此检查点将添加到检查点列表并加载 –ckpt目录 CKPT_DIR 无 具有稳定扩散检查点的目录路径 –gfpgan-目录 GFPGAN_DIR GFPGAN / GFPAN 目录 –gfpgan模型 GFPGAN_模型 GFPAN 模型文件名   –codeformer-模型路径 CODEFORMER_MODELS_PATH 模型/Codeformer/ 带有 codeformer 模型文件的目录路径。 –gfpgan-模型路径 GFPGAN_MODELS_PATH 模型/GFPGAN 包含 GFPGAN 模型文件的目录路径。 –esrgan-模型路径 ESRGAN_MODELS_PATH 型号/ESRGAN ESRGAN 模型文件的目录路径。 –bsrgan-模型路径 BSRGAN_MODELS_PATH 模型/BSRGAN 包含 BSRGAN 模型文件的目录路径。 –realesrgan-models-path REALESRGAN_MODELS_PATH 模型/RealESRGAN 包含 RealESRGAN 模型文件的目录路径。 –scunet 模型路径 SCUNET_MODELS_PATH 模型/ScuNET ScuNET 模型文件的目录路径。 –swinir 模型路径 SWINIR_MODELS_PATH 模型/SwinIR 包含 SwinIR 和 SwinIR v2 模型文件的目录路径。 –ldsr-模型路径 LDSR_MODELS_PATH 模型/LDSR 包含 LDSR 模型文件的目录路径。 –剪辑模型路径 剪辑模型路径 无 CLIP 模型文件的目录路径。 –vae 路径 VAE_路径 无 变分自动编码器模型的路径 –嵌入目录 嵌入目录 嵌入/ 用于文本反转的嵌入目录（默认值：嵌入） –超网络目录 超网络目录 模型/超网络/ 超网络目录 –本地化目录 本地化目录 本地化/ 本地化目录 –样式文件 样式文件 样式.csv 用于样式的文件名 –ui 配置文件 用户界面配置文件 用户界面配置.json 用于 ui 配置的文件名 –无进度条隐藏 无 错误 不要在 gradio UI 中隐藏进度条（我们隐藏它是因为如果你在浏览器中有硬件加速，它会减慢 ML） –最大批次计数 MAX_BATCH_COUNT 16 UI 的最大批计数值 –ui-设置文件 用户界面设置文件 配置.json 用于 ui 设置的文件名 –允许代码 无 错误 允许从 webui 执行自定义脚本 –分享 无 错误 对 gradio 使用 share=True 并使 UI 可以通过他们的网站访问（对我不起作用，但你可能会更幸运） –听 无 错误 以 0.0.0.0 作为服务器名称启动 gradio，允许响应网络请求 –端口 港口 7860 使用给定的服务器端口启动 gradio，您需要端口 &lt; 1024 的根/管理员权限，如果可用，默认为 7860 –hide-ui-dir-config 无 错误 从 webui 隐藏目录配置 –冻结设置 无 错误 禁用编辑设置 –enable-insecure-extension-access 无 错误 无论其他选项如何，都启用扩展选项卡 –gradio调试 无 错误 使用 –debug 选项启动 gradio –gradio-auth GRADIO_AUTH 无 将 gradio 身份验证设置为“用户名：密码”；或逗号分隔多个，如 “u1:p1,u2:p2,u3:p3” –gradio-img2img-工具 {color-sketch,editor} 编辑 gradio image uploader tool：可以是ctopping的编辑器，也可以是绘图的color-sketch –disable-console-progressbars 无 错误 不要向控制台输出进度条 –启用控制台提示 无 错误 使用 txt2img 和 img2img 生成时打印提示到控制台 –API 无 错误 使用 API 启动 webui –nowebui 无 错误 只启动 API，不启动 UI –ui-调试模式 无 错误 不加载模型以快速启动 UI –设备ID 设备 ID 无 选择要使用的默认 CUDA 设备（之前可能需要导出 CUDA_VISIBLE_DEVICES=0,1 等） –管理员 无 错误 管理员权限 性能       –变形金刚 无 错误 为交叉注意力层启用 xformers –reinstall-xformers 无 错误 强制重新安装 xformers。对升级很有用 - 但在升级后将其删除，否则您将永久重新安装 xformers。 –force-enable-xformers 无 错误 为交叉注意力层启用 xformers，无论检查代码是否认为您可以运行它； 如果这不起作用，请不要提交错误报告 –opt-split-attention 无 错误 强制启用 Doggettx 的交叉注意力层优化。默认情况下，它在支持 cuda 的系统上打开。 –opt-split-attention-invokeai 无 错误 强制启用 InvokeAI 的交叉注意层优化。默认情况下，它在 cuda 不可用时打开。 –opt-split-attention-v1 无 错误 启用旧版本的分离注意力优化，它不会消耗它能找到的所有 VRAM –opt-channelslast 无 错误 最后更改记忆类型以稳定扩散到通道 –disable-opt-split-attention 无 错误 强制禁用交叉注意层优化 –使用CPU {all, sd, interrogate, gfpgan, bsrgan, esrgan, scunet, codeformer} 无 指定模块使用CPU作为手电筒设备 –没有一半 无 错误 不要将模型切换为 16 位浮点数 –精度 {full,autocast} 自动铸造 以此精度进行评估 –no-half-vae 无 错误 不要将 VAE 模型切换为 16 位浮点数 –medvram 无 错误 启用稳定的扩散模型优化，为低 VRM 使用牺牲一点速度 –lowvram 无 错误 启用稳定的扩散模型优化以牺牲大量速度以获得非常低的 VRM 使用率 –洛拉姆 无 错误 将稳定的扩散检查点权重加载到 VRAM 而不是 RAM –always-batch-cond-uncond 无 错误 禁用使用 –medvram 或 –lowvram 节省内存的 cond/uncond 批处理 特点       –自动启动 无 错误 启动时在系统默认浏览器中打开 webui URL –主题 无 未设置 打开具有指定主题（“light”或“dark”）的 webui。如果未指定，则使用默认浏览器主题 –使用文本框种子 无 错误 在 UI 中使用文本框作为种子（不能向上/向下，但可以输入长种子） –disable-safe-unpickle 无 错误 禁用检查 pytorch 模型中的恶意代码 –ngrok 恩格罗克 未设置 ngrok authtoken，gradio –share 的替代品 –ngrok 区域 NGROK_区域 未设置 ngrok 应该启动的区域。 失效选项       –show-negative-prompt 无 错误 什么都不做 –deepdanbooru 无 错误 什么都不做 –卸载-gfpgan 无 错误 什么都不做。"
  },"/sdwui-docs/contributing/": {
    "title": "贡献",
    "keywords": "development",
    "url": "/sdwui-docs/contributing/",
    "body": "特征 拉取请求 要做出贡献，请克隆存储库、进行更改、提交并推送到您的克隆，然后提交拉取请求。 通过运行 tests 确保您的更改不会破坏任何内容。 如果您要添加大量代码，请考虑将您的贡献作为 extension，并且只需要在主代码中进行 PR 小的更改以使扩展成为可能。 如果您要对使用的库或安装脚本进行更改，则必须验证它们是否可以从头开始在默认 Windows 安装上运行。如果您无法测试它是否有效（由于您的操作系统或其他原因），请不要进行这些更改（可能的例外情况是明确禁止在 Windows 上通过“if”或其他方式执行的更改）。 代码风格 我主要遵循 PyCharm 建议的代码风格，但禁用的行长度限制除外。请不要提交 PR，您只是采用现有行并重新格式化它们而不改变它们的作用。 建成 Gradio 在某些时候想添加这个部分，以便在贡献部分中展示他们的项目，我当时没有，所以现在就在这里。 对于 Gradio，请查看 docs 以做出贡献： 对 Gradio 有问题或功能请求？在 github 上打开问题/功能请求以获得支持：https://github.com/gradio-app/gradio/issues"
  },"/sdwui-docs/custom-filenames/": {
    "title": "自定义图像文件名和子目录",
    "keywords": "Guides",
    "url": "/sdwui-docs/custom-filenames/",
    "body": "以下信息是关于图像文件名和子目录名，不是Paths for saving \\ Output directorys 默认情况下，Wub UI 将图像保存在输出目录中，文件名结构为 number-seed-[prompt_spaces] 01234-987654321-((masterpiece)), ((best quality)), ((illustration)), extremely detailed,style girl.png 如果用户愿意，可以使用不同的图像文件名和可选的子目录。 图片文件名模式可以配置在. 设置选项卡 &gt; 保存图像/网格 &gt; 图像文件名模式 子目录可以在设置下配置。 设置选项卡 &gt; 保存到目录 &gt; 目录名称模式 彭定康 Web-Ui 提供了几种模式，可用作将信息插入文件名或子目录的占位符， 用户可以将这些模式链接在一起，形成适合其用例的文件名。 |图案 |说明 |示例 | |——————————–|——————————————————|———————————————| [种子] 种子 1234567890 [步骤] 步骤 20 [cfg] CFG规模 7 [采样器] 取样方法 欧拉 [模型哈希] 模型的哈希 7460a6fa [宽度] 图片宽度 512 [身高] 图片高度 512 [样式] 所选样式的名称 我的风格名称 [日期] ISO 格式的计算机日期 2022-10-24 [日期时间] “%Y%m%d%H%M%S”中的日期时间 20221025013106 [日期时间&lt;格式&gt;] 指定&lt;格式&gt; 中的日期时间 [日期时间&lt;%Y%m%d_%H%M%S_%f&gt;]20221025_014350_733877 [日期时间&lt;格式&gt;&lt;时区&gt;] 指定&lt;格式&gt; 中特定&lt;Time Zone&gt; 的日期时间 [日期时间&lt;%Y%m%d_%H%M%S_%f&gt;&lt;亚洲/东京&gt;]`20221025_014350_733877 [prompt_no_styles] 无样式提示 1gir，空白，（（非常重要）），[不重要]，（一些值_1.5），（随便），结束 [提示空格] 提示样式 1gir, 空白, ((非常重要)), [不重要], (some value_1.5), (whatever), 结束, (((crystals texture Hair)))，((( [提示] 提示样式，空格键替换为 _ 1gir,___white_space,_((very_important)),_[不_important],_(some_value_1.5),_(whatever),_the_end, _(((crystals_texture_Hair)))，((( [提示词] 删除了样式、括号和逗号的提示 1gir white space very important not important some value 1 5 whatever the end crystals texture Hair, extremely detailed 日期时间格式详细信息 有关 格式代码 的更多详细信息，请参考 python 文档 日期时间时区详细信息 参考 List of Time Zones 以获取有效时区列表 如果 &lt;Format&gt; 为空或无效，它将使用默认时间格式“%Y%m%d%H%M%S” 提示：您可以在 &lt;Format&gt; 中使用额外的字符作为标点符号，例如 _ - 如果 &lt;TimeZone&gt; 为空或无效，它将使用默认的系统时区 用于上述“[prompt]”示例的提示和样式 迅速的： 1gir, white space, ((very important)), [not important], (some value:1.5), (whatever), the end 精选款式： (((crystals texture Hair)))，(((((extremely detailed CG))))),((8k_wallpaper)) 注意：上面提到的Styles是指generate按钮下方的两个下拉菜单 ###如果提示太长，它会很短 这是由于计算机有最大文件长度 保存时在文件名中添加/删除数字 您可以删除前缀号码 通过取消选中下面的复选框 Setting &gt; Saving images/grids &gt; Add number to filename when saving 带前缀号码 00123-`987654321-((masterpiece)).png 没有前缀号码 987654321-((masterpiece)).png 注意 前缀号的作用是保证保存的图片文件名是唯一的。 如果您决定不使用前缀数字，请确保您的模式将生成一个唯一的文件名， 否则文件可能会被覆盖。 一般datetime精确到秒应该可以保证文件名是唯一的。 [datetime&lt;%Y%m%d_%H%M%S&gt;]-[seed] 20221025_014350-281391998.png 但是对于一些自定义脚本可能会在单批中使用相同的种子生成多个图像， 在这种情况下，更安全的做法是将“微秒”也使用“%f”作为十进制数，用零填充到 6 位数字。 [datetime&lt;%Y%m%d_%H%M%S_%f&gt;]-[seed] 20221025_014350_733877-281391998.png 文件名模式示例 如果您在多台机器上运行 Web-Ui，例如在 Google Colab 和您自己的计算机上，您可能希望使用带有时间前缀的文件名。 这样当您下载犯规时，您可以将它们放在同一个文件夹中。 此外，由于您不知道 Google Colab 使用的时区，因此您需要指定时区。 [datetime&lt;%Y%m%d_%H%M%S_%f&gt;&lt;Asia/Tokyo&gt;]-[seed]-[prompt_words] 20221025_032649_058536-3822510847-1girl.png 设置子目录的日期可能也很有用，这样一个文件夹就不会有太多图像 [datetime&lt;%Y-%m-%d&gt;&lt;Asia/Tokyo&gt;] 2022-10-25"
  },"/sdwui-docs/custom-scripts/": {
    "title": "自定义脚本",
    "keywords": "Guides",
    "url": "/sdwui-docs/custom-scripts/",
    "body": "安装和使用自定义脚本 要安装自定义脚本，请将它们放入 scripts 目录，然后单击设置选项卡底部的 Reload custom script 按钮。安装后，自定义脚本将出现在 txt2img 和 img2img 选项卡左下角的下拉菜单中。以下是 Web UI 用户创建的一些著名的自定义脚本： 来自用户的自定义脚本 改进的提示矩阵 https://github.com/ArrowM/auto1111-improved-prompt-matrix 此脚本是 advanced-prompt-matrix 修改后支持“批次计数”。不创建网格。 用法： 使用 &lt; &gt; 创建一组替代文本。使用“ ”分隔文本选项。可以使用多个组和多个选项。例如： a &lt;corgi|cat&gt; wearing &lt;goggles|a hat&gt; 的输入 会输出4个提示：a corgi wearing goggles, a corgi wearing a hat, a cat wearing goggles, a cat wearing a hat 当使用 batch count &gt; 1 时，将为每个种子生成每个提示变体。 batch size 被忽略。 txt2img2img https://github.com/ThereforeGames/txt2img2img 极大地提高任何角色/主题的可编辑性，同时保留他们的相似性。该脚本的主要动机是提高通过 Textual Inversion 创建的嵌入的可编辑性。 （小心克隆，因为它签入了一些 venv） txt2掩码 https://github.com/ThereforeGames/txt2mask 允许您指定带有文本的修复蒙版，而不是画笔。 蒙版绘图界面 https://github.com/dfaker/stable-diffusion-webui-cv2-external-masking-script 提供由 CV2 提供支持的本地弹出窗口，允许在处理之前添加掩码。 Img2img 视频 https://github.com/memes-forever/Stable-diffusion-webui-video 使用img2img，生成一张张图片。 &lt;/视频&gt; ## 种子旅行 &lt;https://github.com/yownas/seed_travel&gt; 选择两个（或更多）种子并生成一系列在它们之间插值的图像。或者，让它创建结果的视频。 你可以用它做什么的例子： https://www.youtube.com/watch?v=4c71iUclY4U &lt;/视频&gt; ## 高级种子混合 &lt;https://github.com/amotile/stable-diffusion-backend/tree/master/src/process/implementations/automatic1111_scripts&gt; 此脚本允许您将初始噪声基于多个加权种子。 前任。 `seed1:2, seed2:1, seed3:1` 权重被归一化，所以你可以像上面那样使用更大的一次，或者你可以做浮点数： 前任。 `seed1:0.5, seed2:0.25, seed3:0.25` ## 快速混合 &lt;https://github.com/amotile/stable-diffusion-backend/tree/master/src/process/implementations/automatic1111_scripts&gt; 该脚本允许您在生成图像之前通过数学组合文本嵌入来将多个加权提示组合在一起。 前任。 `含有元素{火|冰}的水晶` 它支持嵌套定义，因此您也可以这样做： `含有元素fire:5|ice|earth的水晶` ## 动画师 &lt;https://github.com/Animator-Anon/Animator&gt; 一个基本的 img2img 脚本，它将转储帧并构建视频文件。适合在扭曲电影中创建有趣的缩放，但此时其他不多。 ## 参数序列器 &lt;https://github.com/rewbs/sd-parseq&gt; 通过对许多 Stable Diffusion 参数（例如种子、比例、提示权重、去噪强度...）以及输入处理参数（例如缩放、平移、3D 旋转...）进行严格控制和灵活插值来生成视频 ## 备用噪声时间表 &lt;https://gist.github.com/dfaker/f88aa62e3a14b559fe4e5f6b345db664&gt; 为采样器的西格玛计划使用备用生成器。 允许从 crowsonkb/k-diffusion 访问 Karras、指数和方差保持计划及其参数。 ## 视频对视频 &lt;https://github.com/Filarius/stable-diffusion-webui/blob/master/scripts/vid2vid.py&gt; 从真实视频中，img2img 帧并将它们拼接在一起。不将帧解压缩到硬盘驱动器。 ## Txt2VectorGraphics &lt;https://github.com/GeorgLegato/Txt2Vectorgraphics&gt; 根据您的提示创建自定义、可缩放的图标，如 SVG 或 PDF。 |提示 |PNG |SVG | | :-------- | :-----------------: | :---------------------: | | Happy Einstein | | | | Mountainbike Downhill | | | coffe mug in shape of a heart | | | | Headphones | | | ## 转移注意力 &lt;https://github.com/yownas/shift-attention&gt; 在提示中生成一系列图像来转移注意力。 此脚本使您能够在提示中为令牌的权重指定一个范围，然后生成从第一个到第二个的一系列图像。 ## 环回和叠加 &lt;https://github.com/DiceOwl/StableDiffusionStuff&gt; &lt;https://github.com/DiceOwl/StableDiffusionStuff/blob/main/loopback_superimpose.py&gt; 将 img2img 的输出与强度 alpha 的原始输入图像混合。结果再次输入 img2img（在 loop&gt;=2），并重复此过程。倾向于锐化图像，提高一致性，减少创造力并减少精细细节。 ## 插值 &lt;https://github.com/DiceOwl/StableDiffusionStuff&gt; &lt;https://github.com/DiceOwl/StableDiffusionStuff/blob/main/interpolate.py&gt; 用于生成中间图像的 img2img 脚本。允许两个输入图像进行插值。 [自述文件](https://github.com/DiceOwl/StableDiffusionStuff) 中显示了更多功能。 ## 运行n次 &lt;https://gist.github.com/camenduru/9ec5f8141db9902e375967e93250860f&gt; 使用随机种子运行 n 次。 ## 高级环回 &lt;https://github.com/Extraltodeus/advanced-loopback-for-sd-webui&gt; 具有参数变化和快速切换等功能的动态缩放环回！ ## 提示变形 &lt;https://github.com/feffy380/prompt-morph&gt; 使用 Stable Diffusion 生成变形序列。在两个或多个提示之间进行插值，并在每个步骤中创建一个图像。 使用新的 AND 关键字，并且可以选择将序列导出为视频。 ##提示插值 &lt;https://github.com/EugeoSynthesisThirtyTwo/prompt-interpolation-script-for-sd-webui&gt; 使用此脚本，您可以在两个提示之间插入（使用“AND”关键字），生成任意数量的图像。 您还可以生成带有结果的 gif。适用于 txt2img 和 img2img。 ![gif](https://user-images.githubusercontent.com/24735555/195470874-afc3dfdc-7b35-4b23-9c34-5888a4100ac1.gif) ## 不对称平铺 &lt;https://github.com/tjm35/asymmetric-tiling-sd-webui/&gt; 相互独立地控制水平/垂直无缝平铺。 ## 力对称 https://gist.github.com/1ort/2fe6214cf1abe4c07087aac8d91d0d8a 请参阅 https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/2441 每 n 步对图像应用对称并将结果进一步发送到 img2img。 ## SD 潜在镜像 &lt;https://github.com/dfaker/SD-latent-mirroring&gt; 对潜像应用镜像和翻转以产生从微妙的平衡构图到完美反射的任何东西 ## txt2调色板 &lt;https://github.com/1ort/txt2palette&gt; 通过文本描述生成调色板。此脚本获取生成的图像并将它们转换为调色板。 ## 样式箭头 &lt;https://github.com/some9000/StylePile&gt; 一种将元素混合和匹配到影响结果样式的提示的简单方法。 ## XYZ 绘图脚本 &lt;https://github.com/xrpgame/xyz_plot_script&gt; 生成一个 .html 文件以交互方式浏览图像集。使用滚轮或箭头键在 Z 维度中移动。 ## xyz-plot-grid &lt;https://github.com/Gerschel/xyz-plot-grid&gt; 将 xyz_grid.py 与其他脚本一起放在脚本文件夹中。 像 x/y 图一样工作，就像您期望的那样，但现在有一个 z。就像您期望的那样工作，还有网格图例。 ## 扩展 XY 网格 &lt;https://github.com/0xALIVEBEEF/Expanded-XY-grid&gt; AUTOMATIC1111 的 stable-diffusion-webui 的自定义脚本，它向标准 xy 网格添加了更多功能： - 多工具：允许在一个轴上使用多个参数，理论上允许在一个 xy 网格中调整无限个参数 - 可定制的提示矩阵 - 目录中的组文件 - S/R 占位符 - 用所需值替换占位符值（参数列表中的第一个值）。 - 添加 PNGinfo 到网格图像 示例图像：提示：“达斯·维达骑自行车，修改器”； X：Multitool：“提示 S/R：自行车、摩托车 | CFG 比例：7.5、10 | 提示 S/R 占位符：修饰符、4k、artstation”； Y: Multitool: \"Sampler: Euler, Euler a | Steps: 20, 50\" ## Booru 标签自动补全 &lt;https://github.com/DominikDoom/a1111-sd-webui-tagcomplete&gt; 显示来自“image booru”板（例如 Danbooru）的标签的自动完成提示。使用本地标记 CSV 文件并包含用于自定义的配置。 还支持完成 [wildcards](https://github.com/adieyal/sd-dynamic-prompts#wildcard-files) ## 嵌入到 PNG &lt;https://github.com/dfaker/embedding-to-png-script&gt; 将现有嵌入转换为可共享的图像版本。 ## 阿尔法画布 &lt;https://github.com/TKoestlerx/sdexperiments&gt; 涂刷一个区域。 Infinite outpainting 概念，使用 AUTOMATIC1111 repo 中的两个现有 outpainting 脚本作为基础。 ## 随机网格 &lt;https://github.com/lilly1987/AI-WEBUI-scripts-Random&gt; 随机输入 xy 网格值。 基本逻辑与x/y plot相同，只是在内部，x type固定为step，type y固定为cfg。 在 step1|2 值 (10-30) 范围内生成与步数 (10) 一样多的 x 值 在 cfg1|2 值 (6-15) 范围内生成与 cfg 计数 (10) 一样多的 x 值 即使您将 1|2 范围上限倒置，它也会自动更改。 在cfg值的情况下，将其视为int类型，不读取十进制值。 ＃＃ 随机的 &lt;https://github.com/lilly1987/AI-WEBUI-scripts-Random&gt; 在没有网格的情况下重复简单的次数。 ## 稳定扩散美学评分器 &lt;https://github.com/grexzen/SD-Chad&gt; 评价你的图像。 ## img2tiles &lt;https://github.com/arcanite24/img2tiles&gt; 从基本图像生成图块。基于 SD 高档脚本。 ## img2mosiac &lt;https://github.com/1ort/img2mosaic&gt; 从图像生成马赛克。该脚本将图像切割成图块并分别处理每个图块。每个图块的大小是随机选择的。 ## 深度图 &lt;https://github.com/thygate/stable-diffusion-webui-depthmap-script&gt; 此脚本是 [AUTOMATIC1111 的稳定扩散 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) 的插件，可根据生成的图像创建“深度图”。结果可以在 3D 或全息设备（如 VR 耳机或 [lookingglass](https://lookingglassfactory.com/) 显示器）上查看，在带有位移修改器的平面上用于渲染或游戏引擎，甚至可以 3D 打印. ## 测试我的提示 &lt;https://github.com/Extraltodeus/test_my_prompt&gt; 您是否曾经使用过很长的提示词，但您不确定是否会对您的形象产生任何实际影响？您是否失去了尝试将它们一一移除以测试它们的效果是否值得您强大的 GPU 的勇气？ 好吧，现在您不需要任何勇气，因为这个脚本已经为您量身定做！ 它会生成与提示中的单词一样多的图像（当然，您可以选择分隔符）。 这里的提示很简单：“**banana, on fire, snow**”，正如您所看到的，它生成了每张图像，但其中没有任何描述。 您还可以测试您的否定提示。 ## 像素艺术 &lt;https://github.com/C10udburst/stable-diffusion-webui-scripts&gt; 通过可变数量调整图像大小的简单脚本，还将图像转换为使用给定大小的调色板。 |已禁用 |已启用 x8，没有调整大小，没有调色板 |启用 x8，无调色板 |已启用 x8、16 调色板 | | :---: | :---: | :---: | :---: | |![预览](https://user-images.githubusercontent.com/18114966/201491785-e30cfa9d-c850-4853-98b8-11db8de78c8d.png) | ![预览](https://user-images.githubusercontent.com/18114966/201492204-f4303694-e98d-4ea3-8256-538a88ea26b6.png) | ![预览](https://user-images.githubusercontent.com/18114966/201491864-d0c0c9f1-e34f-4cb6-a68e-7043ec5ce74e.png) | ![预览](https://user-images.githubusercontent.com/18114966/201492175-c55fa260-a17d-47c9-a919-9116e1caa8fe.png) | [使用的模型](https://publicprompts.art/all-in-one-pixel-art-dreambooth-model/) ```text japanese pagoda with blossoming cherry trees, full body game asset, in pixelsprite style Steps: 20, Sampler: DDIM, CFG scale: 7, Seed: 4288895889, Size: 512x512, Model hash: 916ea38c, Batch size: 4 ``` ## 多个超网络 &lt;https://github.com/antis0007/sd-webui-multiple-hypernetworks&gt; 添加一次应用多个超网络的能力。覆盖劫持、优化和 CrossAttention 前向函数，以便顺序应用具有不同权重的多个超网络。 ## 超网络结构（.hns）/变量丢失/猴子补丁 &lt;https://github.com/aria1th/Hypernetwork-MonkeyPatch-Extension&gt; 添加应用超网络结构的能力，如在 .hns 文件中定义的那样。有关详细信息，请参阅[此处](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/4334)。 添加使用适当的可变丢失率的能力，例如 0.05。还修复了训练后立即使用超网络的问题。 添加创建 beta-hypernetwork（dropout）和 beta-training，它允许自动余弦退火和原始图像的无裁剪使用。 ## 配置预设 &lt;https://github.com/Zyin055/Config-Presets-Script-OLD&gt; 使用可配置的预设值下拉列表快速更改 txt2img 和 img2img 选项卡中的设置。 ## 采样过程的保存步骤 该脚本会将采样过程的步骤保存到一个目录中。 ```python import os.path import modules.scripts as scripts import gradio as gr from modules import sd_samplers, shared from modules.processing import Processed, process_images class Script(scripts.Script): def title(self): return \"Save steps of the sampling process to files\" def ui(self, is_img2img): path = gr.Textbox(label=\"Save images to path\") return [path] def run(self, p, path): index = [0] def store_latent(x): image = shared.state.current_image = sd_samplers.sample_to_image(x) image.save(os.path.join(path, f\"sample-{index[0]:05}.png\")) index[0] += 1 fun(x) fun = sd_samplers.store_latent sd_samplers.store_latent = store_latent try: proc = process_images(p) finally: sd_samplers.store_latent = fun return Processed(p, proc.images, p.seed, \"\") ```"
  },"/sdwui-docs/dependencies/": {
    "title": "依赖关系",
    "keywords": "Getting started",
    "url": "/sdwui-docs/dependencies/",
    "body": "Python 3.10.6 和 Git： Windows：下载并运行 Python 3.10.6 的安装程序（网页、exe，或win7版本) 和 git (网页) Linux（基于 Debian）：sudo apt install wget git python3 python3-venv Linux（基于 Red Hat）：sudo dnf install wget git python3 Linux（基于 Arch）：sudo pacman -S wget git python3 来自这个存储库的代码： 首选方式：使用 git：git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git。 之所以提到这种方式，是因为它让您只需运行 git pull 即可更新。 这些命令可以在您右键单击资源管理器并选择“Git Bash here”后打开的命令行窗口中使用。 替代方法：使用回购主页上的“代码”（绿色按钮）-&gt;“下载 ZIP”选项。 即使你选择了这个，你仍然需要安装 git。 要更新，您必须再次下载 zip 并替换文件。 Stable Diffusion模型检查点，一个扩展名为.ckpt的文件，需要下载并放在models/Stable-diffusion目录下。 官方下载 文件存储 洪流 (磁铁:?xt=urn:btih:3a4a612d75ed088ea542acac52f9f45987488d1c&amp;dn=sd-v1-4.ckpt&amp;tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&amp;tr=udp%3a%2f%2ftracker.opentrackr.org% 3a1337) 可选依赖 ESRGAN（升级） ESRGAN 模型，例如来自 模型数据库 的模型，可以放置在 ESRGAN 目录中。 如果文件具有 .pth 扩展名，它将作为模型加载，并且会在 UI 中显示其名称。 注意：RealESRGAN 模型不是 ESRGAN 模型，它们不兼容。不要下载 RealESRGAN 模型。不要将 RealESRGAN 放入包含 ESRGAN 模型的目录中。 sd 2.x 模型的 .yaml 文件 768-v-ema.ckpt 配置) 512-base-ema.ckpt 配置 512-depth-ema.ckpt 配置 下载配置 .yaml 文件并将其存储在与 .ckpt 同名的同一文件夹中。"
  },"/sdwui-docs/developing-custom-scripts/": {
    "title": "开发自定义脚本",
    "keywords": "development",
    "url": "/sdwui-docs/developing-custom-scripts/",
    "body": "安装和使用自定义脚本 脚本类定义可以在 modules/scripts.py 中找到。要创建您自己的自定义脚本，请创建一个实现该类的 python 脚本并将其放入 scripts 文件夹，使用以下示例或文件夹中已有的其他脚本作为指南。 Script 类有四种主要方法，下面将通过一个旋转和/或翻转生成的图像的简单示例脚本进一步详细描述。 import modules.scripts as scripts import gradio as gr import os from modules import images from modules.processing import process_images, Processed from modules.processing import Processed from modules.shared import opts, cmd_opts, state class Script(scripts.Script): # The title of the script. This is what will be displayed in the dropdown menu.     def title(self):         return \"Flip/Rotate Output\" # Determines when the script should be shown in the dropdown menu via the # returned value. As an example: # is_img2img is True if the current tab is img2img, and False if it is txt2img. # Thus, return is_img2img to only show the script on the img2img tab.     def show(self, is_img2img):         return is_img2img # How the script's is displayed in the UI. See https://gradio.app/docs/#components # for the different UI components you can use and how to create them. # Most UI components can return a value, such as a boolean for a checkbox. # The returned values are passed to the run method as parameters.     def ui(self, is_img2img):         angle = gr.Slider(minimum=0.0, maximum=360.0, step=1, value=0,         label=\"Angle\")         hflip = gr.Checkbox(False, label=\"Horizontal flip\")         vflip = gr.Checkbox(False, label=\"Vertical flip\")         overwrite = gr.Checkbox(False, label=\"Overwrite existing files\")         return [angle, hflip, vflip, overwrite] # This is where the additional processing is implemented. The parameters include # self, the model object \"p\" (a StableDiffusionProcessing class, see # processing.py), and the parameters returned by the ui method. # Custom functions can be defined here, and additional libraries can be imported # to be used in processing. The return value should be a Processed object, which is # what is returned by the process_images method.     def run(self, p, angle, hflip, vflip, overwrite):         # function which takes an image from the Processed object, # and the angle and two booleans indicating horizontal and         # vertical flips from the UI, then returns the         # image rotated and flipped accordingly         def rotate_and_flip(im, angle, hflip, vflip):             from PIL import Image                         raf = im                         if angle != 0:                 raf = raf.rotate(angle, expand=True)             if hflip:                 raf = raf.transpose(Image.FLIP_LEFT_RIGHT)             if vflip:                 raf = raf.transpose(Image.FLIP_TOP_BOTTOM)             return raf         # If overwrite is false, append the rotation information to the filename         # using the \"basename\" parameter and save it in the same directory.         # If overwrite is true, stop the model from saving its outputs and         # save the rotated and flipped images instead.         basename = \"\"         if(not overwrite):             if angle != 0:                 basename += \"rotated_\" + str(angle)             if hflip:                 basename += \"_hflip\"             if vflip:                 basename += \"_vflip\"         else:             p.do_not_save_samples = True         proc = process_images(p)         # rotate and flip each image in the processed images # use the save_images method from images.py to save # them.         for i in range(len(proc.images)):             proc.images[i] = rotate_and_flip(proc.images[i], angle, hflip, vflip)             images.save_image(proc.images[i], p.outpath_samples, basename,             proc.seed + i, proc.prompt, opts.samples_format, info= proc.info, p=p)         return proc"
  },"/sdwui-docs/developing-extensions/": {
    "title": "开发扩展",
    "keywords": "development",
    "url": "/sdwui-docs/developing-extensions/",
    "body": "扩展只是 extensions 目录中的一个子目录。 Web ui 通过以下方式与已安装的扩展进行交互： 执行扩展的“install.py”脚本（如果存在）。 scripts 目录中的扩展脚本就像普通用户脚本一样执行，除了： sys.path 被扩展为包含扩展目录，因此您可以放心地导入其中的任何内容 你可以使用 scripts.basedir() 来获取当前扩展的目录（因为用户可以随意命名） 将 javascript 目录中的扩展的 javascript 文件添加到页面 localizations 目录中扩展的本地化文件被添加到设置中；如果有两个具有相同名称的本地化，则不会合并它们，一个替换另一个。 扩展的 style.css 文件被添加到页面 如果扩展在其根目录中有 preload.py 文件，则在解析命令行参数之前加载它 如果扩展的 preload.py 有一个 preload 函数，它被调用，命令行参数解析器作为参数传递给它。以下是如何使用它添加命令行参数的示例： def preload(parser): parser.add_argument(\"--wildcards-dir\", type=str, help=\"directory with wildcards\", default=None) 有关如何开发自定义脚本（通常会完成扩展的大部分工作）的信息，请参阅开发自定义脚本。 本地化扩展 为项目进行本地化的首选方法是通过扩展。扩展的基本文件结构应该是： 📁 webui root directory ┗━━ 📁 extensions ┗━━ 📁 webui-localization-la_LA &lt;----- name of extension ┗━━ 📁 localizations &lt;----- the single directory inside the extension ┗━━ 📄 la_LA.json &lt;----- actual file with translations 使用此文件结构创建一个 github 存储库，并要求合作者部分中列出的任何人将您的扩展添加到 wiki。 如果您的语言需要 javascript/css 甚至 python 支持，您也可以将其添加到扩展中。 安装.py install.py 是启动器 launch.py​​ 在 webui 启动之前在一个单独的进程中启动的脚本，它的目的是安装扩展的依赖项。它必须位于扩展的根目录中，而不是脚本目录中。该脚本在 PYTHONPATH 环境变量设置为 webui 路径的情况下启动，因此您只需 import launch 并使用其功能： import launch if not launch.is_installed(\"aitextgen\"): launch.run_pip(\"install aitextgen==0.6.0\", \"requirements for MagicPrompt\")"
  },"/sdwui-docs/extensions/": {
    "title": "扩展",
    "keywords": "Guides",
    "url": "/sdwui-docs/extensions/",
    "body": "＃ 基本信息 扩展是一种更方便的用户脚本形式。 扩展都存在于 extensions 目录中它们自己的子目录中。你可以像这样使用 git 安装扩展： git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients extensions/aesthetic-gradients 这会将来自 https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients 的扩展安装到 extensions/aesthetic-gradients 目录中。 或者，您可以将目录复制粘贴到“extensions”中。 关于开发扩展，参见开发扩展。 ＃ 安全 由于扩展允许用户安装和运行任意代码，这可能会被恶意使用，并且在使用允许远程用户连接到服务器的选项（--share 或 --listen）运行时默认禁用 -您仍然拥有用户界面，但尝试安装任何东西都会导致错误。如果您想使用这些选项并且仍然能够安装扩展，请使用 --enable-insecure-extension-access 命令行标志。 扩展 审美渐变 https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients 从一张或几张图片创建嵌入，并使用它将其样式应用于生成的图像。 通配符 https://github.com/AUTOMATIC1111/stable-diffusion-webui-wildcards 允许您在提示中使用 name 语法从通配符目录中名为 name.txt 的文件中获取随机行。 动态提示 https://github.com/adieyal/sd-dynamic-prompts AUTOMATIC1111/stable-diffusion-webui 的自定义扩展，实现了用于随机或组合提示生成的表达模板语言以及支持深度通配符目录的功能结构。 自述文件 中显示了更多功能和新增功能。 使用这个扩展，提示： {2$$artist1 artist2 artist3} 的{夏季 冬季 秋季 春季}的{房子 公寓 旅馆 小屋}` 会出现以下任一提示： 艺术家 1、艺术家 2 的夏季房屋 艺术家 3、艺术家 1 的秋天小屋 艺术家 2、艺术家 3 的冬季小屋 … 如果您正在寻找艺术家和风格的有趣组合，这将特别有用。 您还可以从文件中选择一个随机字符串。假设您在 WILDCARD_DIR 中有文件 seasons.txt（见下文），那么： __seasons__ 来了 可能会生成以下内容： 冬天来了 春天来了 … 您也可以两次使用相同的通配符 我爱__seasons__胜过__seasons__ 比起夏天我更喜欢冬天 我爱春天胜过春天 梦想亭 https://github.com/d8ahazard/sd_dreambooth_extension 用户界面中的 Dreambooth。有关调整和配置要求，请参阅项目自述文件。包括 LoRA（低阶适应） 基于 ShivamShiaro 的回购协议。 智能流程 https://github.com/d8ahazard/sd_smartprocess 智能裁剪、字幕和图像增强。 图像浏览器 https://github.com/yfszzx/stable-diffusion-webui-images-browser 提供在网络浏览器中浏览创建的图像的界面。 ＃＃ 灵感 https://github.com/yfszzx/stable-diffusion-webui-inspiration 随机显示艺术家或艺术流派典型风格的图片，选择后显示更多该艺术家或流派的图片。所以你在创作时不必担心选择合适的艺术风格有多难。 ＃＃ 以下 https://github.com/deforum-art/deforum-for-automatic1111-webui Deforum 的官方端口，一个用于 2D 和 3D 动画的扩展脚本，支持关键帧序列、动态数学参数（甚至在提示内）、动态遮罩、深度估计和变形。 艺术家学习 https://github.com/camenduru/stable-diffusion-webui-artists-to-study https://artiststostudy.pages.dev/ 适用于 [web ui] 的扩展（https://github.com/AUTOMATIC1111/stable-diffusion-webui）。 要安装它，请将 repo 克隆到 extensions 目录并重新启动 web ui： git clone https://github.com/camenduru/stable-diffusion-webui-artists-to-study 您可以通过单击将艺术家姓名添加到剪贴板。 （感谢 @gmaciocci 的想法） 审美形象得分手 https://github.com/tsngo/stable-diffusion-webui-aesthetic-image-scorer https://github.com/AUTOMATIC1111/stable-diffusion-webui 的扩展 基于 Chad Scorer 使用 CLIP+MLP 美学分数预测器 计算生成图像的美学分数-乍得/blob/main/chad_scorer.py) 参见讨论 将分数保存到 windows 标签，并计划其他选项 数据集标签编辑器 https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor 日语自述文件 这是 Stable Diffusion web UI by AUTOMATIC1111 训练数据集中编辑字幕的扩展。 它适用于逗号分隔样式的文本标题（例如 DeepBooru 审讯器生成的标签）。 可以加载图片文件名中的字幕，但编辑后的字幕只能以文本文件的形式保存。 自动 sd-paint-ext https://github.com/Interpause/auto-sd-paint-ext 以前称为“auto-sd-krita”。 使用 Krita 插件扩展 AUTOMATIC1111 的 webUI（其他绘图工作室即将推出？） 过时的演示 新用户界面（全部：演示图像） Differences UI no longer freezes during image update Inpainting layer no longer has to be manually hidden, nor use white specifically UI has been improved &amp; squeezed further Scripts API is now possible training-picker https://github.com/Maurdekye/training-picker Adds a tab to the webui that allows the user to automatically extract keyframes from video, and manually extract 512x512 crops of those frames for use in model training. Installation Install AUTOMATIC1111’s Stable Diffusion Webui Install ffmpeg for your operating system Clone this repository into the extensions folder inside the webui Drop videos you want to extract cropped frames from into the training-picker/videos folder Unprompted https://github.com/ThereforeGames/unprompted Supercharge your prompt workflow with this powerful scripting language! Unprompted is a highly modular extension for AUTOMATIC1111’s Stable Diffusion Web UI that allows you to include various shortcodes in your prompts. You can pull text from files, set up your own variables, process text through conditional functions, and so much more - it’s like wildcards on steroids. While the intended usecase is Stable Diffusion, this engine is also flexible enough to serve as an all-purpose text generator. Booru tag autocompletion https://github.com/DominikDoom/a1111-sd-webui-tagcomplete Displays autocompletion hints for tags from “image booru” boards such as Danbooru. Uses local tag CSV files and includes a config for customization. novelai-2-local-prompt https://github.com/animerl/novelai-2-local-prompt Add a button to convert the prompts used in NovelAI for use in the WebUI. In addition, add a button that allows you to recall a previously used prompt. Tokenizer https://github.com/AUTOMATIC1111/stable-diffusion-webui-tokenizer Adds a tab that lets you preview how CLIP model would tokenize your text. Push to 🤗 Hugging Face https://github.com/camenduru/stable-diffusion-webui-huggingface To install it, clone the repo into the extensions directory and restart the web ui: git clone https://github.com/camenduru/stable-diffusion-webui-huggingface pip install huggingface-hub StylePile https://github.com/some9000/StylePile An easy way to mix and match elements to prompts that affect the style of the result. Latent Mirroring https://github.com/dfaker/SD-latent-mirroring Applies mirroring and flips to the latent images to produce anything from subtle balanced compositions to perfect reflections Embeddings editor https://github.com/CodeExplode/stable-diffusion-webui-embedding-editor Allows you to manually edit textual inversion embeddings using sliders. seed travel https://github.com/yownas/seed_travel Small script for AUTOMATIC1111/stable-diffusion-webui to create images that exists between seeds. shift-attention https://github.com/yownas/shift-attention Generate a sequence of images shifting attention in the prompt. This script enables you to give a range to the weight of tokens in a prompt and then generate a sequence of images stepping from the first one to the second. &lt;https://user-images.githubusercontent.com/13150150/193368939-c0a57440-1955-417c-898a-ccd102e207a5.mp4 prompt travel https://github.com/Kahsolt/stable-diffusion-webui-prompt-travel Extension script for AUTOMATIC1111/stable-diffusion-webui to travel between prompts in latent space. Sonar https://github.com/Kahsolt/stable-diffusion-webui-sonar Improve the generated image quality, searches for similar (yet even better!) images in the neighborhood of some known image, focuses on single prompt optimization rather than traveling between multiple prompts. Detection Detailer https://github.com/dustysys/ddetailer An object detection and auto-mask extension for Stable Diffusion web UI. conditioning-highres-fix https://github.com/klimaleksus/stable-diffusion-webui-conditioning-highres-fix This is Extension for rewriting Inpainting conditioning mask strength value relative to Denoising strength at runtime. This is useful for Inpainting models such as sd-v1-5-inpainting.ckpt Randomize https://github.com/stysmmaker/stable-diffusion-webui-randomize Allows for random parameters during txt2img generation. This script is processed for all generations, regardless of the script selected, meaning this script will function with others as well, such as AUTOMATIC1111/stable-diffusion-webui-wildcards. Auto TLS-HTTPS https://github.com/papuSpartan/stable-diffusion-webui-auto-tls-https Allows you to easily, or even completely automatically start using HTTPS. DreamArtist https://github.com/7eu7d7/DreamArtist-sd-webui-extension Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning. WD 1.4 Tagger https://github.com/toriato/stable-diffusion-webui-wd14-tagger Uses a trained model file, produces WD 1.4 Tags. Model link - https://mega.nz/file/ptA2jSSB#G4INKHQG2x2pGAVQBn-yd_U5dMgevGF8YYM9CR_R1SY booru2prompt https://github.com/Malisius/booru2prompt This SD extension allows you to turn posts from various image boorus into stable diffusion prompts. It does so by pulling a list of tags down from their API. You can copy-paste in a link to the post you want yourself, or use the built-in search feature to do it all without leaving SD. also see: https://github.com/stysmmaker/stable-diffusion-webui-booru-prompt gelbooru-prompt https://github.com/antis0007/sd-webui-gelbooru-prompt Fetch tags with image hash. Updates planned. Merge Board https://github.com/bbc-mc/sdweb-merge-board Multiple lane merge support(up to 10). Save and Load your merging combination as Recipes, which is simple text. also see: https://github.com/Maurdekye/model-kitchen Depth Maps https://github.com/thygate/stable-diffusion-webui-depthmap-script Creates depthmaps from the generated images. The result can be viewed on 3D or holographic devices like VR headsets or lookingglass display, used in Render or Game- Engines on a plane with a displacement modifier, and maybe even 3D printed. multi-subject-render https://github.com/Extraltodeus/multi-subject-render It is a depth aware extension that can help to create multiple complex subjects on a single image. It generates a background, then multiple foreground subjects, cuts their backgrounds after a depth analysis, paste them onto the background and finally does an img2img for a clean finish. depthmap2mask https://github.com/Extraltodeus/depthmap2mask Create masks for img2img based on a depth estimation made by MiDaS. ABG_extension https://github.com/KutsuyaYuki/ABG_extension Automatically remove backgrounds. Uses an onnx model fine-tuned for anime images. Runs on GPU. Visualize Cross-Attention &lt;https://github.com/benkyoujouzu/stable-diffusion-webui-visualize-cross-attention-extension Generates highlighted sectors of a submitted input image, based on input prompts. Use with tokenizer extension. See the readme for more info. DAAM &lt;https://github.com/kousw/stable-diffusion-webui-daam DAAM stands for Diffusion Attentive Attribution Maps. Enter the attention text (must be a string contained in the prompt) and run. An overlapping image with a heatmap for each attention will be generated along with the original image. Prompt Gallery &lt;https://github.com/dr413677671/PromptGallery-stable-diffusion-webui Build a yaml file filled with prompts of your character, hit generate, and quickly preview them by their word attributes and modifiers. embedding-inspector &lt;https://github.com/tkalayci71/embedding-inspector Inspect any token(a word) or Textual-Inversion embeddings and find out which embeddings are similar. You can mix, modify, or create the embeddings in seconds. Much more intriguing options have since been released, see here. Infinity Grid Generator &lt;https://github.com/mcmonkeyprojects/sd-infinity-grid-generator-script Build a yaml file with your chosen parameters, and generate infinite-dimensional grids. Built-in ability to add description text to fields. See readme for usage details. NSFW checker &lt;https://github.com/AUTOMATIC1111/stable-diffusion-webui-nsfw-censor Replaces NSFW images with black. Diffusion Defender &lt;https://github.com/WildBanjos/DiffusionDefender Prompt blacklist, find and replace, for semi-private and public instances. Config-Presets &lt;https://github.com/Zyin055/Config-Presets Adds a configurable dropdown to allow you to change UI preset settings in the txt2img and img2img tabs. Preset Utilities &lt;https://github.com/Gerschel/sd_web_ui_preset_utils Preset tool for UI. Planned support for some other custom scripts. DH Patch &lt;https://github.com/d8ahazard/sd_auto_fix Random patches by D8ahazard. Auto-load config YAML files for v2, 2.1 models; patch latent-diffusion to fix attention on 2.1 models (black boxes without no-half), whatever else I come up with. Riffusion &lt;https://github.com/enlyth/sd-webui-riffusion Use Riffusion model to produce music in gradio. To replicate original interpolation technique, input the prompt travel extension output frames into the riffusion tab. Save Intermediate Images &lt;https://github.com/AlUlkesh/sd_save_intermediate_images Implements saving intermediate images, with more advanced features. openOutpaint extension &lt;https://github.com/zero01101/openOutpaint-webUI-extension A tab with the full openOutpaint UI. Run with the –api flag. Enhanced-img2img &lt;https://github.com/OedoSoldier/enhanced-img2img An extension with support for batched and better inpainting."
  },"/sdwui-docs/features/": {
    "title": "特征",
    "keywords": "Guides",
    "url": "/sdwui-docs/features/",
    "body": "这是 Stable Diffusion web UI 的功能展示页面。 除非另有说明，否则所有示例都是非精选的。 稳定扩散 2.0 基本模型 支持的模型：768-v-ema.ckpt（[模型]（https://huggingface.co/stabilityai/stable-diffusion-2/blob/main/768-v-ema.ckpt），[配置]（https ://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/v2-inference-v.yaml)) 和 512-base-ema.ckpt (模型, [配置](https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/稳定扩散/v2-inference.yaml））。 2.1 检查站也应该工作。 下载检查点（从这里：https://huggingface.co/stabilityai/stable-diffusion-2） 将其放入 models/Stable-Diffusion 目录 从 SD2.0 存储库中获取配置并将其放入与检查点相同的位置，将其重命名为具有相同的文件名（即，如果您的检查点名为“768-v-ema.ckpt”，则配置应命名为“768- v-ema.yaml`) 从 UI 中选择新的检查点 对于 2.0 模型，训练选项卡很可能会被破坏。 如果 2.0 或 2.1 正在生成黑色图像，请使用 --no-half 启用全精度或尝试使用 --xformers 优化。 注意： SD 2.0 和 2.1 对 FP16 数值不稳定性更敏感（正如他们自己所指出的此处）由于它们新的交叉注意力模块。 在 fp16 上：comment 在 webui-user.bat 中启用： @回声关闭 设置蟒蛇= 设置 GIT= 设置 VENV_DIR= 设置 COMMANDLINE_ARGS=你的命令行选项 set STABLE_DIFFUSION_COMMIT_HASH=”c12d960d1ee4f9134c2516862ef991ec52d3f59e” 设置 ATTN_PRECISION=fp16 调用webui.bat ##深度引导模型 更多信息。 PR。 指示： 下载 512-depth-ema.ckpt 检查点 将其放置在模型/稳定扩散中 获取 config 并将其放在与检查点相同的文件夹中 将配置重命名为“512-depth-ema.yaml” 从 UI 中选择新的检查点 深度引导模型仅适用于 img2img 选项卡。 涂装 Outpainting 扩展了原始图像并修复了创建的空白区域。 例子： |原创 |外涂 |再次超越 | |——————————|——————————|——————————| 来自 4chan 的匿名用户的原始图像。谢谢匿名用户。 您可以在底部的 img2img 选项卡中找到该功能，位于脚本 -&gt; 穷人的涂装下。 Outpainting 与普通图像生成不同，它似乎从大步数中获益良多。一个好的涂装的秘诀 是一个很好的提示，与图片相匹配，用于去噪的滑块和 CFG 比例设置为最大，步数为 50 到 100 Euler 祖先或 DPM2 祖先采样器。 | 81 步，欧拉 A | 30 步，欧拉 A | 10 步，欧拉 A | 80 步，欧拉 A | |————————————-|—————————————|————————————–|————————————-| 修复 在 img2img 选项卡中，在图像的一部分上绘制遮罩，该部分将被补漆。 修复选项： 在网络编辑器中自己绘制一个面具 在外部编辑器中删除部分图片并上传透明图片。任何稍微透明的区域都将成为蒙版的一部分。请注意 [某些编辑器](https://docs.krita.org/en/reference_manual/layers_and_masks/split_alpha.html#how-to-save-a-png-texture-and-keep-color-values-in-全透明区域）默认将完全透明区域保存为黑色。 将模式（图片右下角）更改为“上传蒙版”并为蒙版选择单独的黑白图像（白色=修复）。 ##修复模型 RunwayML 已经训练了一个专门为修复而设计的附加模型。这个模型接受额外的输入——没有噪声的初始图像加上蒙版——并且似乎在工作上要好得多。 该模型的下载和额外信息在这里：https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion 要使用该模型，您必须重命名检查点，使其文件名以“inpainting.ckpt”结尾，例如“1.5-inpainting.ckpt”。 之后只需选择检查点，因为您通常会选择任何检查点，您就可以开始了。 屏蔽内容 屏蔽的内容字段决定了在修复之前将内容放入屏蔽区域。 | mask | fill | original | latent noise | latent nothing | |————————————————-|————————————————-|—————————————————–|———————————————————|———————————————————–| 以全分辨率修复 通常，修复会将图像调整为 UI 中指定的目标分辨率。全分辨率 Inpaint 启用后，仅调整遮罩区域的大小，并在处理后粘贴回原始图片。 这允许您处理大图片，并允许您以更大的分辨率渲染修复的对象。 |输入 |修复正常 |全分辨率修复 | |————————————-|———————————-|———————————–| 屏蔽模式 屏蔽模式有两个选项： Inpaint masked - 遮罩下的区域被修复 Inpaint not masked - 面具下没有变化，其他一切都被修复 阿尔法蒙版 |输入 |输出 | |——————————|——————————-| 提示矩阵 使用 | 字符分隔多个提示，系统将为它们的每个组合生成一个图像。 例如，如果您使用“现代城市中繁忙的城市街道|插图|电影灯光”提示，则有四种可能的组合（提示的第一部分始终保留）： 现代城市中繁忙的城市街道 现代城市中繁忙的城市街道，插图 现代城市中繁忙的城市街道，电影般的灯光 -现代城市中繁忙的城市街道，插图，电影灯光 将按此顺序生成四个图像，所有图像都具有相同的种子，每个图像都有相应的提示： 另一个例子，这次有 5 个提示和 16 个变体： 您可以在底部的脚本 -&gt; 提示矩阵下找到该功能。 彩色素描 img2img 的基本着色工具。要在 img2img 中使用此功能，请在命令行参数中使用“–gradio-img2img-tool color-sketch”启用。要在修复模式下使用此功能，请启用“–gradio-inpaint-tool color-sketch”。基于 Chromium 的浏览器支持滴管工具。 （见图） #稳定扩散高档 使用 RealESRGAN/ESRGAN 的高档图像，然后遍历结果的图块，使用 img2img 改进它们。 它还有一个选项，让您可以在外部程序中自己完成放大部分，只需使用 img2img 遍历图块。 原创想法：https://github.com/jquesnelle/txt2imghd。这是一个独立的实现。 要使用此功能，请从脚本下拉选择中选择“SD upscale”（img2img 选项卡）。 输入图像将放大到原始图像的两倍 宽度和高度，以及 UI 的宽度和高度滑块指定各个图块的大小。因为重叠， 拼贴的大小可能非常重要：512x512 图像需要九个 512x512 拼贴（因为重叠），但仅 四个 640x640 的图块。 升级的推荐参数： 采样方法：欧拉a 降噪强度：0.2，如果你喜欢冒险，可以提高到 0.4 |原创 |真ESRGAN |黄玉十亿像素 |标清高档| |——————————————-|———————————————|———————————————————|———————————————| 注意/强调 在提示中使用 () 会增加模型对封闭词的注意力，而 [] 会减少它。您可以组合多个修饰符： 备忘单： a (word) - 将对 word 的关注度提高 1.1 倍 a ((word)) - 将对 word 的关注度提高 1.21 (= 1.1 * 1.1) a [word] - 将对 word 的关注度降低 1.1 倍 a (word:1.5) - 将对 word 的关注度提高 1.5 倍 a (word:0.25) - 将对 word 的关注度降低 4 倍（= 1 / 0.25） a \\(word\\) - 在提示中使用文字 () 字符 使用 ()，可以像这样指定权重：(text:1.4)。如果未指定权重，则假定为 1.1。指定权重仅适用于 () 而不适用于 []。 如果你想在提示中使用任何文字 ()[] 字符，请使用反斜杠将它们转义：anime_\\(character\\)。 在 2022-09-29，添加了支持转义字符和数字权重的新实现。新实现的一个缺点是旧实现并不完美，有时会吃掉字符：例如，“a (((farm))), daytime”，如果没有逗号，将变成“a farm daytime”。正确保留所有文本的新实现不共享此行为，这意味着您保存的种子可能会产生不同的图片。目前，设置中有一个选项可以使用旧的实现。 NAI 使用我在 2022 年 9 月 29 日之前的实现，除了他们使用 1.05 作为乘数并使用“{}”而不是“()”。所以转换适用： 他们的 {word} = 我们的 (word:1.05) 他们的 `` = 我们的 (word:1.1025) 他们的 [word] = 我们的 (word:0.952) (0.952 = 1/1.05) 他们的 [[word]] = 我们的 (word:0.907) (0.907 = 1/1.05/1.05) 环回 在 img2img 中选择环回脚本允许您自动将输出图像作为下一批的输入。相当于 保存输出图像，并用它替换输入图像。批次计数设置控制多少次迭代 这是你得到的。 通常，在执行此操作时，您会自己从众多图像中选择一个用于下一次迭代，因此有用性 这个功能可能有问题，但我已经设法用它获得了一些我无法获得的非常好的输出 否则。 示例：（精选结果） 来自 4chan 的匿名用户的原始图像。谢谢匿名用户。 X/Y 图 创建具有不同参数的图像网格。选择哪些参数应由行和列使用共享 X 类型和Y 类型字段，并将以逗号分隔的那些参数输入到X 值/Y 值字段中。对于整数， 和浮点数，并支持范围。例子： 简单范围： 1-5 = 1、2、3、4、5 括号内增量的范围： 1-5 (+2) = 1, 3, 5 10-5 (-3) = 10, 7 1-3 (+0.5) = 1, 1.5, 2, 2.5, 3 方括号中包含计数的范围： 1-10 [5] = 1, 3, 5, 7, 10 0.0-1.0 [6] = 0.0, 0.2, 0.4, 0.6, 0.8, 1.0 以下是创建上图的设置： 提示S/R Prompt S/R 是 X/Y Plot 较难理解的操作模式之一。 S/R 代表搜索/替换，这就是它的作用 - 您输入一个单词或短语列表，它从列表中获取第一个并将其视为关键字，然后用列表中的其他条目替换该关键字的所有实例. 例如，使用提示 a man holding an apple, 8k clean 和 Prompt S/R an apple, a watermelon, a gun 你会得到三个提示： -一个拿着苹果的男人，8k 干净 -一个拿着西瓜的人，8k 干净 -一个拿着枪的男人，8k 干净 该列表使用与 CSV 文件中的一行相同的语法，因此如果您想在条目中包含逗号，您必须将文本放在引号中并确保引号和分隔逗号之间没有空格： darkness, light, green, heat - 4 items - darkness, light, green, heat darkness, \"light, green\", heat - 错误 - 4 items - darkness, \"light, green\", heat darkness,\"light, green\", heat - RIGHT - 3 items - darkness, light, green, heat 文本反转 简短说明：将您的嵌入放入 embeddings 目录，并在提示中使用文件名。 详细解释：Textual Inversion 调整大小 在 img2img 模式下调整输入图像的大小有三个选项： 调整大小 - 简单地将源图像调整为目标分辨率，导致宽高比不正确 裁剪和调整大小 - 调整源图像的大小以保持纵横比，使其占据整个目标分辨率，并裁剪突出的部分 调整大小和填充 - 调整源图像的大小，保留宽高比，使其完全适合目标分辨率，并按源图像的行/列填充空白空间 例子： 采样方式选择 为txt2img挑出多种采样方式： 种子调整大小 此功能允许您从不同分辨率的已知种子生成图像。通常，当您更改分辨率时， 即使保留所有其他参数（包括种子），图像也会完全改变。通过调整种子大小，您可以指定分辨率 的原始图像，并且模型很可能会产生看起来非常相似的东西，即使分辨率不同。 在下面的示例中，最左边的图片是 512x512，其他图片使用完全相同的参数生成，但垂直方向更大 解析度。 |资讯 |图片 | |—————————|——————————-| 未启用种子调整大小 种子从 512x512 调整大小 祖先采样器在这方面比其他采样器差一点。 您可以通过单击种子附近的“额外”复选框找到此功能。 ＃ 变化 变化强度滑块和变化种子字段允许您指定现有图片应更改多少以使其看起来 像一个不同的。在最大强度下，您将获得带有变异种子的图片，至少 - 带有原始种子的图片（除了 用于使用祖先采样器时）。 您可以通过单击种子附近的“额外”复选框找到此功能。 样式 按“将提示另存为样式”按钮将当前提示写入 styles.csv，这是一个包含样式集合的文件。一个保管箱 提示的右侧将允许您从以前保存的样式中选择任何样式，并自动将其附加到您的输入中。 要删除样式，请从 styles.csv 中手动删除它并重新启动程序。 如果您在样式中使用特殊字符串“{prompt}”，它会将提示中当前的任何内容替换到该位置，而不是将样式附加到您的提示中。 否定提示 允许您使用另一个提示，提示模型在生成图片时应避免的事情。这通过使用 采样过程中无条件条件的否定提示，而不是空字符串。 进阶解释：【否定提示】（Negative-prompt） |原创 |负：紫色 |负面：触角 | |——————————-|———————————|————————————| #CLIP 询问器 最初来自：https://github.com/pharmapsychotic/clip-interrogator CLIP 询问器允许您从图像中检索提示。提示不允许您重现此内容 精确的图像（有时它甚至不会很接近），但这可能是一个好的开始。 第一次运行 CLIP interrogator 时，它会下载几千兆字节的模型。 CLIP 询问器有两个部分：一个是 BLIP 模型，它根据图片创建文本描述。 另一个是 CLIP 模型，它会从列表中挑选出与图片相关的几行。默认情况下，有 只是一个列表 - 艺术家列表（来自 artists.csv）。您可以通过执行以下操作添加更多列表： 在与 webui 相同的位置创建 interrogate 目录 将文本文件放入其中，每行都有相关描述 有关要使用的文本文件的示例，请参阅 https://github.com/pharmapsychotic/clip-interrogator/tree/main/data。 事实上，您可以从那里获取文件并使用它们 - 只需跳过 artists.txt 因为您已经有了一个列表 artists.csv 中的艺术家（或者也使用它，谁会阻止你）。每个文件在最终描述中添加一行文本。 如果您添加“.top3”。到文件名，例如 flavors.top3.txt，该文件中最相关的三行将是 添加到提示中（其他号码也可以）。 有与此功能相关的设置： Interrogate: keep models in VRAM - 请勿在使用后从内存中卸载 Interrogate 模型。对于具有大量 VRAM 的用户。 Interrogate: use artists from artists.csv - 在询问时添加来自 artists.csv 的艺术家。当你在 interrogate 目录中有你的艺术家列表时，禁用它可能很有用 Interrogate: num_beams for BLIP - 影响 BLIP 模型详细描述的参数（生成提示的第一部分） Interrogate: minimum description length - BLIP 模型文本的最小长度 Interrogate: maximum descripton length - BLIP 模型文本的最大长度 Interrogate: text file 中的最大行数 - interrogator 将只考虑文件中这么多的第一行。设置为 0，默认为 1500，大约是 4GB 视频卡可以处理的量。 快速编辑 即时编辑允许您开始对一张图片进行采样，但在中间切换到其他图片。其基本语法是： [from:to:when] 其中“from”和“to”是任意文本，“when”是一个数字，它定义了应该在采样周期的多晚进行切换。越晚，模型绘制“to”文本代替“from”文本的能力就越小。如果“when”是一个介于 0 和 1 之间的数字，则它是进行切换之前的步数的一小部分。如果它是一个大于零的整数，它就是进行切换之前的步骤。 将一个提示编辑嵌套在另一个提示编辑中确实有效。 此外： [to:when] - 在固定数量的步骤后将 to 添加到提示中（when） [from::when] - 在固定步数（when）后从提示中删除 from 例子： [幻想：赛博朋克：16]风景 开始时，模型将绘制“幻想风景”。 在第 16 步之后，它将切换到绘制“赛博朋克风景”，从它停止的地方继续幻想。 这是一个包含多个编辑的更复杂的示例： 带有 [mountain:lake:0.25] 和 [an oak:a christmas tree:0.75][ 前景 ::0.6][ 背景 0.25] [shoddy:masterful:0.5] 的奇幻风景（采样器有 100 个步骤） 开始时，“前景中有一座山和一棵橡树的奇幻风景” 在第 25 步之后，“前景中有一个湖泊和一棵橡树的奇幻风景，背景很差” 在第 50 步之后，“前景中有一个湖泊和一棵橡树的奇幻风景非常棒” 在第 60 步之后，“以湖泊和橡树为背景的奇幻景观精湛” 在第 75 步之后，“背景中有湖泊和一棵圣诞树的奇幻景观精湛” 顶部的图片是根据提示制作的： `微笑的第二次世界大战将军的官方肖像，[男：女：0.99]，开朗，快乐，详细的脸，20 世纪，高度详细，电影照明，Greg Rutkowski 的数字艺术绘画 数字 0.99 将替换为您在图像的列标签中看到的任何内容。 图片最后一列是[male:female:0.0]，这本质上是让模型从头开始画女性，而不是从男性将军开始，这就是为什么它看起来与其他人如此不同。 交替词 每隔一步交换的方便语法。 [牛 马]在田野里 在第 1 步中，提示是“田间的奶牛”。第 2 步是“田野里的马”。第 3 步是“田野里的奶牛”，依此类推。 请参阅下面的更高级示例。在第 8 步，链条从“人”循环回到“牛”。 [牛 牛 马 人 东北虎 牛 人]在田野里 提示编辑首先由 Doggettx 在 this myspace.com post 中实现。 ＃ 高分辨率。使固定 一个方便的选项，可以以较低的分辨率部分渲染您的图像，放大它，然后以高分辨率添加细节。默认情况下，txt2img 以非常高的分辨率制作可怕的图像，这使得避免使用小图片的构图成为可能。通过选中 txt2img 页面上的“Highres.fix”复选框启用。 |没有|与| |——————————-|———————————| 可组合扩散 允许组合多个提示的方法。 使用大写字母 AND 组合提示 一只猫和一只狗 支持提示权重：a cat :1.2 AND a dog AND a penguin :2.2 默认权重值为 1。 将多个嵌入组合到结果中可能非常有用：creature_embedding in the woods:0.7 AND arcane_embedding:0.5 AND glitch_embedding:0.2 使用低于 0.1 的值几乎没有效果。 a cat AND a dog:0.03 将产生与 a cat 基本相同的输出 通过继续将更多提示附加到您的总数，这对于生成微调的递归变体可能很方便。 log 上的 creature_embedding AND frog:0.13 AND yellow eyes:0.08 ＃ 打断 按中断按钮停止当前处理。 4GB 显卡支持 针对具有低 VRAM 的 GPU 的优化。这应该可以在具有 4GB 内存的视频卡上生成 512x512 图像。 --lowvram 是 basujindal 优化思想的重新实现。 模型被分成模块，GPU内存中只保留一个模块；当另一个模块需要运行时，前一个 从 GPU 内存中删除。这种优化的本质使处理运行得更慢——大约慢 10 倍 与我的 RTX 3090 上的正常操作相比。 --medvram 是另一个优化，应该通过不处理条件和显着减少 VRAM 使用 同一批次无条件去噪。 这种优化的实现不需要对原始的稳定扩散代码进行任何修改。 人脸修复 让您使用 GFPGAN 或 CodeFormer 改善图片中的人脸。每个选项卡中都有一个复选框来使用面部修复， 还有一个单独的选项卡，只允许您在任何图片上使用面部修复，并带有一个滑块来控制可见度 效果是。您可以在设置中选择这两种方法。 |原创 | GFPGAN |代码形成者 | |————————-|——————————–|————————————| 保存 单击输出部分下的保存按钮，生成的图像将保存到设置中指定的目录中； 生成参数将附加到同一目录中的 csv 文件。 加载中 Gradio 的加载图形对神经网络的处理速度有非常不利的影响。 当带渐变的选项卡未处于活动状态时，我的 RTX 3090 使图像速度提高约 10%。默认情况下，用户界面 现在隐藏加载进度动画并将其替换为静态“正在加载…”文本，从而实现 同样的效果。使用 --no-progressbar-hiding 命令行选项来恢复它并显示加载动画。 提示验证 Stable Diffusion 对输入文本长度有限制。如果你的提示太长，你会得到一个 文本输出字段中的警告，显示文本的哪些部分被模型截断和忽略。 PNG 信息 将有关生成参数的信息作为文本块添加到 PNG。你 以后可以使用任何支持查看的软件查看此信息 PNG 块信息，例如：https://www.nayuki.io/page/png-file-chunk-inspector 设置 带有设置的选项卡，允许您使用 UI 编辑以前无法编辑的一半以上的参数 是命令行。设置保存到 config.js 文件。保留为命令行的设置 选项是启动时需要的选项。 文件名格式 “设置”选项卡中的“图像文件名模式”字段允许自定义生成的 txt2img 和 img2img 图像文件名。此模式定义了要包含在文件名中的生成参数及其顺序。支持的标签是： [steps], [cfg], [prompt], [prompt_no_styles], [prompt_spaces], [width], [height], [styles], [sampler], [seed], [model_hash], [prompt_words], [日期], [日期时间], [job_timestamp]. 这个列表会随着新的添加而发展。您可以通过将鼠标悬停在 UI 中的“图像文件名模式”标签上来获取最新的支持标签列表。 模式示例：[seed]-[steps]-[cfg]-[sampler]-[prompt_spaces] 关于“提示”标签的注意事项：[prompt] 将在提示词之间添加下划线，而 [prompt_spaces] 将保持提示完整（更容易再次复制/粘贴到 UI 中）。 [prompt_words] 是您的提示的简化和清理版本，已用于生成子目录名称，仅包含您的提示文字（无标点符号）。 如果将此字段留空，将应用默认模式 ([seed]-[prompt_spaces])。 请注意，标签实际上在模式内部被替换了。这意味着您还可以向此模式添加非标签词，以使文件名更加明确。例如：s=[seed],p=[prompt_spaces] 用户脚本 如果程序使用 --allow-code 选项启动，脚本代码的额外文本输入字段 位于页面底部的脚本 -&gt; 自定义代码下。它允许你输入python 将处理图像的代码。 在代码中，使用“p”变量从 Web UI 访问参数，并为 Web UI 提供输出 使用 display(images, seed, info) 函数。脚本中的所有全局变量也可以访问。 一个简单的脚本，只处理图像并正常输出： import modules.processing processed = modules.processing.process_images(p) print(\"Seed was: \" + str(processed.seed)) display(processed.images, processed.seed, processed.info) 界面配置 您可以更改 UI 元素的参数： 无线电组：默认选择 滑块：默认值、最小值、最大值、步长 复选框：选中状态 文本和数字输入：默认值 该文件是 webui 目录下的 ui-config.json ，如果程序启动时没有，它会自动创建。 通常会展开隐藏部分的复选框在设置为 UI 配置条目时最初不会这样做。 某些设置会中断处理，例如宽度和高度不能被 64 整除的步长，以及某些设置，例如更改默认设置 img2img 选项卡上的功能，可能会破坏 UI。我没有计划在不久的将来解决这些问题。 #ESRGAN 可以在 Extras 选项卡以及 SD 高档中使用 ESRGAN 模型。 要使用 ESRGAN 模型，请将它们放入与 webui.py 相同位置的 ESRGAN 目录中。 如果文件具有 .pth 扩展名，则该文件将作为模型加载。从 模型数据库 中获取模型。 并非数据库中的所有模型都受支持。很可能不支持所有 2x 模型。 img2img 替代测试 使用 Euler 扩散器的反向解构输入图像，以创建用于构建输入提示的噪声模式。 例如，您可以使用此图像。从 scripts 部分选择 img2img 替代测试。 调整重建过程的设置： 使用场景的简短描述：“一个棕色头发的微笑女人。”描述您要更改的功能会有所帮助。将其设置为您的启动提示，并在脚本设置中设置“原始输入提示”。 你必须使用欧拉采样方法，因为这个脚本是建立在它之上的。 采样步数：50-60。这与脚本中的解码步骤值非常匹配，否则您会遇到麻烦。在此演示中使用 50。 CFG 等级：2 或更低。对于此演示，请使用 1.8。 （提示，您可以编辑 ui-config.json 将“img2img/CFG Scale/step”更改为 .1 而不是 .5。 去噪强度 - 这确实重要，与旧文档所说的相反。将其设置为 1。 宽度/高度 - 使用输入图像的宽度/高度。 种子……你可以忽略这个。反向欧拉现在正在为图像生成噪声。 解码 cfg 比例 - 低于 1 的地方是最佳点。对于演示，使用 1。 解码步骤 - 如上所述，这应该与您的采样步骤相匹配。 50 用于演示，考虑增加到 60 以获得更详细的图像。 拨入以上所有内容后，您应该能够点击“生成”并返回一个非常接近原始结果的结果。 在验证脚本以良好的准确度重新生成源照片后，您可以尝试更改提示的详细信息。原始图像的较大变化可能会导致图像的构图与源图像完全不同。 使用上述设置和下面提示的示例输出（图中未显示红头发/小马） “一个微笑的蓝头发女人。”作品。 “一个皱着眉头的棕色头发的女人。”作品。 “一个皱着眉头的红头发女人。”作品。 “一个皱着眉头的红发女子骑着马。”似乎完全取代了女人，现在我们有了一匹姜黄色的小马。 用户.css 在 webui.py 附近创建一个名为 user.css 的文件，并将自定义 CSS 代码放入其中。例如，这会使画廊更高： #txt2img_gallery, #img2img_gallery{ min-height: 768px; } 一个有用的提示是您可以将 /?__theme=dark 附加到您的 webui url 以启用内置的 dark 主题 例如(http://127.0.0.1:7860/?__theme=dark) 或者，您可以将 --theme=dark 添加到 webui-user.bat 中的 set COMMANDLINE_ARGS= 例如set COMMANDLINE_ARGS=--theme=dark 通知.mp3 如果名为 notification.mp3 的音频文件存在于 webui 的根文件夹中，它将在生成过程完成时播放。 作为灵感来源： https://pixabay.com/sound-effects/search/ding/?duration=0-30 https://pixabay.com/sound-effects/search/notification/?duration=0-30 调整 忽略 CLIP 模型的最后一层 这是设置中的一个滑块，它控制 CLIP 网络处理提示的时间应该多早停止。 更详细的解释： CLIP 是一种非常先进的神经网络，可将您的提示文本转换为数字表示。神经网络可以很好地处理这种数字表示，这就是为什么 SD 的开发者选择 CLIP 作为稳定扩散图像生成方法中涉及的 3 个模型之一的原因。由于 CLIP 是一个神经网络，这意味着它有很多层。您的提示以简单的方式数字化，然后通过层馈送。您在第一层之后获得提示的数字表示，将其提供给第二层，将其结果提供给第三层，依此类推，直到到达最后一层，这就是稳定版中使用的 CLIP 的输出扩散。这是 1 的滑块值。但是您可以提前停止，并使用倒数第二层的输出 - 这是 2 的滑块值。您停止得越早，处理提示的神经网络层数就越少。 一些模型是通过这种调整进行训练的，因此设置此值有助于在这些模型上产生更好的结果。"
  },"/sdwui-docs/install-on-amd/": {
    "title": "在 AMD 上安装",
    "keywords": "development",
    "url": "/sdwui-docs/install-on-amd/",
    "body": "以下说明仅适用于 Linux！可以在此处（未经测试）找到适用于 Windows 用户的替代指南。 原生运行 执行以下操作： git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui cd stable-diffusion-webui python -m venv venv source venv/bin/activate python -m pip install --upgrade pip wheel # It's possible that you don't need \"--precision full\", dropping \"--no-half\" however crashes my drivers TORCH_COMMAND='pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/rocm5.1.1' python launch.py --precision full --no-half 在接下来的运行中，您只需要执行： cd stable-diffusion-webui # Optional: \"git pull\" to update the repository source venv/bin/activate # It's possible that you don't need \"--precision full\", dropping \"--no-half\" however crashes my drivers TORCH_COMMAND='pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/rocm5.1.1' python launch.py --precision full --no-half 启动 WebUI 后的第一代可能需要很长时间，您可能会看到类似这样的消息： MIOpen(HIP)：警告 [SQLiteBase] 缺少系统数据库文件：gfx1030_40.kdb 性能可能会降低。请关注 安装说明：https://github.com/ROCmSoftwarePlatform/MIOpen#installing-miopen-kernels-package 下一代应该以正常的表现工作。你可以点击消息中的链接，如果你碰巧 要使用相同的操作系统，请按照此处的步骤解决此问题。如果没有明确的编译方式或 为您的操作系统安装 MIOpen 内核，请考虑遵循下面的“在 Docker 中运行”指南。 在 Docker 中运行 拉取最新的 rocm/pytorch Docker 镜像，启动镜像并附加到容器（取自 rocm/pytorch 文档）：docker run -it --network=host --device=/dev/kfd --device=/dev/dri --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -v $HOME/dockerx:/dockerx rocm/pytorch 在容器内执行以下命令： cd /dockerx git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui cd stable-diffusion-webui python -m venv venv source venv/bin/activate python -m pip install --upgrade pip wheel # It's possible that you don't need \"--precision full\", dropping \"--no-half\" however crashes my drivers TORCH_COMMAND='pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/rocm5.1.1' REQS_FILE='requirements.txt' python launch.py --precision full --no-half 以下运行只需要您重新启动容器，再次附加到它并在 容器：从这个列表中找到容器名称：docker container ls --all，选择与 rocm/pytorch 图像，重新启动它：docker container restart &lt;container-id&gt; 然后附加到它：`docker exec -it bash`。 ```bash cd /dockerx/stable-diffusion-webui # Optional: \"git pull\" to update the repository source venv/bin/activate # It's possible that you don't need \"--precision full\", dropping \"--no-half\" however crashes my drivers TORCH_COMMAND='pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/rocm5.1.1' REQS_FILE='requirements.txt' python launch.py --precision full --no-half ``` 容器内的 `/dockerx` 文件夹应该可以在您的主目录中以相同的名称访问。 ## 在 Docker 中更新 Python 版本 如果 Web UI 与 Docker 映像中预安装的 Python 3.7 版本不兼容，这里是 有关如何更新它的说明（假设您已成功遵循“在 Docker 中运行”）： 在容器内执行以下命令： ```bash apt install python3.9-full # Confirm every prompt update-alternatives --install /usr/local/bin/python python /usr/bin/python3.9 1 echo 'PATH=/usr/local/bin:$PATH' &gt;&gt; ~/.bashrc ``` 然后重新启动容器并再次附加。如果您检查“python --version”，它现在应该显示“Python 3.9.5”或更新版本。 在容器内运行 rm -rf /dockerx/stable-diffusion-webui/venv ，然后按照“在内部运行”中的步骤操作 Docker”，跳过 `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui` 并使用修改后的 下面的启动命令： ```bash TORCH_COMMAND='pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/rocm5.1.1' python launch.py --precision full --no-half ``` 您可能不需要“--precision full”，删除“--no-half”，但它可能不适用于所有人。 某些显卡，如 Radeon RX 6000 系列和 RX 500 系列，无需选项 `--precision full --no-half` 即可正常运行，从而节省大量显存。 （注意 [此处](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/5468)。） 从现在开始始终使用这个新的启动命令，在后续运行中重新启动 Web UI 时也是如此。"
  },"/sdwui-docs/install-on-nvidia/": {
    "title": "在英伟达上安装",
    "keywords": "development",
    "url": "/sdwui-docs/install-on-nvidia/",
    "body": "在尝试安装之前，请确保满足所有必需的 dependencies。 自动安装 窗口 从 Windows Explorer 正常运行 webui-user.bat，非管理员，用户。 请参阅 故障排除 部分了解如果出现问题该怎么办。 Linux 要在默认目录/home/$(whoami)/stable-diffusion-webui/ 中安装，请运行： bash &lt;(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh) 为了自定义安装，将存储库克隆到所需位置，更改 webui-user.sh 中所需的变量并运行： bash webui.sh 几乎自动安装和启动 要在不创建虚拟环境的情况下通过 pip 安装所需的包，请运行： python launch.py 命令行参数可以直接传递，例如： python launch.py --opt-split-attention --ckpt ../secret/anime9999.ckpt 手动安装 手动安装非常过时，可能无法正常工作。查看存储库自述文件中的 colab 以获取说明。 以下过程在 Windows 或 Linux 上手动安装所有内容（后者需要将 dir 替换为 ls）： # install torch with CUDA support. See https://pytorch.org/get-started/locally/ for more instructions if this fails. pip install torch --extra-index-url https://download.pytorch.org/whl/cu113 # check if torch supports GPU; this must output \"True\". You need CUDA 11. installed for this. You might be able to use # a different version, but this is what I tested. python -c \"import torch; print(torch.cuda.is_available())\" # clone web ui and go into its directory git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git cd stable-diffusion-webui # clone repositories for Stable Diffusion and (optionally) CodeFormer mkdir repositories git clone https://github.com/CompVis/stable-diffusion.git repositories/stable-diffusion git clone https://github.com/CompVis/taming-transformers.git repositories/taming-transformers git clone https://github.com/sczhou/CodeFormer.git repositories/CodeFormer git clone https://github.com/salesforce/BLIP.git repositories/BLIP # install requirements of Stable Diffusion pip install transformers==4.19.2 diffusers invisible-watermark --prefer-binary # install k-diffusion pip install git+https://github.com/crowsonkb/k-diffusion.git --prefer-binary # (optional) install GFPGAN (face restoration) pip install git+https://github.com/TencentARC/GFPGAN.git --prefer-binary # (optional) install requirements for CodeFormer (face restoration) pip install -r repositories/CodeFormer/requirements.txt --prefer-binary # install requirements of web ui pip install -r requirements.txt --prefer-binary # update numpy to latest version pip install -U numpy --prefer-binary # (outside of command line) put stable diffusion model into web ui directory # the command below must output something like: 1 File(s) 4,265,380,512 bytes dir model.ckpt 安装完成，启动web ui，运行： python webui.py Windows 11 WSL2 说明 要在 Windows 11 的 WSL2 中的 Linux 发行版下安装： # install conda (if not already done) wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh chmod +x Anaconda3-2022.05-Linux-x86_64.sh ./Anaconda3-2022.05-Linux-x86_64.sh # Clone webui repo git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git cd stable-diffusion-webui # Create and activate conda env conda env create -f environment-wsl2.yaml conda activate automatic 此时，可以应用手动安装的说明，从步骤“# clone repositories for Stable Diffusion and (optional) CodeFormer”开始。 在 Windows 上使用 Conda 进行替代安装 先决条件 *（仅当您没有时才需要）*。假设安装了 Chocolatey。 # 安装 git choco 安装 git # 安装畅达 choco 安装 anaconda3 可选参数：git, conda 安装（警告：某些文件超过数 GB，请先确保您有空间） 下载为 .zip 并解压或使用 git 克隆。 git 克隆 https://github.com/AUTOMATIC1111/stable-diffusion-webui.git 启动 Anaconda 提示符。应该注意的是，您可以使用较旧的 Python 版本，但您可能被迫手动删除缓存优化等功能，这会降低您的性能。 ```狂欢 导航到git目录 cd “GIT\\StableDiffusion” 创建环境 conda create -n StableDiffusion python=3.10.6 激活环境 conda 激活 StableDiffusion 验证环境是否被选中 conda 环境列表 启动本地网络服务器 webui用户.bat 等待“在本地 URL 上运行：http://127.0.0.1:7860”并打开该 URI。 ``` *（可选）* 转到 CompVis 并下载最新模型，例如 1.4 并将其解压为 ex: 狂欢 GIT\\StableDiffusion\\模型\\稳定扩散 之后通过重新启动 Anaconda 提示符和 狂欢 webui用户.bat 值得尝试的替代默认设置： 尝试 euler a (Ancestral Euler) 和更高的 Sampling Steps 例如：40 或其他 100。 将“设置 &gt; 用户界面 &gt; 每 N 个采样步骤显示图像创建进度”设置为 1，然后选择一个确定性的 种子 值。可以直观地看到图像解散是如何发生的，并使用 ScreenToGif 录制一个 .gif。 3.使用恢复人脸。通常，结果会更好，但质量是以速度为代价的。"
  },"/sdwui-docs/install-on-apple/": {
    "title": "在苹果上安装",
    "keywords": "development",
    "url": "/sdwui-docs/install-on-apple/",
    "body": "Mac 用户：请提供反馈，说明这些说明是否适合您，如果有任何不清楚的地方，或者您仍然遇到当前未在此处提及 的安装问题/AUTOMATIC1111/stable-diffusion-webui/discussions/5461)。 重要笔记 当前，Web UI 中的大多数功能都可以在 macOS 上正常运行，最明显的例外是 CLIP 询问器和培训。尽管训练似乎确实有效，但它非常慢并且消耗过多的内存。可以使用 CLIP 询问器，但它不能与 macOS 使用的 GPU 加速一起正常工作，因此默认配置将完全通过 CPU 运行它（这很慢）。 众所周知，大多数采样器都可以工作，唯一的例外是使用稳定扩散 2.0 模型时的 PLMS 采样器。在 macOS 上使用 GPU 加速生成的图像通常应该匹配或几乎匹配在具有相同设置和种子的 CPU 上生成的图像。 自动安装 新安装： 如果未安装 Homebrew，请按照 https://brew.sh 上的说明进行安装。保持终端窗口打开并按照“后续步骤”下的说明将 Homebrew 添加到您的 PATH。 打开一个新的终端窗口并运行brew install cmake protobuf rust python@3.10 git wget 通过运行 git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui 克隆 Web UI 存储库 将要使用的 Stable Diffusion 模型/检查点放入 stable-diffusion-webui/models/Stable-diffusion。如果没有，请参阅 下载稳定的扩散模型以下。 cd stable-diffusion-webui 然后 ./webui.sh 运行 web UI。将使用 venv 创建并激活 Python 虚拟环境，并且将自动下载并安装任何剩余的缺失依赖项。 要稍后重新启动 Web UI 进程，请再次运行 ./webui.sh。请注意，它不会自动更新网络用户界面；要更新，请在运行 ./webui.sh 之前运行 git pull。 现有安装： 如果您已经安装了使用“setup_mac.sh”创建的 Web UI，请从“stable-diffusion-webui”文件夹中删除“run_webui_mac.sh”文件和“repositories”文件夹。然后运行 ​​git pull 来更新 web UI，然后运行 ​​./webui.sh 来运行它。 下载稳定的扩散模型 如果您没有任何模型可以使用，可以从 Hugging Face 下载 Stable Diffusion 模型。要下载，请单击模型，然后单击“文件和版本”标题。查找以“.ckpt”或“.safetensors”扩展名列出的文件，然后单击文件大小右侧的向下箭头以下载它们。 一些流行的官方稳定扩散模型是： 稳定扩散 1.4 (sd-v1-4.ckpt) 稳定扩散 1.5 ([v1-5-pruned-emaonly.ckpt](https://huggingface.co/runwayml/stable- diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt）） 稳定扩散 1.5 修复 (sd-v1-5-inpainting.ckpt) Stable Diffusion 2.0 和 2.1 需要模型和配置文件，生成图像时图像宽度和高度需要设置为 768 或更高： 稳定扩散 2.0 (768-v-ema.ckpt) 稳定扩散 2.1 (v2-1_768-ema-pruned.ckpt) 配置文件，按住键盘上的option键，点击这里 下载 v2-inference-v.yaml（它可能会下载为 v2-inference-v.yaml.yml）。在 Finder 中选择该文件，然后转到菜单并选择“文件”&gt;“获取信息”。在出现的窗口中选择文件名并将其更改为模型的文件名，除了文件扩展名为“.yaml”而不是“.ckpt”，按键盘上的回车键（如果出现提示，请确认更改文件扩展名），然后将它放在与模型相同的文件夹中（例如，如果您下载了 768-v-ema.ckpt 模型，请将其重命名为 768-v-ema.yaml 并将其放入 stable-diffusion-webui/models /Stable-diffusion 以及模型）。 还提供了 Stable Diffusion 2.0 深度模型 (512-depth-ema.ckpt)。通过按住键盘上的选项并单击 此处，然后按照上面提到的方式用.yaml扩展名重命名，和模型一起放在stable-diffusion-webui/models/Stable-diffusion中。请注意，此模型适用于 512 宽度/高度或更高而不是 768 的图像尺寸。 故障排除 Web UI 无法启动： 如果您在尝试使用 ./webui.sh 启动 Web UI 时遇到错误，请尝试从 stable-diffusion-webui 文件夹中删除 repositories 和 venv 文件夹，然后使用 git pull 更新 Web UI ` 在再次运行 ./webui.sh 之前。 ＃＃＃ 表现不佳： 目前 macOS 上的 GPU 加速使用了_lot_内存。如果性能不佳（如果使用任何采样器生成 20 个步骤的 512x512 图像需要超过一分钟）首先尝试从 --opt-split-attention-v1 命令行选项（即 ./webui. sh --opt-split-attention-v1) 看看是否有帮助。如果这没有太大区别，则打开位于 /Applications/Utilities 中的 Activity Monitor 应用程序并检查 Memory 选项卡下的内存压力图。如果生成图像时内存压力显示为红色，请关闭 web UI 进程，然后添加 --medvram 命令行选项（即 ./webui.sh --opt-split-attention-v1 -- medvram）。如果该选项的性能仍然很差并且内存压力仍然是红色，那么请尝试使用 --lowvram（即 ./webui.sh --opt-split-attention-v1 --lowvram）。如果使用任何采样器生成 20 个步骤的 512x512 图像仍然需要几分钟以上，那么您可能需要关闭 GPU 加速。在 Xcode 中打开 webui-user.sh 并将 #export COMMANDLINE_ARGS=\"\" 更改为 export COMMANDLINE_ARGS=\"--skip-torch-cuda-test --no-half --use-cpu all\"。 此处的讨论/反馈：https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/5461"
  },"/sdwui-docs/list-of-time-zones/": {
    "title": "时区列表",
    "keywords": "Guides",
    "url": "/sdwui-docs/list-of-time-zones/",
    "body": "您可以通过运行此 Python 脚本生成有效时区列表 import pytz for tz in pytz.all_timezones: print(tz) 或者您可以参考这个预先生成的列表（可能已过时）。 Africa/Accra Africa/Addis_Ababa Africa/Algiers Africa/Asmara Africa/Asmera Africa/Bamako Africa/Bangui Africa/Banjul Africa/Bissau Africa/Blantyre Africa/Brazzaville Africa/Bujumbura Africa/Cairo Africa/Casablanca Africa/Ceuta Africa/Conakry Africa/Dakar Africa/Dar_es_Salaam Africa/Djibouti Africa/Douala Africa/El_Aaiun Africa/Freetown Africa/Gaborone Africa/Harare Africa/Johannesburg Africa/Juba Africa/Kampala Africa/Khartoum Africa/Kigali Africa/Kinshasa Africa/Lagos Africa/Libreville Africa/Lome Africa/Luanda Africa/Lubumbashi Africa/Lusaka Africa/Malabo Africa/Maputo Africa/Maseru Africa/Mbabane Africa/Mogadishu Africa/Monrovia Africa/Nairobi Africa/Ndjamena Africa/Niamey Africa/Nouakchott Africa/Ouagadougou Africa/Porto-Novo Africa/Sao_Tome Africa/Timbuktu Africa/Tripoli Africa/Tunis Africa/Windhoek America/Adak America/Anchorage America/Anguilla America/Antigua America/Araguaina America/Argentina/Buenos_Aires America/Argentina/Catamarca America/Argentina/ComodRivadavia America/Argentina/Cordoba America/Argentina/Jujuy America/Argentina/La_Rioja America/Argentina/Mendoza America/Argentina/Rio_Gallegos America/Argentina/Salta America/Argentina/San_Juan America/Argentina/San_Luis America/Argentina/Tucuman America/Argentina/Ushuaia America/Aruba America/Asuncion America/Atikokan America/Atka America/Bahia America/Bahia_Banderas America/Barbados America/Belem America/Belize America/Blanc-Sablon America/Boa_Vista America/Bogota America/Boise America/Buenos_Aires America/Cambridge_Bay America/Campo_Grande America/Cancun America/Caracas America/Catamarca America/Cayenne America/Cayman America/Chicago America/Chihuahua America/Coral_Harbour America/Cordoba America/Costa_Rica America/Creston America/Cuiaba America/Curacao America/Danmarkshavn America/Dawson America/Dawson_Creek America/Denver America/Detroit America/Dominica America/Edmonton America/Eirunepe America/El_Salvador America/Ensenada America/Fort_Nelson America/Fort_Wayne America/Fortaleza America/Glace_Bay America/Godthab America/Goose_Bay America/Grand_Turk America/Grenada America/Guadeloupe America/Guatemala America/Guayaquil America/Guyana America/Halifax America/Havana America/Hermosillo America/Indiana/Indianapolis America/Indiana/Knox America/Indiana/Marengo America/Indiana/Petersburg America/Indiana/Tell_City America/Indiana/Vevay America/Indiana/Vincennes America/Indiana/Winamac America/Indianapolis America/Inuvik America/Iqaluit America/Jamaica America/Jujuy America/Juneau America/Kentucky/Louisville America/Kentucky/Monticello America/Knox_IN America/Kralendijk America/La_Paz America/Lima America/Los_Angeles America/Louisville America/Lower_Princes America/Maceio America/Managua America/Manaus America/Marigot America/Martinique America/Matamoros America/Mazatlan America/Mendoza America/Menominee America/Merida America/Metlakatla America/Mexico_City America/Miquelon America/Moncton America/Monterrey America/Montevideo America/Montreal America/Montserrat America/Nassau America/New_York America/Nipigon America/Nome America/Noronha America/North_Dakota/Beulah America/North_Dakota/Center America/North_Dakota/New_Salem America/Nuuk America/Ojinaga America/Panama America/Pangnirtung America/Paramaribo America/Phoenix America/Port-au-Prince America/Port_of_Spain America/Porto_Acre America/Porto_Velho America/Puerto_Rico America/Punta_Arenas America/Rainy_River America/Rankin_Inlet America/Recife America/Regina America/Resolute America/Rio_Branco America/Rosario America/Santa_Isabel America/Santarem America/Santiago America/Santo_Domingo America/Sao_Paulo America/Scoresbysund America/Shiprock America/Sitka America/St_Barthelemy America/St_Johns America/St_Kitts America/St_Lucia America/St_Thomas America/St_Vincent America/Swift_Current America/Tegucigalpa America/Thule America/Thunder_Bay America/Tijuana America/Toronto America/Tortola America/Vancouver America/Virgin America/Whitehorse America/Winnipeg America/Yakutat America/Yellowknife Antarctica/Casey Antarctica/Davis Antarctica/DumontDUrville Antarctica/Macquarie Antarctica/Mawson Antarctica/McMurdo Antarctica/Palmer Antarctica/Rothera Antarctica/South_Pole Antarctica/Syowa Antarctica/Troll Antarctica/Vostok Arctic/Longyearbyen Asia/Aden Asia/Almaty Asia/Amman Asia/Anadyr Asia/Aqtau Asia/Aqtobe Asia/Ashgabat Asia/Ashkhabad Asia/Atyrau Asia/Baghdad Asia/Bahrain Asia/Baku Asia/Bangkok Asia/Barnaul Asia/Beirut Asia/Bishkek Asia/Brunei Asia/Calcutta Asia/Chita Asia/Choibalsan Asia/Chongqing Asia/Chungking Asia/Colombo Asia/Dacca Asia/Damascus Asia/Dhaka Asia/Dili Asia/Dubai Asia/Dushanbe Asia/Famagusta Asia/Gaza Asia/Harbin Asia/Hebron Asia/Ho_Chi_Minh Asia/Hong_Kong Asia/Hovd Asia/Irkutsk Asia/Istanbul Asia/Jakarta Asia/Jayapura Asia/Jerusalem Asia/Kabul Asia/Kamchatka Asia/Karachi Asia/Kashgar Asia/Kathmandu Asia/Katmandu Asia/Khandyga Asia/Kolkata Asia/Krasnoyarsk Asia/Kuala_Lumpur Asia/Kuching Asia/Kuwait Asia/Macao Asia/Macau Asia/Magadan Asia/Makassar Asia/Manila Asia/Muscat Asia/Nicosia Asia/Novokuznetsk Asia/Novosibirsk Asia/Omsk Asia/Oral Asia/Phnom_Penh Asia/Pontianak Asia/Pyongyang Asia/Qatar Asia/Qostanay Asia/Qyzylorda Asia/Rangoon Asia/Riyadh Asia/Saigon Asia/Sakhalin Asia/Samarkand Asia/Seoul Asia/Shanghai Asia/Singapore Asia/Srednekolymsk Asia/Taipei Asia/Tashkent Asia/Tbilisi Asia/Tehran Asia/Tel_Aviv Asia/Thimbu Asia/Thimphu Asia/Tokyo Asia/Tomsk Asia/Ujung_Pandang Asia/Ulaanbaatar Asia/Ulan_Bator Asia/Urumqi Asia/Ust-Nera Asia/Vientiane Asia/Vladivostok Asia/Yakutsk Asia/Yangon Asia/Yekaterinburg Asia/Yerevan Atlantic/Azores Atlantic/Bermuda Atlantic/Canary Atlantic/Cape_Verde Atlantic/Faeroe Atlantic/Faroe Atlantic/Jan_Mayen Atlantic/Madeira Atlantic/Reykjavik Atlantic/South_Georgia Atlantic/St_Helena Atlantic/Stanley Australia/ACT Australia/Adelaide Australia/Brisbane Australia/Broken_Hill Australia/Canberra Australia/Currie Australia/Darwin Australia/Eucla Australia/Hobart Australia/LHI Australia/Lindeman Australia/Lord_Howe Australia/Melbourne Australia/NSW Australia/North Australia/Perth Australia/Queensland Australia/South Australia/Sydney Australia/Tasmania Australia/Victoria Australia/West Australia/Yancowinna Brazil/Acre Brazil/DeNoronha Brazil/East Brazil/West CET CST6CDT Canada/Atlantic Canada/Central Canada/Eastern Canada/Mountain Canada/Newfoundland Canada/Pacific Canada/Saskatchewan Canada/Yukon Chile/Continental Chile/EasterIsland Cuba EET EST EST5EDT Egypt Eire Etc/GMT Etc/GMT+0 Etc/GMT+1 Etc/GMT+10 Etc/GMT+11 Etc/GMT+12 Etc/GMT+2 Etc/GMT+3 Etc/GMT+4 Etc/GMT+5 Etc/GMT+6 Etc/GMT+7 Etc/GMT+8 Etc/GMT+9 Etc/GMT-0 Etc/GMT-1 Etc/GMT-10 Etc/GMT-11 Etc/GMT-12 Etc/GMT-13 Etc/GMT-14 Etc/GMT-2 Etc/GMT-3 Etc/GMT-4 Etc/GMT-5 Etc/GMT-6 Etc/GMT-7 Etc/GMT-8 Etc/GMT-9 Etc/GMT0 Etc/Greenwich Etc/UCT Etc/UTC Etc/Universal Etc/Zulu Europe/Amsterdam Europe/Andorra Europe/Astrakhan Europe/Athens Europe/Belfast Europe/Belgrade Europe/Berlin Europe/Bratislava Europe/Brussels Europe/Bucharest Europe/Budapest Europe/Busingen Europe/Chisinau Europe/Copenhagen Europe/Dublin Europe/Gibraltar Europe/Guernsey Europe/Helsinki Europe/Isle_of_Man Europe/Istanbul Europe/Jersey Europe/Kaliningrad Europe/Kiev Europe/Kirov Europe/Kyiv Europe/Lisbon Europe/Ljubljana Europe/London Europe/Luxembourg Europe/Madrid Europe/Malta Europe/Mariehamn Europe/Minsk Europe/Monaco Europe/Moscow Europe/Nicosia Europe/Oslo Europe/Paris Europe/Podgorica Europe/Prague Europe/Riga Europe/Rome Europe/Samara Europe/San_Marino Europe/Sarajevo Europe/Saratov Europe/Simferopol Europe/Skopje Europe/Sofia Europe/Stockholm Europe/Tallinn Europe/Tirane Europe/Tiraspol Europe/Ulyanovsk Europe/Uzhgorod Europe/Vaduz Europe/Vatican Europe/Vienna Europe/Vilnius Europe/Volgograd Europe/Warsaw Europe/Zagreb Europe/Zaporozhye Europe/Zurich GB GB-Eire GMT GMT+0 GMT-0 GMT0 Greenwich HST Hongkong Iceland Indian/Antananarivo Indian/Chagos Indian/Christmas Indian/Cocos Indian/Comoro Indian/Kerguelen Indian/Mahe Indian/Maldives Indian/Mauritius Indian/Mayotte Indian/Reunion Iran Israel Jamaica Japan Kwajalein Libya MET MST MST7MDT Mexico/BajaNorte Mexico/BajaSur Mexico/General NZ NZ-CHAT Navajo PRC PST8PDT Pacific/Apia Pacific/Auckland Pacific/Bougainville Pacific/Chatham Pacific/Chuuk Pacific/Easter Pacific/Efate Pacific/Enderbury Pacific/Fakaofo Pacific/Fiji Pacific/Funafuti Pacific/Galapagos Pacific/Gambier Pacific/Guadalcanal Pacific/Guam Pacific/Honolulu Pacific/Johnston Pacific/Kanton Pacific/Kiritimati Pacific/Kosrae Pacific/Kwajalein Pacific/Majuro Pacific/Marquesas Pacific/Midway Pacific/Nauru Pacific/Niue Pacific/Norfolk Pacific/Noumea Pacific/Pago_Pago Pacific/Palau Pacific/Pitcairn Pacific/Pohnpei Pacific/Ponape Pacific/Port_Moresby Pacific/Rarotonga Pacific/Saipan Pacific/Samoa Pacific/Tahiti Pacific/Tarawa Pacific/Tongatapu Pacific/Truk Pacific/Wake Pacific/Wallis Pacific/Yap Poland Portugal ROC ROK Singapore Turkey UCT US/Alaska US/Aleutian US/Arizona US/Central US/East-Indiana US/Eastern US/Hawaii US/Indiana-Starke US/Michigan US/Mountain US/Pacific US/Samoa UTC Universal W-SU WET Zulu"
  },"/sdwui-docs/localization/": {
    "title": "本土化",
    "keywords": "Guides",
    "url": "/sdwui-docs/localization/",
    "body": "使用本地化文件 现在进行本地化的预期方法是通过扩展。看： https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Developing-extensions 创建本地化文件 转到设置并单击底部的“下载本地化模板”按钮。这将下载一个您可以编辑的本地化模板。 使用新密钥更新旧本地化 此存储库 包含旧的孤立本地化。如果你想用新密钥更新它们，你可以使用以下脚本： import json files=['localization_template.json', 'old_localization.json'] with open('new_localization.json', \"w\") as outfile: res = dict() for f in files: dct = dict(json.load(open(f, \"r\").read())) res.update(dct) outfile.write(res) 然后翻译添加的内容。"
  },"/sdwui-docs/negative-prompt/": {
    "title": "否定提示",
    "keywords": "Guides",
    "url": "/sdwui-docs/negative-prompt/",
    "body": "否定提示是一种使用 Stable Diffusion 的方式，允许用户指定他不想看到的内容，而无需对模型进行任何额外负载或要求。据我所知，我是第一个使用这种方法的人；添加它的提交是 757bb7c4。该功能在用户中非常受欢迎，他们消除了稳定扩散的常见畸形，如额外的肢体。除了能够指定您不想看到的内容（有时可以通过通常的提示，有时则不能）之外，这还允许您在不使用任何 75 个令牌的情况下执行此操作，提示包括. 否定提示的工作方式是在进行采样时使用用户指定的文本而不是空字符串作为“unconditional_conditioning”。 这是来自 txt2img.py 的（简化）代码： # prompts = [\"a castle in a forest\"] # batch_size = 1 c = model.get_learned_conditioning(prompts) uc = model.get_learned_conditioning(batch_size * [\"\"]) samples_ddim, _ = sampler.sample(conditioning=c, unconditional_conditioning=uc, [...]) 这将启动重复的采样器： 去噪图片引导它看起来更像你的提示（调节） 去噪图片引导它看起来更像一个空提示（unconditional_conditioning） 查看它们之间的差异并使用它来为嘈杂的图片产生一组变化（不同的采样器以不同的方式执行该部分） 要使用否定提示，所需要的只是： # prompts = [\"a castle in a forest\"] # negative_prompts = [\"grainy, fog\"] c = model.get_learned_conditioning(prompts) uc = model.get_learned_conditioning(negative_prompts) samples_ddim, _ = sampler.sample(conditioning=c, unconditional_conditioning=uc, [...]) 然后，采样器将查看去噪后看起来像您的提示（城堡）的图像与去噪后看起来像您的负面提示（颗粒状、雾状）的图像之间的差异，并尝试将最终结果移向前者并远离后者。 ＃＃＃ 例子： a colorful photo of a castle in the middle of a forest with trees and (((bushes))), by Ismail Inceoglu, ((((shadows)))), ((((high contrast)))), dynamic shading, ((hdr)), detailed vegetation, digital painting, digital drawing, detailed painting, a detailed digital painting, gothic art, featured on deviantart Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 749109862, Size: 896x448, Model hash: 7460a6fa | negative prompt | image | |———————|—————————————————————————————————————————| 无 雾 粒状 雾，颗粒状 雾，颗粒状，紫色"
  },"/sdwui-docs/online-services/": {
    "title": "在线服务",
    "keywords": "Getting started",
    "url": "/sdwui-docs/online-services/",
    "body": "谷歌协作 由 TheLastBen 维护 由 camenduru 维护 由 ddPn08 维护 由 Akaibu 维护 Colab，AUTOMATIC1111 原创，已过时。 俄罗斯 colab，由 PR0LAPSE 维护 纸空间 由 Cyber​​es 维护 卡格尔 由 camenduru 维护 SageMaker Studio 实验室 由分形维护 抱脸 由 camenduru 维护 安装指南 -azure-ml-(commit)"
  },"/sdwui-docs/optimizations/": {
    "title": "优化",
    "keywords": "Guides",
    "url": "/sdwui-docs/optimizations/",
    "body": "命令行参数 可以启用一些优化： | commandline argument | explanation | |——————————–|————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————–| --xformers 使用 xformers 库。显着改善内存消耗和速度。 Windows 版本安装由 C43H66N12O12S2 维护的二进制文件。只会在一小部分配置上启用，因为这就是我们拥有二进制文件的目的。 文档 --force-enable-xformers 无论程序是否认为您可以运行它，都启用上面的 xformers。不要报告你运行这个的错误。 --opt-split-attention 交叉注意层优化显着减少了内存使用，几乎没有成本（一些报告提高了性能）。黑魔法。 默认情况下为 torch.cuda 打开，其中包括 NVidia 和 AMD 卡。 --disable-opt-split-attention Disables the optimization above. --opt-split-attention-v1 使用上面优化的旧版本，它不会占用大量内存（它会使用更少的 VRAM，但会更多地限制您可以制作的图片的最大尺寸）。 --medvram 通过将 Stable Diffusion 模型分成三部分 - cond（用于将文本转换为数字表示）、first_stage（用于将图片转换为潜在空间并返回）和 unet（用于潜在空间的实际去噪）并制作这样一来，始终只有一个在 VRAM 中，而将其他的发送到 CPU RAM。会降低性能，但只会降低一点——除非启用了实时预览。 --lowvram 对上面更彻底的优化，将unet拆分成很多模块，VRAM中只保留一个模块。对性能具有破坏性。 *不要批处理条件未处理 防止在采样期间对正负提示进行批处理，这实际上可以让您以 0.5 的批处理大小运行，从而节省大量内存。降低性能。不是命令行选项，而是使用“–medvram”或​​“–lowvram”隐式启用的优化。 --always-batch-cond-uncond 禁用上面的优化。只有与 --medvram 或 --lowvram 一起使用才有意义 --opt-channelslast Changes torch memory type for stable diffusion to channels last. Effects not closely studied. 额外提示（Windows）： https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/3889 禁用硬件 GPU 调度。 禁用浏览器硬件加速 进入 nvidia 控制面板，3d 参数，并将电源配置文件更改为“最大性能”"
  },"/sdwui-docs/": {
    "title": "稳定扩散 webUI",
    "keywords": "Getting started",
    "url": "/sdwui-docs/",
    "body": "基于用于稳定扩散的 Gradio 库的浏览器界面。 查看 自定义脚本 wiki 页面以获取用户开发的额外脚本。 ＃＃ 特征 带有图像的详细功能展示： 原始的 txt2img 和 img2img 模式 一键安装并运行脚本（但您仍然必须安装 python 和 git） 涂装 修复 彩色素描 提示矩阵 稳定扩散高档 注意，指定模型应该更加注意的文本部分 a man in a ((tuxedo)) - 会更加注意燕尾服 一个穿燕尾服的男人 (tuxedo:1.21) - 替代语法 选择文本并按 ctrl+up 或 ctrl+down 自动调整对所选文本的关注（代码由匿名用户提供） Loopback，多次运行img2img处理 X/Y 图，一种绘制具有不同参数的二维图像图的方法 文本倒置 拥有任意数量的嵌入，并为它们使用任何你喜欢的名称 使用每个标记具有不同数量向量的多个嵌入 适用于半精度浮点数 在 8GB 上训练嵌入（还有 6GB 工作的报告） 附加选项卡： GFPGAN，修复人脸的神经网络 CodeFormer，面部修复工具作为 GFPGAN 的替代品 RealESRGAN，神经网络升级器 ESRGAN，具有大量第三方模型的神经网络升级器 SwinIR 和 Swin2SR（见此处），神经网络升频器 LDSR，潜在扩散超分辨率升级 调整宽高比选项 取样方法选择 调整采样器 eta 值（噪声倍增器） 更高级的噪音设置选项 随时中断处理 4GB 视频卡支持（还有 2GB 工作报告） 正确的批次种子 实时提示令牌长度验证 生成参数 用于生成图像的参数与该图像一起保存 在 PNG 的 PNG 块中，在 JPEG 的 EXIF 中 可将图片拖拽至PNG信息标签，恢复生成参数并自动复制到UI中 可以在设置中禁用 将图像/文本参数拖放到提示框 读取生成参数按钮，将提示框中的参数加载到 UI 设置页面 从 UI 运行任意 python 代码（必须使用 –allow-code 运行才能启用） 大多数 UI 元素的鼠标悬停提示 可以通过文本配置更改 UI 元素的默认值/混合/最大/步进值 随机艺术家按钮 平铺支持，用于创建可以像纹理一样平铺的图像的复选框 进度条和实时图像生成预览 否定提示，一个额外的文本字段，允许您列出您不想在生成的图像中看到的内容 样式，一种保存部分提示并稍后通过下拉列表轻松应用它们的方法 变体，一种生成相同图像但有微小差异的方法 调整种子大小，一种生成相同图像但分辨率略有不同的方法 CLIP 询问器，一个尝试从图像中猜测提示的按钮 Prompt Editing，一种改变prompt mid-generation的方法，说开始制作西瓜并中途切换到动漫女孩 批处理，使用 img2img 处理一组文件 Img2img Alternative, reverse Euler method of cross attention control Highres Fix，一个方便的选项，可以一键生成高分辨率图片而不会出现通常的失真 即时重新加载检查点 检查点合并，一个允许您将最多 3 个检查点合并为一个的选项卡 自定义脚本 具有来自社区的许多扩展 Composable-Diffusion，一种同时使用多个提示的方法 使用大写的 AND 分隔提示 还支持提示权重：a cat :1.2 AND a dog AND a penguin :2.2 提示没有令牌限制（原始稳定扩散让您最多使用 75 个令牌） DeepDanbooru 集成，为动漫提示创建 danbooru 风格标签 xformers，选择卡片的主要速度提升：（添加 –xformers 到命令行参数） 通过扩展：历史选项卡：在 UI 中方便地查看、定向和删除图像 生成永久选项 培训标签 超网络和嵌入选项 预处理图像：使用 BLIP 或 deepdanbooru（用于动漫）裁剪、镜像、自动标记 剪辑跳过 使用超网络 使用 VAE 进度条中的预计完成时间 应用程序接口 支持 RunwayML 的专用修复模型。 通过扩展：Aesthetic Gradients，一种通过使用剪辑图像嵌入生成具有特定美感的图像的方法（https: //github.com/vicgalle/stable-diffusion-aesthetic-gradients) 稳定扩散 2.0 支持 - 请参阅 wiki 的说明 安装和运行 确保满足所需的 dependencies 并遵循适用于 NVidia 的说明/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs）（推荐）和 [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and -在 AMD-GPU 上运行）GPU。 或者，使用在线服务（如 Google Colab）： 在线服务列表 在 Windows 上自动安装 安装Python 3.10.6，勾选“Add Python to PATH” 安装 git。 下载 stable-diffusion-webui 存储库，例如通过运行 git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git。 将 model.ckpt 放在 models 目录中（参见 dependencies 获取它的位置）。 *（可选）* 将 GFPGANv1.4.pth 与 webui.py 放在基本目录中（参见 [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/ wiki/Dependencies）获取它的位置）。 以普通非管理员用户身份从 Windows 资源管理器运行 webui-user.bat。 在 Linux 上自动安装 1.安装依赖： # Debian-based: sudo apt install wget git python3 python3-venv # Red Hat-based: sudo dnf install wget git python3 # Arch-based: sudo pacman -S wget git python3 要安装在/home/$(whoami)/stable-diffusion-webui/，运行： bash &lt;(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh) 在 Apple Silicon 上安装 此处 找到说明。 贡献 以下是向此存储库添加代码的方法：贡献 文档 文档已从本自述文件移至项目的 wiki。 ##学分 稳定扩散 - https://github.com/CompVis/stable-diffusion, https://github.com/CompVis/taming-transformers k-diffusion - https://github.com/crowsonkb/k-diffusion.git GFPGAN - https://github.com/TencentARC/GFPGAN.git CodeFormer - https://github.com/sczhou/CodeFormer ESRGAN - https://github.com/xinntao/ESRGAN SwinIR - https://github.com/JingyunLiang/SwinIR Swin2SR - https://github.com/mv-lab/swin2sr LDSR - https://github.com/Hafiidz/latent-diffusion MiDaS - https://github.com/isl-org/MiDaS 优化思路 - https://github.com/basujindal/stable-diffusion 交叉注意层优化 - Doggettx - https://github.com/Doggettx/stable-diffusion，快速编辑的原创想法。 交叉注意层优化 - InvokeAI，lstein - https://github.com/invoke-ai/InvokeAI（原 http://github.com/lstein/stable-diffusion） Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion（我们没有使用他的代码，但我们正在使用他的想法）。 SD 高档的想法 - https://github.com/jquesnelle/txt2imghd 为 outpainting mk2 生成噪音 - https://github.com/parlance-zz/g-diffuser-bot CLIP 审讯器的想法和借用一些代码 - https://github.com/pharmapsychotic/clip-interrogator 可组合扩散的想法 - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch xformers - https://github.com/facebookresearch/xformers DeepDanbooru - 动漫扩散器审讯器 https://github.com/KichangKim/DeepDanbooru 安全建议 - RyotaK 初始 Gradio 脚本 - 由匿名用户发布在 4chan 上。谢谢匿名用户。 （你）"
  },"/sdwui-docs/tests": {
    "title": "测试",
    "keywords": "development",
    "url": "/sdwui-docs/tests",
    "body": "＃＃ 入门 []（） []（） 指南 ＃＃ 发展 有一些测试只是验证基本图像创建是否适用于 vi API。 要运行测试，请将 --tests 作为命令行参数添加到 launch.py​​ 以及其他命令行参数： python launch.py --skip-torch-cuda-test --deepdanbooru --no-half-vae --tests 您会在 test/stdout.txt 和 test/stderr.txt 中找到主程序的输出。"
  },"/sdwui-docs/textual-inversion/": {
    "title": "文字倒置",
    "keywords": "Guides",
    "url": "/sdwui-docs/textual-inversion/",
    "body": "什么是文本倒置？ Textual Inversion 允许您在自己的图片上训练神经网络的一小部分，并在生成新图片时使用结果。在这种情况下，嵌入是您训练的神经网络的一小部分的名称。 训练的结果是一个 .pt 或 .bin 文件（前者是原作者使用的格式，后者是扩散器库使用的格式），其中包含嵌入。 有关什么是文本反转的更多详细信息，请参见原始站点：https://textual-inversion.github.io/。 使用预训练嵌入 将嵌入放入 embeddings 目录并在提示中使用其文件名。您不必重新启动程序即可使其工作。 例如，这里是 Usada Pekora 的嵌入，我在 WD1.2 模型上训练了 53 张图片（ 119 增加）19500 步，每个标记设置 8 个向量。 它生成的图片： portrait of usada pekora Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 45dee52b 您可以在一个提示中组合多个嵌入： portrait of usada pekora, mignon Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 45dee52b 非常小心你使用的是哪种模型与你的嵌入：它们与你在训练期间使用的模型一起工作得很好，但在不同的模型上不太好。例如，这里是上面的嵌入和香草 1.4 稳定扩散模型： portrait of usada pekora Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 4077357776, Size: 512x512, Model hash: 7460a6fa 训练嵌入 文本反转选项卡 对用户界面中训练嵌入的实验支持。 创建一个新的空嵌入，选择带有图像的目录，在其上训练嵌入 该功能非常原始，使用风险自负 经过几万步后，我能够重现我在其他回购协议中将动漫艺术家训练为风格的结果 适用于半精度浮点数，但需要试验以查看结果是否同样好 如果你有足够的内存，使用 --no-half --precision full 运行更安全 用于自动运行图像预处理的 UI 部分。 你可以在不丢失任何数据的情况下中断和恢复训练（AdamW 优化参数除外，但似乎现有的回购协议都没有保存这些参数，所以一般认为它们并不重要） 不支持批量大小或梯度累积 不应使用 --lowvram 和 --medvram 标志来运行它。 参数说明 创建嵌入 名称：创建嵌入的文件名。在提及嵌入时，您还将在提示中使用此文本。 初始化文本：您创建的嵌入最初将填充此文本的向量。如果您创建一个名为“zzzz1234”的单向量嵌入，并将“tree”作为初始化文本，并在提示中使用它而不进行训练，那么提示“a zzzz1234 by monet”将产生与“a tree by monet”相同的图片。 每个标记的向量数量：嵌入的大小。这个值越大，你可以嵌入到主题中的信息就越多，但它会从你的提示容许量中拿走的词也越多。对于稳定扩散，您在提示中有 75 个标记的限制。如果您在提示中使用包含 16 个向量的嵌入，那将为您留出 75 - 16 = 59 的空间。同样根据我的经验，向量的数量越多，获得良好结果所需的图片就越多。 预处理 这从目录中获取图像，处理它们以准备好进行文本反转，并将结果写入另一个目录。这是一个方便的功能，您可以根据需要自行预处理图片。 源目录：包含图像的目录 目标目录：将写入结果的目录 创建翻转副本：对于每个图像，还写入其镜像副本 将超大图像一分为二：如果图像太高或太宽，调整其大小以使其短边与所需分辨率相匹配，并从中创建两个可能相交的图片。 使用 BLIP 标题作为文件名：使用询问器的 BLIP 模型为文件名添加标题。 训练嵌入 嵌入：从此下拉列表中选择要训练的嵌入。 学习率：训练应该进行多快。将此参数设置为较高值的危险在于，如果将其设置得太高，则可能会破坏嵌入。如果你在训练信息文本框中看到“Loss: nan”，这意味着你失败了，嵌入已经死了。使用默认值，这不应该发生。可以使用以下语法在此设置中指定多个学习率：0.005:100, 1e-3:1000, 1e-5 - 这将在前 100 步使用 0.005 的 lr 进行训练，然后是 1e-3 直到 1000 步，然后 1e-5 直到结束。 数据集目录：包含用于训练的图像的目录。它们都必须是正方形的。 日志目录：样本图像和部分训练嵌入的副本将写入此目录。 提示模板文件：带有提示的文本文件，每行一个，用于训练模型。请参阅目录“textual_inversion_templates”中的文件，了解您可以使用这些文件做什么。在训练样式时使用 style.txt，在训练对象嵌入时使用 subject.txt。文件中可以使用以下标签： [name]: 嵌入的名称 [filewords]：来自数据集中图像文件名的单词。有关更多信息，请参见下文。 最大步数：训练将在完成这么多步后停止。一个步骤是当一张图片（或一批图片，但目前不支持批量）显示给模型并用于改进嵌入。如果您中断训练并在稍后恢复，则步数会保留。 保存嵌入 PNG 块中的图像：每次生成图像时，它都会与最近记录的嵌入相结合，并以既可以作为图像共享又可以放入嵌入文件夹的格式保存到 image_embeddings并加载。 预览提示：如果不为空，该提示将用于生成预览图片。如果为空，将使用培训提示。 文件字 [filewords] 是提示模板文件的标签，允许您将文件名中的文本插入到提示中。默认情况下，文件的扩展名以及文件名开头的所有数字和破折号 (-) 都被删除。所以这个文件名：000001-1-a man in suit.png 将变成提示文本：a man in suit。文件名中的文本格式保持原样。 可以使用选项 Filename word regex 和 Filename join string 来改变文件名中的文本：例如，使用 word regex = \\w+ 和 join string = , ，上面的文件将产生这个文本: a, man, in, suit。正则表达式用于从文本中提取单词（它们是 ['a', 'man', 'in', 'suit', ]），并在这些单词之间放置连接字符串 (‘, ‘) 以创建一个文本：a, man, in, suit。 也可以创建一个与图像文件名相同的文本文件（000001-1-a man in suit.txt），然后将提示文本放在那里。不会使用文件名和正则表达式选项。 第三方回购 我使用这些存储库成功地训练了嵌入： nicolai256 lstein 其他选择是在 colabs 上训练和/或使用扩散器库，我对此一无所知。 在线查找嵌入 huggingface 概念库 - 很多不同的嵌入，但是 16777216c - NSFW，一个神秘陌生人的动漫艺术家风格。 cattoroboto - anon 的一些动漫嵌入。 viper1 - NSFW，毛茸茸的女孩。 anon 的嵌入 - NSFW，动漫艺术家。 rentry - 一个包含来自许多来源的嵌入链接的页面。 超网络 Hypernetworks 是一个新颖的（明白吗？）概念，用于在不影响其任何权重的情况下微调模型。 当前训练超网络的方法是在文本反转选项卡中。 训练的工作方式与文本倒置相同。 唯一的要求是使用非常非常低的学习率，比如 0.000005 或 0.0000005。 愚蠢的愚蠢的指南 一位匿名用户编写了一份使用超网络的图片指南：https://rentry.org/hypernetwork4dumdums 训练时从 VRAM 中卸载 VAE 和 CLIP 设置选项卡上的此选项允许您以较慢的预览图片生成为代价节省一些内存。"
  },"/sdwui-docs/troubleshooting/": {
    "title": "故障排除",
    "keywords": "Guides",
    "url": "/sdwui-docs/troubleshooting/",
    "body": "该程序经测试可在 Python 3.10.6 上运行。除非你想找麻烦，否则不要使用其他版本。 安装程序创建一个 python 虚拟环境，因此安装的模块都不会影响现有的 python 系统安装。 要使用系统的 python 而不是创建虚拟环境，请使用自定义参数替换“set VENV_DIR=-”。 要从头开始重新安装，请删除目录：venv、repositories。 第一次启动程序时，会显示 python 解释器的路径。如果这不是您安装的 python，您可以在 webui-user 脚本中指定完整路径；请参阅 使用自定义参数运行。 如果所需的 Python 版本不在 PATH 中，请使用 python 可执行文件的完整路径修改 webui-user.bat 中的 set PYTHON=python 行。 示例：set PYTHON=B:\\soft\\Python310\\python.exe 来自“requirements_versions.txt”的安装程序要求，其中列出了专门与 Python 3.10.6 兼容的模块的版本。如果这不适用于其他版本的 Python，设置自定义参数“set REQS_FILE=requirements.txt”可能会有所帮助。 低 VRAM 视频卡 在具有少量 VRAM (&lt;=4GB) 的视频卡上运行时，可能会出现内存不足错误。 可以通过命令行参数启用各种优化，牺牲一些/很多速度以支持使用更少的 VRAM： 如果您有 4GB VRAM 并想制作 512x512（或最多 640x640）图像，请使用 --medvram。 如果您有 4GB VRAM 并想制作 512x512 图像，但使用 --medvram 时出现内存不足错误，请改用 --medvram --opt-split-attention。 如果您有 4GB VRAM 并想制作 512x512 图像，但仍然出现内存不足错误，请改用 --lowvram --always-batch-cond-uncond --opt-split-attention。 如果您有 4GB VRAM 并且想要使图像比使用 --medvram 时更大，请使用 --lowvram --opt-split-attention。 如果你有更多的 VRAM 并且想要制作比你通常可以制作的更大的图像（例如 1024x1024 而不是 512x512），请使用 --medvram --opt-split-attention。您也可以使用 --lowvram 但效果可能几乎不明显。 否则，不要使用其中任何一个。 绿屏或黑屏 显卡 在不支持半精度浮点数的视频卡上运行时（16xx 卡的一个已知问题），可能会出现绿色或黑色屏幕，而不是生成的图片。 这可以通过在 VRAM 使用量显着增加时使用命令行参数 --precision full --no-half 来解决，这可能需要 --medvram。 启用 xformers 后出现“CUDA 错误：没有可在设备上执行的内核映像” 您安装的 xformers 与您的 GPU 不兼容。如果您使用 Python 3.10，拥有 Pascal 或更高版本的卡并在 Windows 上运行，请将 --reinstall-xformers --xformers 添加到您的 COMMANDLINE_ARGS 以升级到工作版本。升级后删除 --reinstall-xformers。 NameError: 名称 ‘xformers’ 未定义 如果你使用 Windows，这意味着你的 Python 太旧了。使用 3.10 如果是 Linux，则必须自己构建 xformers 或避免使用 xformers。"
  },"/sdwui-docs/ui-defaults/": {
    "title": "更改用户界面默认值",
    "keywords": "Getting started",
    "url": "/sdwui-docs/ui-defaults/",
    "body": "Web UI 中的默认值可以通过编辑 ui-config.json 来更改，它在第一次运行后出现在包含 webui.py 的基本目录中。 更改仅在重新启动后应用。 { \"txt2img/Sampling Steps/value\": 20, \"txt2img/Sampling Steps/minimum\": 1, \"txt2img/Sampling Steps/maximum\": 150, \"txt2img/Sampling Steps/step\": 1, \"txt2img/Batch count/value\": 1, \"txt2img/Batch count/minimum\": 1, \"txt2img/Batch count/maximum\": 32, \"txt2img/Batch count/step\": 1, \"txt2img/Batch size/value\": 1, \"txt2img/Batch size/minimum\": 1, # ... }"
  },"/sdwui-docs/xformers/": {
    "title": "变形金刚",
    "keywords": "Guides",
    "url": "/sdwui-docs/xformers/",
    "body": "Xformers 库是加速图像生成的可选方法。 除了一种特定配置外，没有适用于 Windows 的二进制文件，但您可以自己构建它。 来自一位匿名用户的指南，尽管我认为它适用于在 Linux 上构建： 关于如何构建 XFORMERS 的指南 还包括如何摆脱 sm86 对 voldy 新提交的限制 1.进入webui目录 2.来源./venv/bin/activate 3.cd 存储库 3.git 克隆 https://github.com/facebookresearch/xformers.git 4.cd xformers git 子模块更新 --init --recursive 6.pip install -r requirements.txt 7.pip 安装-e. @duckness 在 Windows 上构建 xFormers 如果您在 Python 3.10 中使用 Pascal、Turing、Ampere、Lovelace 或 Hopper 卡，则无需再手动构建。卸载现有的 xformers 并使用 --xformers 启动 repo。将安装兼容的车轮。 1.【安装VS Build Tools 2022】(https://visualstudio.microsoft.com/downloads/?q=build+tools#build-tools-for-visual-studio-2022)，你只需要Desktop development with C++ Install CUDA 11.3（后面的版本没测试），选择custom，你只需要下面的（VS integration估计不需要） ): 3.克隆xFormers repo，创建一个venv并激活它 git clone https://github.com/facebookresearch/xformers.git cd xformers git submodule update --init --recursive python -m venv venv ./venv/scripts/activate 为了避免获取 CPU 版本的问题，单独安装 pyTorch： pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113 5.然后安装其余的依赖项： pip install -r requirements.txt pip install wheel 由于 CUDA 11.3 相当旧，您需要强制启用它以在 MS Build Tools 2022 上构建。如果在 powershell 上执行 $env:NVCC_FLAGS = \"-allow-unsupported-compiler\"，或 set NVCC_FLAGS =-allow-unsupported-compiler 如果在 cmd 您终于可以构建 xFormers，请注意构建将花费很长时间（可能 10-20 分钟），它最初可能会抱怨一些错误，但它应该仍然可以正确编译。 可选提示：要进一步加快多核 CPU Windows 系统的速度，请安装 ninja https://github.com/ninja-build/ninja。 安装步骤： 从 https://github.com/ninja-build/ninja/releases 下载 ninja-win.zip 并解压 将 ninja.exe 放在 C:\\Windows 下或将提取的 ninja.exe 的完整路径添加到系统 PATH 中 在 cmd 中运行 ninja -h 并验证是否看到打印的帮助消息 运行以下命令开始构建。它应该自动使用 Ninja，不需要额外的配置。您应该会看到明显更高的 CPU 使用率 (40%+)。 python setup.py 构建 python setup.py bdist_wheel 这将配备 AMD 5800X CPU 的 Windows PC 的构建时间从 1.5 小时减少到 10 分钟。 Linux 和 MacOS 也支持 Ninja，但我没有这些操作系统可以测试，因此无法提供分步教程。 运行以下命令： python setup.py 构建 蟒蛇 setup.py bdist_wheel In xformers directory, navigate to the dist folder and copy the .whl file to the base directory of stable-diffusion-webui In stable-diffusion-webui directory, install the .whl, change the name of the file in the command below if the name is different: ./venv/脚本/激活 pip 安装 xformers-0.0.14.dev0-cp310-cp310-win_amd64.whl Ensure that xformers is activated by launching stable-diffusion-webui with --force-enable-xformers Non-deterministic / unstable / inconsistent results: Known issue. See this list on the discussion page."
  },"/sdwui-docs/api/": {
    "title": "API指南",
    "keywords": "Guides",
    "url": "/sdwui-docs/api/",
    "body": "首先，当然是使用 --api 命令行参数运行 web ui 在你的“webui-user.bat”中的例子：set COMMANDLINE_ARGS=--api 这会启用可在 http://127.0.0.1:7860/docs（或任何 URL 为 + /docs）查看的 api 我感兴趣的基本是这两个。让我们只关注/sdapi/v1/txt2img 当您展开该选项卡时，它会提供一个要发送到 API 的负载示例。我经常用这个作为参考。 这就是后端。 API 基本上说明了可用的内容、要求的内容以及将其发送到哪里。现在转到前端，我将开始使用我想要的参数构建有效负载。一个例子可以是： payload = { \"prompt\": \"maltese puppy\", \"steps\": 5 } 我可以在有效载荷中放入尽可能少或尽可能多的参数。 API 将对我未设置的任何内容使用默认值。 之后，我可以将它发送到 API response = requests.post(url=f'http://127.0.0.1:7860/sdapi/v1/txt2img', json=payload) 同样，此 URL 需要与 Web 用户界面的 URL 相匹配。 如果我们执行此代码，Web UI 将根据负载生成图像。那很好，但那又怎样呢？哪里都没有图。。。 在后端完成它的工作后，API 将响应发送回上面分配的变量：response。响应包含三个条目； “图像”、“参数”和“信息”，我必须想办法从这些条目中获取信息。 首先，我放置了这一行 r = response.json() 以便更容易处理响应。 “图像”是生成的图像，这是我最想要的。没有链接或任何东西；这是一串巨大的随机字符，显然我们必须对其进行解码。我是这样做的： for i in r['images']: image = Image.open(io.BytesIO(base64.b64decode(i.split(\",\",1)[0]))) 这样，我们在 image 变量中就有了一张可以使用的图像，例如使用 image.save('output.png') 保存它。 “参数”显示发送到 API 的内容，这可能很有用，但在这种情况下我想要的是“信息”。我用它来将元数据插入到图像中，这样我就可以将它放到 web ui PNG Info 中。为此，我可以访问/sdapi/v1/png-info API。我需要将上面得到的图像输入其中。 png_payload = { \"image\": \"data:image/png;base64,\" + i } response2 = requests.post(url=f'http://127.0.0.1:7860/sdapi/v1/png-info', json=png_payload) 之后，我可以使用 response2.json().get(\"info\") 获取信息 应该工作的示例代码如下所示： import json import requests import io import base64 from PIL import Image, PngImagePlugin url = \"http://127.0.0.1:7860\" payload = { \"prompt\": \"puppy dog\", \"steps\": 5 } response = requests.post(url=f'{url}/sdapi/v1/txt2img', json=payload) r = response.json() for i in r['images']: image = Image.open(io.BytesIO(base64.b64decode(i.split(\",\",1)[0]))) png_payload = { \"image\": \"data:image/png;base64,\" + i } response2 = requests.post(url=f'{url}/sdapi/v1/png-info', json=png_payload) pnginfo = PngImagePlugin.PngInfo() pnginfo.add_text(\"parameters\", response2.json().get(\"info\")) image.save('output.png', pnginfo=pnginfo) 导入我需要的东西 定义要发送的 url 和有效载荷 通过 API 将所述有效负载发送到所述 url 在循环中抓取“图像”并对其进行解码 对于每个图像，将其发送到 png 信息 API 并取回该信息 定义一个插件来添加png信息，然后将我定义的png信息添加到其中 最后，使用 png 信息保存图像 关于“override_settings”的注释。 此端点的目的是覆盖单个请求的 Web UI 设置，例如 CLIP 跳过。可以传递到此参数的设置在 url 的 /docs 中可见。 您可以展开选项卡，API 将提供一个列表。有几种方法可以将此值添加到负载中，但我就是这样做的。我将使用“filter_nsfw”和“CLIP_stop_at_last_layers”进行演示。 payload = { \"prompt\": \"cirno\", \"steps\": 20 } override_settings = {} override_settings[\"filter_nsfw\"] = true override_settings[\"CLIP_stop_at_last_layers\"] = 2 override_payload = { \"override_settings\": override_settings } payload.update(override_payload) 有正常的有效载荷 之后，初始化一个字典（我称之为“override_settings”，但可能不是最好的名字） 然后我可以添加任意数量的键：值对 仅使用此参数创建一个新的有效载荷 更新原始有效载荷以将其添加到其中 所以在这种情况下，当我发送有效载荷时，我应该在 20 步时得到一个“cirno”，CLIP 跳过 2 步，并且 NSFW 过滤器打开。 对于某些设置或情况，您可能希望保留更改。为此，您可以发布到 /sdapi/v1/options API 端点 我们可以使用到目前为止学到的知识并为此轻松设置代码。这是一个例子： url = \"http://127.0.0.1:7860\" option_payload = { \"sd_model_checkpoint\": \"Anything-V3.0-pruned.ckpt [2700c435]\", \"CLIP_stop_at_last_layers\": 2 } response = requests.post(url=f'{url}/sdapi/v1/options', json=option_payload) 将此有效负载发送到 API 后，模型应切换到我设置的模型并将 CLIP 跳转设置为 2。重申一下，这与“override_settings”不同，因为此更改将持续存在，而“override_settings”是针对单个请求的. 请注意，如果您要更改 sd_model_checkpoint，则该值应该是检查点在 Web 用户界面中显示的名称。这可以通过此 API 端点引用（与我们引用“选项”API 的方式相同） “标题”（名称和散列）是您想要使用的。 这是提交 47a44c7 要获得更完整的前端实现，如果有人想以我的 Discord 机器人为例，请点击 此处。大多数操作发生在 stablecog.py 中。有很多注释解释了每个代码的作用。 该指南可以在 讨论 页面中找到。 另外，查看这个用于 webui 的 python API 客户端库：https://github.com/mix1009/sdwebuiapi"
  },"/sdwui-docs/install/": {
    "title": "安装",
    "keywords": "Getting started",
    "url": "/sdwui-docs/install/",
    "body": "如果您使用的是较旧且功能较弱的计算机，请考虑使用其中一种在线服务（例如 Colab）。虽然可以在内存小于 4Gb 的 GPU 甚至 TPU 上运行生成模型并进行一些优化，但依赖云服务通常更快、更实用。 在线服务 如果您决定在本地安装，请确保满足所需的 dependencies。 在 Windows 上自动安装 这是针对基于 NVidia 的 GPU 的说明。对于 AMD，请参阅 此处。 安装Python 3.10.6，勾选“Add Python to PATH” 安装 git。 下载 stable-diffusion-webui 存储库，例如通过运行 git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git。 将 model.ckpt 放在 models 目录中（参见 dependencies 获取它的位置）。 *（可选）* 将 GFPGANv1.4.pth 与 webui.py 放在基本目录中（参见 [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/ wiki/Dependencies）获取它的位置）。 以普通非管理员用户身份从 Windows 资源管理器运行 webui-user.bat。 在 Linux 上自动安装 这是针对基于 NVidia 的 GPU 的说明。对于 AMD，请参阅 AMD 1.安装依赖： # Debian-based: sudo apt install wget git python3 python3-venv # Red Hat-based: sudo dnf install wget git python3 # Arch-based: sudo pacman -S wget git python3 要安装在/home/$(whoami)/stable-diffusion-webui/，运行： bash &lt;(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh) 在 Apple Silicon 上安装 在 此处 找到说明。"
  },"/sdwui-docs/2022-12-27-my-new-post.html": {
    "title": "Release note example",
    "keywords": "",
    "url": "/sdwui-docs/2022-12-27-my-new-post.html",
    "body": "Test"
  }}
